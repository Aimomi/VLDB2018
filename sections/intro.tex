\section{Introduction}
In the big data era, sharing data with partners or releasing data to the public frequently occurs. Discussing and sharing data with other data scientists is ideal because there are multiple analytical techniques and the diversity in analytical perspectives is the key that may lead to success. However, privacy should be the top priority in the sharing process to protect people who were willing to share valuable information.

Anonymization techniques remove identifiers (such as social security numbers) and modify quasi-identifiers (such as gender, ZIP code, age, occupation, and so forth). However, other sensitive attributes that are neither identifiers nor quasi-identifiers are often disclosed without any modification. If adversaries possess background knowledge or other information sources, then they can recover the identification of records (i.e., \textit{re-identification attack}). Data perturbation is occasionally preferred because it changes or adds noise to values. Unfortunately,  data usability is negatively impacted after these modifications.  Moreover,  methods have been developed to recognize and remove noise~\cite{Agrawal:2000:PDM:335191.335438}.

In fact, \textbf{it is very difficult to simultaneously achieve good privacy and usability levels after anonymization or other  modifications.} In general, privacy level and data utility are inversely proportional to each other. 
% If we  suppress modifications, our partners can access high-quality information and produce good results, but it introduces a risk of unwanted information leakage.  This does not mean that our partners are adversaries; they can also be attacked, and consequently, the private data can  be leaked. This shows the difficulty when anonymizing data. Even for reliable partners, we may have to modify the data a lot, thereby diminishing its utility.
We propose a data synthesis method based on generative adversarial networks (GANs). GANs are a generative model very recently proposed by deep learning researchers~\cite{goodfellow2014generative}. These models have shown significant improvements over other generative models in image and text datasets. Our method, named \textit{table-GAN},  is specialized for synthesizing relational databases (i.e., tables) that contain categorical, discrete, and continuous values --- we leave other types of data as future work. The main advantages of generating synthetic tables are as follows:
\begin{enumerate}
\item There is no one-to-one relationship\footnote{An anonymized (or perturbed) table is created by modifying records in the original table one by one and there typically exists one-to-one correspondence between the two tables, which is the main reason why re-identification attacks are possible.} between real records and synthetic records, and re-identification attacks are impossible.
\item All attribute values are fake and safe from attribute disclosure.
\item Machine learning models trained using very carefully synthesized tables show behavior similar to that of models trained using the original table; they can replace each other (i.e., \textbf{\textit{model compatibility}}).
\item Synthesized tables can be shared with partners without any concerns of information leakage.
\end{enumerate}


\textbf{In existing anonymization and perturbation methods, modifications should be suppressed if one wants to improve model compatibility or data utility. However, our method can obtain both of privacy perseverance and model compatibility simultaneously by generating fake tables that have global statistics similar to that of the original table even though they differ at the record level.}

Whereas original GANs consist of two neural networks (\textit{generator} and \textit{discriminator} neural networks), our table-GAN consists of three neural networks (\textit{generator}, \textit{discriminator}, and \textit{classifier} neural networks). The discriminator attempts to distinguish between real and synthetic records (i.e., binary classification), and the generator obfuscates the task of the discriminator by generating realistic records. They continue iterating the adversarial game, and the generator can achieve unprecedented generation performance at the end of the two-player game. In our table-GAN, we add an additional classifier neural network to increase the \textbf{\textit{semantic integrity}} of synthetic records. For instance, (cholesterol=60.1, diabetes=1) is not a semantically correct record (because the cholesterol level is too low to be diagnosed as diabetes), and there may be no such record in the original table. We prevent the generation of such records by adding a classifier (that learns the semantics from the original table) into the training process because  otherwise it is easy to determine that the table is fabricated.

Loss (or objective) functions are key for training neural networks. In general, neural networks are trained by minimizing loss functions. For instance, Equation~\eqref{eq:gan} shows the objective function of conventional GANs, denoted as \textit{original loss} in our paper. In addition to this function, we also design two additional loss functions -- \textit{information loss} and \textit{classification loss} -- that are specialized in the table synthesis process.

\textit{Information loss} matches the first-order (i.e., mean) and second-order (i.e., standard deviation) statistics of the original and synthetic tables; thus, synthetic records have the same statistical characteristics as the original records. \textit{Classification loss} maintains the semantic integrity. We found that synthesizing a semantically sound table while maintaining a good balance between privacy and usability is very challenging. Therefore, our training process is considerably more complicated than the original GAN model; however, in our experiments, the training time is less than 20 minutes.

For our experiments, we use four datasets from different domains and consider many state-of-the-art techniques including $k$-anonymity, $t$-closeness, $(\epsilon,d)$-differential privacy, $\delta$-disclosure, post-randomization, and so forth. In general, existing anonymization techniques that do not actively change sensitive values present the best model compatibility, but their privacy level is too low against capable adversaries. Among all baseline methods, the proposed table-GAN shows the best trade-off between privacy and model compatibility. Its model compatibility is slightly worse than that of existing anonymization techniques in general (surprisingly, our table-GAN occasionally exhibits better model compatibility than anonymization techniques), but its privacy level is considerably higher than that of such techniques.

Our paper outline is as follows. In Section 2, we introduce articles related to privacy preserving techniques, privacy risk evaluation methods, and generative models. We sketch the overall architecture of our method in Section 3, followed by detailed descriptions in Section 4. We  show that our method is scalable in Subsection 4.5. In Section 5, We utilize four real-world relational databases for in-depth comparisons with existing privacy preserving techniques. In Section 6, we will conclude the paper with final remarks and future work.