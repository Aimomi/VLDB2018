
@incollection{domingo-ferrer_survey_2008,
	series = {Advances in {Database} {Systems}},
	title = {A {Survey} of {Inference} {Control} {Methods} for {Privacy}-{Preserving} {Data} {Mining}},
	isbn = {978-0-387-70991-8 978-0-387-70992-5},
	abstract = {Inference control in databases, also known as Statistical Disclosure Control (SDC), is about protecting data so they can be published without revealing confidential information that can be linked to specific individuals among those to which the data correspond. This is an important application in several areas, such as official statistics, health statistics, e-commerce (sharing of consumer data), etc. Since data protection ultimately means data modification, the challenge for SDC is to achieve protection with minimum loss of the accuracy sought by database users. In this chapter, we survey the current state of the art in SDC methods for protecting individual data (microdata). We discuss several information loss and disclosure risk measures and analyze several ways of combining them to assess the performance of the various methods. Last but not least, topics which need more research in the area are identified and possible directions hinted.},
	language = {en},
	urldate = {2017-09-05TZ},
	booktitle = {Privacy-{Preserving} {Data} {Mining}},
	publisher = {Springer, Boston, MA},
	author = {Domingo-Ferrer, Josep},
	year = {2008},
	note = {00071 
DOI: 10.1007/978-0-387-70992-5\_3},
	pages = {53--80}
}

@incollection{prasser_putting_2015,
	title = {Putting {Statistical} {Disclosure} {Control} into {Practice}: {The} {ARX} {Data} {Anonymization} {Tool}},
	isbn = {978-3-319-23632-2 978-3-319-23633-9},
	shorttitle = {Putting {Statistical} {Disclosure} {Control} into {Practice}},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-23633-9_6},
	abstract = {The sharing of sensitive personal data has become a core element of biomedical research. To protect privacy, a broad spectrum of techniques must be implemented, including data anonymization. In this article, we present ARX, an anonymization tool for structured data which supports a broad spectrum of methods for statistical disclosure control by providing (1) models for analyzing re-identification risks, (2) risk-based anonymization, (3) syntactic privacy criteria, such as k-anonymity, ℓ-diversity, t-closeness and δ-presence, (4) methods for automated and manual evaluation of data utility, and (5) an intuitive coding model using generalization, suppression and microaggregation. ARX is highly scalable and allows for anonymizing datasets with several millions of records on commodity hardware. Moreover, it offers a comprehensive graphical user interface with wizards and visualizations that guide users through different aspects of the anonymization process. ARX is not just a toolbox, but a fully-fledged application, meaning that all implemented methods have been harmonized and integrated with each other. It is well understood that balancing privacy and data utility requires user feedback. To facilitate this interaction, ARX is highly configurable and provides various methods for exploring the solution space.},
	language = {en},
	urldate = {2017-09-11TZ},
	booktitle = {Medical {Data} {Privacy} {Handbook}},
	publisher = {Springer, Cham},
	author = {Prasser, Fabian and Kohlmayer, Florian},
	year = {2015},
	note = {DOI: 10.1007/978-3-319-23633-9\_6},
	pages = {111--148}
}

@article{tan_artgan:_2017,
	title = {{ArtGAN}: {Artwork} {Synthesis} with {Conditional} {Categorical} {GANs}},
	shorttitle = {{ArtGAN}},
	url = {http://arxiv.org/abs/1702.03410},
	abstract = {This paper proposes an extension to the Generative Adversarial Networks (GANs), namely as ARTGAN to synthetically generate more challenging and complex images such as artwork that have abstract characteristics. This is in contrast to most of the current solutions that focused on generating natural images such as room interiors, birds, flowers and faces. The key innovation of our work is to allow back-propagation of the loss function w.r.t. the labels (randomly assigned to each generated images) to the generator from the discriminator. With the feedback from the label information, the generator is able to learn faster and achieve better generated image quality. Empirically, we show that the proposed ARTGAN is capable to create realistic artwork, as well as generate compelling real world images that globally look natural with clear shape on CIFAR-10.},
	journal = {arXiv:1702.03410 [cs]},
	author = {Tan, Wei Ren and Chan, Chee Seng and Aguirre, Hernan and Tanaka, Kiyoshi},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.03410},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@incollection{sukhbaatar_learning_2016,
	title = {Learning {Multiagent} {Communication} with {Backpropagation}},
	url = {http://papers.nips.cc/paper/6398-learning-multiagent-communication-with-backpropagation.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Sukhbaatar, Sainbayar and szlam, arthur and Fergus, Rob},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	pages = {2244--2252}
}

@article{mirza_conditional_2014,
	title = {Conditional {Generative} {Adversarial} {Nets}},
	url = {http://arxiv.org/abs/1411.1784},
	abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
	journal = {arXiv:1411.1784 [cs, stat]},
	author = {Mirza, Mehdi and Osindero, Simon},
	month = nov,
	year = {2014},
	note = {arXiv: 1411.1784},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Statistics - Machine Learning}
}

@article{radford_unsupervised_2015,
	title = {Unsupervised {Representation} {Learning} with {Deep} {Convolutional} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1511.06434},
	abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	journal = {arXiv:1511.06434 [cs]},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.06434},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning}
}

@incollection{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Nets}},
	url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	pages = {2672--2680}
}

@incollection{wu_survey_2010,
	series = {Advances in {Database} {Systems}},
	title = {A {Survey} of {Privacy}-{Preservation} of {Graphs} and {Social} {Networks}},
	isbn = {978-1-4419-6044-3 978-1-4419-6045-0},
	abstract = {Social networks have received dramatic interest in research and development. In this chapter, we survey the very recent research development on privacypreserving publishing of graphs and social network data. We categorize the state-of-the-art anonymization methods on simple graphs in three main categories: K-anonymity based privacy preservation via edge modification, probabilistic privacy preservation via edge randomization, and privacy preservation via generalization. We then review anonymization methods on rich graphs. We finally discuss challenges and propose new research directions in this area.},
	urldate = {2017-09-05TZ},
	booktitle = {Managing and {Mining} {Graph} {Data}},
	publisher = {Springer, Boston, MA},
	author = {Wu, Xintao and Ying, Xiaowei and Liu, Kun and Chen, Lei},
	year = {2010},
	note = {00102 
DOI: 10.1007/978-1-4419-6045-0\_14},
	pages = {421--453}
}

@inproceedings{aggarwal_general_2008,
	series = {Advances in {Database} {Systems}},
	title = {A {General} {Survey} of {Privacy}-{Preserving} {Data} {Mining} {Models} and {Algorithms}},
	isbn = {978-0-387-70991-8 978-0-387-70992-5},
	urldate = {2017-09-05TZ},
	booktitle = {Privacy-{Preserving} {Data} {Mining}},
	publisher = {Springer, Boston, MA},
	author = {Aggarwal, Charu C. and Yu, Philip S.},
	year = {2008},
	note = {00787 
DOI: 10.1007/978-0-387-70992-5\_2},
	pages = {11--52}
}

@article{halfond_precise_2009,
	title = {Precise interface identification to improve testing and analysis of web applications},
	url = {http://portal.acm.org/citation.cfm?doid=1572272.1572305},
	doi = {10.1145/1572272.1572305},
	abstract = {As web applications become more widespread, sophisticated, and complex, automated quality assurance techniques for such applications have grown in importance. Accurate interface identification is fundamental for many of these techniques, as the components of a web application communicate extensively via implicitly-defined interfaces to generate customized and dynamic content. However, current techniques for identifying web application interfaces can be incomplete or imprecise, which hinders the effectiveness of quality assurance techniques. To address these limitations, we present a new approach for identifying web application interfaces that is based on a specialized form of symbolic execution. In our empirical evaluation, we show that the set of interfaces identified by our approach is more accurate than those identified by other approaches. We also show that this increased accuracy leads to improvements in several important quality assurance techniques for web applications: test-input generation, penetration testing, and invocation verification.},
	journal = {{\textbackslash}ldots on Software testing and analysis},
	author = {Halfond, Wgj and Anand, Saswat and Orso, Alessandro},
	year = {2009},
	note = {00078},
	keywords = {interface identifica-, web application testing},
	pages = {285}
}

@inproceedings{li_preservation_2008,
	address = {New York, NY, USA},
	series = {{SIGMOD} '08},
	title = {Preservation of {Proximity} {Privacy} in {Publishing} {Numerical} {Sensitive} {Data}},
	isbn = {978-1-60558-102-6},
	url = {http://doi.acm.org/10.1145/1376616.1376666},
	doi = {10.1145/1376616.1376666},
	booktitle = {Proceedings of the 2008 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {Li, Jiexing and Tao, Yufei and Xiao, Xiaokui},
	year = {2008},
	note = {00124},
	keywords = {Anonymization, numeric, privacy},
	pages = {473--486}
}

@inproceedings{wong__2006,
	address = {New York, NY, USA},
	series = {{KDD} '06},
	title = {(a, {K})-anonymity: {An} {Enhanced} {K}-anonymity {Model} for {Privacy} {Preserving} {Data} {Publishing}},
	isbn = {978-1-59593-339-3},
	url = {http://doi.acm.org/10.1145/1150402.1150499},
	doi = {10.1145/1150402.1150499},
	abstract = {Privacy preservation is an important issue in the release of data for mining purposes. The k-anonymity model has been introduced for protecting individual identification. Recent studies show that a more sophisticated model is necessary to protect the association of individuals to sensitive information. In this paper, we propose an (α, k)-anonymity model to protect both identifications and relationships to sensitive information in data. We discuss the properties of (α, k)-anonymity model. We prove that the optimal (α, k)-anonymity problem is NP-hard. We first presentan optimal global-recoding method for the (α, k)-anonymity problem. Next we propose a local-recoding algorithm which is more scalable and result in less data distortion. The effectiveness and efficiency are shown by experiments. We also describe how the model can be extended to more general case.},
	booktitle = {Proceedings of the 12th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Wong, Raymond Chi-Wing and Li, Jiuyong and Fu, Ada Wai-Chee and Wang, Ke},
	year = {2006},
	note = {00003 
bibtex: wong\_\_2006},
	keywords = {anonymity, data mining, data publishing, privacy preservation},
	pages = {754--759}
}

@article{rawat_evolving_2013,
	title = {Evolving indigestible codes: {Fuzzing} interpreters with genetic programming},
	doi = {10.1109/CICYBS.2013.6597203},
	journal = {Proceedings of the 2013 IEEE Symposium on Computational Intelligence in Cyber Security, CICS 2013 - 2013 IEEE Symposium Series on Computational Intelligence, SSCI 2013},
	author = {Rawat, Sanjay and Duchene, Fabien and Groz, Roland and Richier, Jean Luc},
	year = {2013},
	note = {00006},
	keywords = {Black-Box, Evolutionary Testing, Fuzzing, Genetic Programming, Interpreter, Memory Corruption},
	pages = {37--39}
}

@inproceedings{su_essence_2006,
	title = {The essence of command injection attacks in web applications},
	publisher = {ACM Press},
	author = {Su, Zhendong},
	year = {2006},
	note = {00582 
bibtex: Su06theessence},
	keywords = {FV, bank account through a, command injection attacks, erated content, for example, gram-, he is using, logs on to his, mars, parsing, runtime verification, they are ubiquitous, web applications, web browser, when a user},
	pages = {372--382}
}

@inproceedings{huang_securing_2004,
	address = {New York, NY, USA},
	series = {{WWW} '04},
	title = {Securing {Web} {Application} {Code} by {Static} {Analysis} and {Runtime} {Protection}},
	isbn = {978-1-58113-844-3},
	url = {http://doi.acm.org/10.1145/988672.988679},
	doi = {10.1145/988672.988679},
	abstract = {Security remains a major roadblock to universal acceptance of the Web for many kinds of transactions, especially since the recent sharp increase in remotely exploitable vulnerabilities have been attributed to Web application bugs. Many verification tools are discovering previously unknown vulnerabilities in legacy C programs, raising hopes that the same success can be achieved with Web applications. In this paper, we describe a sound and holistic approach to ensuring Web application security. Viewing Web application vulnerabilities as a secure information flow problem, we created a lattice-based static analysis algorithm derived from type systems and typestate, and addressed its soundness. During the analysis, sections of code considered vulnerable are instrumented with runtime guards, thus securing Web applications in the absence of user intervention. With sufficient annotations, runtime overhead can be reduced to zero. We also created a tool named.WebSSARI (Web application Security by Static Analysis and Runtime Inspection) to test our algorithm, and used it to verify 230 open-source Web application projects on SourceForge.net, which were selected to represent projects of different maturity, popularity, and scale. 69 contained vulnerabilities. After notifying the developers, 38 acknowledged our findings and stated their plans to provide patches. Our statistics also show that static analysis reduced potential runtime overhead by 98.4\%.},
	booktitle = {Proceedings of the 13th {International} {Conference} on {World} {Wide} {Web}},
	publisher = {ACM},
	author = {Huang, Yao-Wen and Yu, Fang and Hang, Christian and Tsai, Chung-Hung and Lee, Der-Tsai and Kuo, Sy-Yen},
	year = {2004},
	note = {00666},
	keywords = {Web application security, information flow, information flow, noninterference, noninterference, program security, program security, security vulnerabilities, security vulnerabilities, type systems, type systems, verification, verification, web application security},
	pages = {40--52}
}

@inproceedings{wassermann_dynamic_2008,
	address = {New York, NY, USA},
	series = {{ISSTA} '08},
	title = {Dynamic {Test} {Input} {Generation} for {Web} {Applications}},
	isbn = {978-1-60558-050-0},
	url = {http://doi.acm.org/10.1145/1390630.1390661},
	doi = {10.1145/1390630.1390661},
	abstract = {Web applications routinely handle sensitive data, and many people rely on them to support various daily activities, so errors can have severe and broad-reaching consequences. Unlike most desktop applications, many web applications are written in scripting languages, such as PHP. The dynamic features commonly supported by these languages significantly inhibit static analysis and existing static analysis of these languages can fail to produce meaningful results on realworld web applications. Automated test input generation using the concolic testing framework has proven useful for finding bugs and improving test coverage on C and Java programs, which generally emphasize numeric values and pointer-based data structures. However, scripting languages, such as PHP, promote a style of programming for developing web applications that emphasizes string values, objects, and arrays. In this paper, we propose an automated input test generation algorithm that uses runtime values to analyze dynamic code, models the semantics of string operations, and handles operations whose argument and return values may not share a common type. As in the standard concolic testing framework, our algorithm gathers constraints during symbolic execution. Our algorithm resolves constraints over multiple types by considering each variable instance individually, so that it only needs to invert each operation. By recording constraints selectively, our implementation successfully finds bugs in real-world web applications which state-of-the-art static analysis tools fail to analyze.},
	urldate = {2016-10-04TZ},
	booktitle = {Proceedings of the 2008 {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {ACM},
	author = {Wassermann, Gary and Yu, Dachuan and Chander, Ajay and Dhurjati, Dinakar and Inamura, Hiroshi and Su, Zhendong},
	year = {2008},
	note = {00170},
	keywords = {Applications, Automated tests, Computer softwar, Computer software, Concol, DN, READ, Software testing, automatic test generation, automatic test generation, concolic testing, concolic testing, directed random testing, directed random testing, web applications, web applications},
	pages = {249--260}
}

@inproceedings{nadji_document_2009,
	title = {Document {Structure} {Integrity}: {A} {Robust} {Basis} for {Cross}-site {Scripting} {Defense}.},
	volume = {2009},
	shorttitle = {Document {Structure} {Integrity}},
	url = {http://www.comp.nus.edu.sg/~prateeks/papers/dsi-ndss09.pdf},
	urldate = {2016-03-13TZ},
	booktitle = {{NDSS}},
	author = {Nadji, Yacin and Saxena, Prateek and Song, Dawn},
	year = {2009},
	note = {00204},
	pages = {20}
}

@inproceedings{pietraszek_defending_2006,
	address = {Berlin, Heidelberg},
	series = {{RAID}'05},
	title = {Defending {Against} {Injection} {Attacks} {Through} {Context}-sensitive {String} {Evaluation}},
	isbn = {978-3-540-31778-4},
	url = {http://dx.doi.org/10.1007/11663812_7},
	doi = {10.1007/11663812_7},
	abstract = {Injection vulnerabilities pose a major threat to application-level security. Some of the more common types are SQL injection, cross-site scripting and shell injection vulnerabilities. Existing methods for defending against injection attacks, that is, attacks exploiting these vulnerabilities, rely heavily on the application developers and are therefore error-prone. In this paper we introduce CSSE, a method to detect and prevent injection attacks. CSSE works by addressing the root cause why such attacks can succeed, namely the ad-hoc serialization of user-provided input. It provides a platform-enforced separation of channels, using a combination of assignment of metadata to user-provided input, metadata-preserving string operations and context-sensitive string evaluation. CSSE requires neither application developer interaction nor application source code modifications. Since only changes to the underlying platform are needed, it effectively shifts the burden of implementing countermeasures against injection attacks from the many application developers to the small team of security-savvy platform developers. Our method is effective against most types of injection attacks, and we show that it is also less error-prone than other solutions proposed so far. We have developed a prototype CSSE implementation for PHP, a platform that is particularly prone to these vulnerabilities. We used our prototype with phpBB, a well-known bulletin-board application, to validate our method. CSSE detected and prevented all the SQL injection attacks we could reproduce and incurred only reasonable run-time overhead.},
	urldate = {2016-03-11TZ},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Recent} {Advances} in {Intrusion} {Detection}},
	publisher = {Springer-Verlag},
	author = {Pietraszek, Tadeusz and Berghe, Chris Vanden},
	year = {2006},
	note = {00407},
	keywords = {FV, Injection attacks, Internal sensors, Intrusion prevention, PHP, PHP, ST, injection attacks, internal sensors, intrusion prevention, web applications, web applications},
	pages = {124--145}
}

@inproceedings{tripp_taj:_2009,
	address = {New York, NY, USA},
	series = {{PLDI} '09},
	title = {{TAJ}: {Effective} {Taint} {Analysis} of {Web} {Applications}},
	isbn = {978-1-60558-392-1},
	shorttitle = {{TAJ}},
	url = {http://doi.acm.org/10.1145/1542476.1542486},
	doi = {10.1145/1542476.1542486},
	abstract = {Taint analysis, a form of information-flow analysis, establishes whether values from untrusted methods and parameters may flow into security-sensitive operations. Taint analysis can detect many common vulnerabilities in Web applications, and so has attracted much attention from both the research community and industry. However, most static taint-analysis tools do not address critical requirements for an industrial-strength tool. Specifically, an industrial-strength tool must scale to large industrial Web applications, model essential Web-application code artifacts, and generate consumable reports for a wide range of attack vectors. We have designed and implemented a static Taint Analysis for Java (TAJ) that meets the requirements of industry-level applications. TAJ can analyze applications of virtually any size, as it employs a set of techniques designed to produce useful answers given limited time and space. TAJ addresses a wide variety of attack vectors, with techniques to handle reflective calls, flow through containers, nested taint, and issues in generating useful reports. This paper provides a description of the algorithms comprising TAJ, evaluates TAJ against production-level benchmarks, and compares it with alternative solutions.},
	urldate = {2016-09-29TZ},
	booktitle = {Proceedings of the 30th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Tripp, Omer and Pistoia, Marco and Fink, Stephen J. and Sridharan, Manu and Weisman, Omri},
	year = {2009},
	note = {00267},
	keywords = {Java, READ, ST, Security, Security, Static analysis, java, program analysis, program analysis, slicing, slicing, static analysis, taint analysis, taint analysis, web application, web application},
	pages = {87--97}
}

@article{tonella_web_nodate,
	title = {Web {Application} {Slicing} in {Presence} of {Dynamic} {Code} {Generation}},
	volume = {12},
	issn = {0928-8910, 1573-7535},
	url = {http://link.springer.com/article/10.1007/s10515-005-6208-8},
	doi = {10.1007/s10515-005-6208-8},
	abstract = {The computation of program slices on Web applications may be useful during debugging, when the amount of code to be inspected can be reduced, and during understanding, since the search for a given functionality can be better focused. The system dependence graph is an appropriate data structure for slice computation, in that it explicitly represents all dependences that have to be taken into account in slice determination.Construction of the system dependence graph for Web applications is complicated by the presence of dynamically generated code. In fact, a Web application builds the HTML code to be transmitted to the browser at run time. Knowledge of such code is essential for slicing. In this paper an algorithm for the static approximation of the dynamically generated HTML code is proposed. The concatenations of constant strings and variables are propagated according to special purpose flow equations, allowing the estimation of the generated code and the refinement of the system dependence graph.},
	language = {en},
	number = {2},
	urldate = {2016-10-03TZ},
	journal = {Automated Software Engineering},
	author = {Tonella, Paolo and Ricca, Filippo},
	note = {00026},
	keywords = {program slicing, system dependence graph, web applications},
	pages = {259--288}
}

@inproceedings{zhang_time-aware_2009,
	address = {Chicago, IL, USA},
	series = {{ISSTA} '09},
	title = {Time-aware {Test}-case {Prioritization} {Using} {Integer} {Linear} {Programming}},
	isbn = {978-1-60558-338-9},
	url = {http://doi.acm.org/10.1145/1572272.1572297},
	doi = {10.1145/1572272.1572297},
	booktitle = {Proceedings of the {Eighteenth} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {ACM},
	author = {Zhang, Lu and Hou, Shan-Shan and Guo, Chao and Xie, Tao and Mei, Hong},
	year = {2009},
	note = {00111 
bibtex: Zhang:2009:TTP:1572272.1572297 
bibtex[numpages=12;acmid=1572297]},
	keywords = {Zotero Import (Mar 12), Zotero Import (Mar 12)/My Library, integer linear programming, integer linear programming, test-case prioritization, test-case prioritization},
	pages = {213--224}
}

@inproceedings{heimdahl_test-suite_2004,
	title = {Test-suite reduction for model based tests: {Effects} on test quality and implications for testing},
	booktitle = {Proceedings of the 19th {IEEE} international conference on {Automated} software engineering},
	publisher = {IEEE Computer Society},
	author = {Heimdahl, Mats PE and George, Devaraj},
	year = {2004},
	note = {00119 
bibtex: heimdahl2004test},
	keywords = {Zotero Import (Mar 12), Zotero Import (Mar 12)/My Library, Zotero Import (Mar 12)/My Library/UNCC/MSR-Fellowship},
	pages = {176--185}
}

@inproceedings{shoshitaishvili_sok:state_2016,
	title = {{SOK}:({State} of) {The} {Art} of {War}: {Offensive} {Techniques} in {Binary} {Analysis}},
	booktitle = {2016 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	publisher = {IEEE},
	author = {Shoshitaishvili, Yan and Wang, Ruoyu and Salls, Christopher and Stephens, Nick and Polino, Mario and Dutcher, Andrew and Grosen, John and Feng, Siji and Hauser, Christophe and Kruegel, Christopher and {others}},
	year = {2016},
	note = {00000 
bibtex: shoshitaishvili2016sok},
	pages = {138--157}
}

@article{deepa_securing_2016,
	title = {Securing web applications from injection and logic vulnerabilities: {Approaches} and challenges},
	volume = {74},
	issn = {0950-5849},
	shorttitle = {Securing web applications from injection and logic vulnerabilities},
	url = {http://www.sciencedirect.com/science/article/pii/S0950584916300234},
	doi = {10.1016/j.infsof.2016.02.005},
	abstract = {Context: Web applications are trusted by billions of users for performing day-to-day activities. Accessibility, availability and omnipresence of web applications have made them a prime target for attackers. A simple implementation flaw in the application could allow an attacker to steal sensitive information and perform adversary actions, and hence it is important to secure web applications from attacks. Defensive mechanisms for securing web applications from the flaws have received attention from both academia and industry. Objective: The objective of this literature review is to summarize the current state of the art for securing web applications from major flaws such as injection and logic flaws. Though different kinds of injection flaws exist, the scope is restricted to SQL Injection (SQLI) and Cross-site scripting (XSS), since they are rated as the top most threats by different security consortiums. Method: The relevant articles recently published are identified from well-known digital libraries, and a total of 86 primary studies are considered. A total of 17 articles related to SQLI, 35 related to XSS and 34 related to logic flaws are discussed. Results: The articles are categorized based on the phase of software development life cycle where the defense mechanism is put into place. Most of the articles focus on detecting the flaws and preventing the attacks against web applications. Conclusion: Even though various approaches are available for securing web applications from SQLI and XSS, they are still prevalent due to their impact and severity. Logic flaws are gaining attention of the researchers since they violate the business specifications of applications. There is no single solution to mitigate all the flaws. More research is needed in the area of fixing flaws in the source code of applications.},
	journal = {Information and Software Technology},
	author = {Deepa, G. and Thilagam, P. Santhi},
	month = jun,
	year = {2016},
	note = {00007},
	keywords = {Application logic vulnerabilities, Business logic vulnerabilities, Injection flaws, SQL injection, Web application security, cross-site scripting},
	pages = {160--180}
}

@inproceedings{mcminn_search-based_2012,
	title = {Search-{Based} {Test} {Input} {Generation} for {String} {Data} {Types} {Using} the {Results} of {Web} {Queries}},
	doi = {10.1109/ICST.2012.94},
	abstract = {Generating realistic, branch-covering string inputs is a challenging problem, due to the diverse and complex types of real-world data that are naturally encodable as strings, for example resource locators, dates of different localised formats, international banking codes, and national identity numbers. This paper presents an approach in which examples of inputs are sought from the Internet by reformulating program identifiers into web queries. The resultant URLs are downloaded, split into tokens, and used to augment and seed a search-based test data generation technique. The use of the Internet as part of test input generation has two key advantages. Firstly, web pages are a rich source of valid inputs for various types of string data that may be used to improve test coverage. Secondly, the web pages tend to contain realistic, human-readable values, which are invaluable when test cases need manual confirmation due to the lack of an automated oracle. An empirical evaluation of the approach is presented, involving string input validation code from 10 open source projects. Well-formed, valid string inputs were retrieved from the web for 96\% of the different string types analysed. Using the approach, coverage was improved for 75\% of the Java classes studied by an average increase of 14\%.},
	booktitle = {Verification and {Validation} 2012 {IEEE} {Fifth} {International} {Conference} on {Software} {Testing}},
	author = {McMinn, P. and Shahbaz, M. and Stevenson, M.},
	month = apr,
	year = {2012},
	note = {00046},
	keywords = {Automatic test data generation, Electronic mail, Internet, Java, Search engines, Search problems, Testing, URL, Unified Modeling Language, Web page, Web pages, Web query, Web services, automated oracle, automatic test pattern generation, branch covering string, information retrieval, open source project, program identifier, program testing, public domain software, query processing, search based testing, search-based test data generation, string data type, string inputs, test coverage, web queries},
	pages = {141--150}
}

@inproceedings{shin_recognizing_2015,
	title = {Recognizing functions in binaries with neural networks},
	booktitle = {24th {USENIX} {Security} {Symposium} ({USENIX} {Security} 15)},
	author = {Shin, Eui Chul Richard and Song, Dawn and Moazzezi, Reza},
	year = {2015},
	note = {00034 
bibtex: shin2015recognizing},
	pages = {611--626}
}

@article{zhai_prioritizing_2014,
	title = {Prioritizing test cases for regression testing of location-based services: {Metrics}, techniques, and case study},
	volume = {7},
	number = {1},
	journal = {IEEE Transactions on Services Computing},
	author = {Zhai, Ke and Jiang, Bo and Chan, WK},
	year = {2014},
	note = {00030 
bibtex: zhai2014prioritizing},
	keywords = {Zotero Import (Mar 12), Zotero Import (Mar 12)/My Library, Zotero Import (Mar 12)/My Library/UNCC/MSR-Fellowship},
	pages = {54--67}
}

@inproceedings{meyerson_complexity_2004,
	address = {New York, NY, USA},
	series = {{PODS} '04},
	title = {On the {Complexity} of {Optimal} {K}-anonymity},
	isbn = {978-1-58113-858-0},
	url = {http://doi.acm.org/10.1145/1055558.1055591},
	doi = {10.1145/1055558.1055591},
	abstract = {The technique of k-anonymization has been proposed in the literature as an alternative way to release public information, while ensuring both data privacy and data integrity. We prove that two general versions of optimal k-anonymization of relations are NP-hard, including the suppression version which amounts to choosing a minimum number of entries to delete from the relation. We also present a polynomial time algorithm for optimal k-anonymity that achieves an approximation ratio independent of the size of the database, when k is constant. In particular, it is a O(k log k)-approximation where the constant in the big-O is no more than 4, However, the runtime of the algorithm is exponential in k. A slightly more clever algorithm removes this condition, but is a O(k log m)-approximation, where m is the degree of the relation. We believe this algorithm could potentially be quite fast in practice.},
	booktitle = {Proceedings of the {Twenty}-third {ACM} {SIGMOD}-{SIGACT}-{SIGART} {Symposium} on {Principles} of {Database} {Systems}},
	publisher = {ACM},
	author = {Meyerson, Adam and Williams, Ryan},
	year = {2004},
	note = {00868},
	pages = {223--228}
}

@inproceedings{korel_model-based_2007,
	title = {Model-based test prioritization heuristic methods and their evaluation},
	booktitle = {Proceedings of the 3rd international workshop on {Advances} in model-based testing},
	publisher = {ACM},
	author = {Korel, Bogdan and Koutsogiannakis, George and Tahat, Luay H},
	year = {2007},
	note = {00075 
bibtex: Korel2007-bq},
	keywords = {Zotero Import (Mar 12), Zotero Import (Mar 12)/My Library, Zotero Import (Mar 12)/My Library/UNCC/MSR-Fellowship},
	pages = {34--43}
}

@inproceedings{lindorfer_marvin:_2015,
	title = {{MARVIN}: {Efficient} and {Comprehensive} {Mobile} {App} {Classification} through {Static} and {Dynamic} {Analysis}},
	volume = {2},
	shorttitle = {{MARVIN}},
	doi = {10.1109/COMPSAC.2015.103},
	abstract = {Android dominates the smartphone operating system market and consequently has attracted the attention of malware authors and researchers alike. Despite the considerable number of proposed malware analysis systems, comprehensive and practical malware analysis solutions are scarce and often short-lived. Systems relying on static analysis alone struggle with increasingly popular obfuscation and dynamic code loading techniques, while purely dynamic analysis systems are prone to analysis evasion. We present MARVIN, a system that combines static with dynamic analysis and which leverages machine learning techniques to assess the risk associated with unknown Android apps in the form of a malice score. MARVIN performs static and dynamic analysis, both off-device, to represent properties and behavioral aspects of an app through a rich and comprehensive feature set. In our evaluation on the largest Android malware classification data set to date, comprised of over 135,000 Android apps and 15,000 malware samples, MARVIN correctly classifies 98.24\% of malicious apps with less than 0.04\% false positives. We further estimate the necessary retraining interval to maintain the detection performance and demonstrate the long-term practicality of our approach.},
	booktitle = {2015 {IEEE} 39th {Annual} {Computer} {Software} and {Applications} {Conference}},
	author = {Lindorfer, M. and Neugschwandtner, M. and Platzer, C.},
	month = jul,
	year = {2015},
	note = {00026},
	keywords = {Android (operating system), Android apps, Android malware classification data, Androids, Feature extraction, Google, Humanoid robots, MARVIN system, Mobile communication, Static analysis, Training, analysis evasion, app behavioral aspects, classification, detection performance, dynamic analysis systems, dynamic code loading techniques, invasive software, learning (artificial intelligence), machine learning techniques, malice score, malicious apps, malware, malware analysis, malware analysis solutions, malware analysis systems, mobile app classification, mobile computing, mobile security, obfuscation, pattern classification, program diagnostics, smart phones, smartphone operating system market},
	pages = {422--433}
}

@article{sun_internet_2016,
	title = {Internet of {Things} and {Big} {Data} {Analytics} for {Smart} and {Connected} {Communities}},
	volume = {4},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2016.2529723},
	abstract = {This paper promotes the concept of smart and connected communities SCC, which is evolving from the concept of smart cities. SCC are envisioned to address synergistically the needs of remembering the past (preservation and revitalization), the needs of living in the present (livability), and the needs of planning for the future (attainability). Therefore, the vision of SCC is to improve livability, preservation, revitalization, and attainability of a community. The goal of building SCC for a community is to live in the present, plan for the future, and remember the past. We argue that Internet of Things (IoT) has the potential to provide a ubiquitous network of connected devices and smart sensors for SCC, and big data analytics has the potential to enable the move from IoT to real-time control desired for SCC. We highlight mobile crowdsensing and cyber-physical cloud computing as two most important IoT technologies in promoting SCC. As a case study, we present TreSight, which integrates IoT and big data analytics for smart tourism and sustainable cultural heritage in the city of Trento, Italy.},
	journal = {IEEE Access},
	author = {Sun, Y. and Song, H. and Jara, A. J. and Bie, R.},
	year = {2016},
	note = {00054},
	keywords = {Big Data, Cultural differences, Data analytics, Economics, Internet of Things, IoT, Italy, SCC, Smart and Connected Communities, Sustainable Cultural Heritage, Sustainable development, TreSight, Trento, Urban areas, big data analytics, cultural heritage, cyber-physical cloud computing, data analysis, mobile computing, mobile crowdsensing, sensors, smart cities, smart sensors, smart tourism, travel industry, ubiquitous network},
	pages = {766--773}
}

@incollection{felderer_integrating_2012,
	series = {Lecture {Notes} in {Business} {Information} {Processing}},
	title = {Integrating {Manual} and {Automatic} {Risk} {Assessment} for {Risk}-{Based} {Testing}},
	copyright = {©2012 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-642-27212-7 978-3-642-27213-4},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-27213-4_11},
	abstract = {In this paper we define a model-based risk assessment procedure that integrates automatic risk assessment by static analysis, semi-automatic risk assessment and guided manual risk assessment. In this process probability and impact criteria are determined by metrics which are combined to estimate the risk of specific system development artifacts. The risk values are propagated to the assigned test cases providing a prioritization of test cases. This supports to optimize the allocation of limited testing time and budget in a risk-based testing methodology. Therefore, we embed our risk assessment process into a generic risk-based testing methodology. The calculation of probability and impact metrics is based on system and requirements artifacts which are formalized as model elements. Additional time metrics consider the temporal development of the system under test and take for instance the bug and version history of the system into account. The risk assessment procedure integrates several stakeholders and is explained by a running example.},
	language = {en},
	number = {94},
	urldate = {2016-09-23TZ},
	booktitle = {Software {Quality}. {Process} {Automation} in {Software} {Development}},
	publisher = {Springer Berlin Heidelberg},
	author = {Felderer, Michael and Haisjackl, Christian and Breu, Ruth and Motz, Johannes},
	editor = {Biffl, Stefan and Winkler, Dietmar and Bergsmann, Johannes},
	month = jan,
	year = {2012},
	note = {00030 
DOI: 10.1007/978-3-642-27213-4\_11},
	keywords = {Management of Computing and Information Systems, Project Management, Software Engineering},
	pages = {159--180}
}

@misc{noauthor_html5_nodate,
	title = {{HTML}5 {Security} {Cheatsheet}},
	url = {https://html5sec.org/},
	urldate = {2016-10-10TZ},
	note = {00006}
}

@misc{noauthor_grammar_nodate,
	title = {Grammar of {CSS} 2.1},
	url = {https://www.w3.org/TR/CSS2/grammar.html},
	urldate = {2016-11-23TZ},
	note = {00004}
}

@inproceedings{xie_gamifying_2015,
	title = {Gamifying software security education and training via secure coding duels in code hunt},
	doi = {http://dx.doi.org/10.1145/2746194.2746220},
	booktitle = {Proceedings of the 2015 {Symposium} and {Bootcamp} on the {Science} of {Security}},
	publisher = {ACM},
	author = {Xie, Tao and Bishop, Judith and Tillmann, Nikolai and de Halleux, Jonathan},
	year = {2015},
	note = {00001 
bibtex: xie2015gamifying},
	pages = {26}
}

@inproceedings{shoshitaishvili_firmalice-automatic_2015,
	title = {Firmalice-{Automatic} {Detection} of {Authentication} {Bypass} {Vulnerabilities} in {Binary} {Firmware}.},
	booktitle = {{NDSS}},
	author = {Shoshitaishvili, Yan and Wang, Ruoyu and Hauser, Christophe and Kruegel, Christopher and Vigna, Giovanni},
	year = {2015},
	note = {00039 
bibtex: shoshitaishvili2015firmalice}
}

@inproceedings{zhang_empirically_2014,
	address = {New York, NY, USA},
	series = {{ISSTA} 2014},
	title = {Empirically {Revisiting} the {Test} {Independence} {Assumption}},
	isbn = {978-1-4503-2645-2},
	url = {http://doi.acm.org/10.1145/2610384.2610404},
	doi = {10.1145/2610384.2610404},
	abstract = {In a test suite, all the test cases should be independent: no test should affect any other test’s result, and running the tests in any order should produce the same test results. Techniques such as test prioritization generally assume that the tests in a suite are independent. Test dependence is a little-studied phenomenon. This paper presents five results related to test dependence.   First, we characterize the test dependence that arises in practice. We studied 96 real-world dependent tests from 5 issue tracking systems. Our study shows that test dependence can be hard for programmers to identify. It also shows that test dependence can cause non-trivial consequences, such as masking program faults and leading to spurious bug reports.   Second, we formally define test dependence in terms of test suites as ordered sequences of tests along with explicit environments in which these tests are executed. We formulate the problem of detecting dependent tests and prove that a useful special case is NP-complete.   Third, guided by the study of real-world dependent tests, we propose and compare four algorithms to detect dependent tests in a test suite.   Fourth, we applied our dependent test detection algorithms to 4 real-world programs and found dependent tests in each human-written and automatically-generated test suite.   Fifth, we empirically assessed the impact of dependent tests on five test prioritization techniques. Dependent tests affect the output of all five techniques; that is, the reordered suite fails even though the original suite did not.},
	urldate = {2016-10-07TZ},
	booktitle = {Proceedings of the 2014 {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {ACM},
	author = {Zhang, Sai and Jalali, Darioush and Wuttke, Jochen and Muşlu, Kıvanç and Lam, Wing and Ernst, Michael D. and Notkin, David},
	year = {2014},
	note = {00045},
	keywords = {Test dependence, detection algorithms, empirical studies},
	pages = {385--396}
}

@misc{noauthor_ecmascript_nodate,
	title = {{ECMAScript} {Language} {Specification} - {ECMA}-262 {Edition} 5.1},
	url = {http://www.ecma-international.org/ecma-262/5.1/},
	urldate = {2016-08-04TZ},
	note = {00000}
}

@inproceedings{hale_testbed_2015,
	title = {A {Testbed} and {Process} for {Analyzing} {Attack} {Vectors} and {Vulnerabilities} in {Hybrid} {Mobile} {Apps} {Connected} to {Restful} {Web} {Services}},
	doi = {10.1109/SERVICES.2015.35},
	abstract = {Web traffic is increasingly trending towards mobile devices driving developers to tailor web content to small screens and customize web apps using mobile-only capabilities such as geo-location, accelerometers, offline storage, and camera features. Hybrid apps provide a cross-platform, device independent, means for developers to utilize these features. They work by wrapping web-based code, i.e., HTML5, CSS, and JavaScript, in thin native containers that expose device features. This design pattern encourages re-use of existing code, reduces development time, and leverages existing web development talent that doesn't depend on platform specific languages. Despite these advantages, the newness of hybrid apps raises new security challenges associated with integrating code designed for a web browser with features native to a mobile device. This paper explores these security concerns and defines three forms of attack that can specifically target and exploit hybrid apps connected to web services. Contributions of the paper include a high level process for discovering hybrid app attacks and vulnerabilities, definitions of emerging hybrid attack vectors, and a test bed platform for analyzing vulnerabilities. As an evaluation, hybrid attacks are analyzed in the test bed showing that it provides insight into vulnerabilities and helps assess risk.},
	booktitle = {2015 {IEEE} {World} {Congress} on {Services}},
	author = {Hale, M. L. and Hanson, S.},
	month = jun,
	year = {2015},
	note = {00003},
	keywords = {Accelerometers, Browsers, Cameras, Mobile applications, Mobile communication, RESTful Web service, Security, Software Engineering, Web development, Web services, attack vector analysis, attack vectors, hybrid mobile app, hybrid mobile application, mobile computing, mobile device, program testing, security of data, smart phones, test bed platform, thin native containers, vulnerabilities, vulnerability analysis, web browser},
	pages = {181--188}
}

@article{younis_assessing_2016,
	title = {Assessing vulnerability exploitability risk using software properties},
	volume = {24},
	issn = {0963-9314, 1573-1367},
	url = {https://link.springer.com/article/10.1007/s11219-015-9274-6},
	doi = {10.1007/s11219-015-9274-6},
	abstract = {Attacks on computer systems are now attracting increased attention. While the current trends in software vulnerability discovery indicate that the number of newly discovered vulnerabilities continues to be significant, the time between the public disclosure of vulnerabilities and the release of an automated exploit is shrinking. Thus, assessing the vulnerability exploitability risk is critical because this allows decision-makers to prioritize among vulnerabilities, allocate resources to patch and protect systems from these vulnerabilities, and choose between alternatives. Common vulnerability scoring system (CVSS) metrics have become the de facto standard for assessing the severity of vulnerabilities. However, the CVSS exploitability measures assign subjective values based on the views of experts. Two of the factors in CVSS, Access Vector and Authentication, are the same for almost all vulnerabilities. CVSS does not specify how the third factor, Access Complexity, is measured, and hence it is unknown whether it considers software properties as a factor. In this work, we introduce a novel measure, Structural Severity, which is based on software properties, namely attack entry points, vulnerability location, the presence of the dangerous system calls, and reachability analysis. These properties represent metrics that can be objectively derived from attack surface analysis, vulnerability analysis, and exploitation analysis. To illustrate the proposed approach, 25 reported vulnerabilities of Apache HTTP server and 86 reported vulnerabilities of Linux Kernel have been examined at the source code level. The results show that the proposed approach, which uses more detailed information, can objectively measure the risk of vulnerability exploitability and results can be different from the CVSS base scores.},
	language = {en},
	number = {1},
	urldate = {2017-04-16TZ},
	journal = {Software Quality Journal},
	author = {Younis, Awad and Malaiya, Yashwant K. and Ray, Indrajit},
	month = mar,
	year = {2016},
	note = {00011},
	pages = {159--202}
}

@article{anand_orchestrated_2013,
	title = {An orchestrated survey of methodologies for automated software test case generation},
	volume = {86},
	issn = {0164-1212},
	url = {http://www.sciencedirect.com/science/article/pii/S0164121213000563},
	doi = {10.1016/j.jss.2013.02.061},
	abstract = {Test case generation is among the most labour-intensive tasks in software testing. It also has a strong impact on the effectiveness and efficiency of software testing. For these reasons, it has been one of the most active research topics in software testing for several decades, resulting in many different approaches and tools. This paper presents an orchestrated survey of the most prominent techniques for automatic generation of software test cases, reviewed in self-standing sections. The techniques presented include: (a) structural testing using symbolic execution, (b) model-based testing, (c) combinatorial testing, (d) random testing and its variant of adaptive random testing, and (e) search-based testing. Each section is contributed by world-renowned active researchers on the technique, and briefly covers the basic ideas underlying the method, the current state of the art, a discussion of the open research problems, and a perspective of the future development of the approach. As a whole, the paper aims at giving an introductory, up-to-date and (relatively) short overview of research in automatic test case generation, while ensuring a comprehensive and authoritative treatment.},
	number = {8},
	urldate = {2016-10-04TZ},
	journal = {Journal of Systems and Software},
	author = {Anand, Saswat and Burke, Edmund K. and Chen, Tsong Yueh and Clark, John and Cohen, Myra B. and Grieskamp, Wolfgang and Harman, Mark and Harrold, Mary Jean and McMinn, Phil and Bertolino, Antonia and Jenny Li, J. and Zhu, Hong},
	month = aug,
	year = {2013},
	note = {00172},
	keywords = {Adaptive random testing, Combinatorial testing, Combinatorial testing, Model-based testing, Orchestrated survey, Search-based software testing, Software testing, Software testing, Symbolic execution, Test automation, Test case generation, adaptive random testing, model-based testing, orchestrated survey, search-based software testing, symbolic execution, test automation, test case generation},
	pages = {1978--2001}
}

@inproceedings{saxena_scriptgard_2011,
	title = {{ScriptGard} : {Automatic} {Context}-{Sensitive} {Sanitization} for {Large}-{Scale} {Legacy} {Web} {Applications} {Categories} and {Subject} {Descriptors}},
	isbn = {978-1-4503-0948-6},
	url = {http://delivery.acm.org/10.1145/2050000/2046776/p601-saxena.pdf?ip=119.40.120.194&acc=ACTIVE SERVICE&key=C2716FEBFA981EF1C103103C99BB4CAC18BDBC9CDB7D3111&CFID=336324593&CFTOKEN=12977014&__acm__=1370601434_ab55dcef94ceef70f833ee9e6e1cf862 http://dl.acm.org},
	doi = {10.1145/2046707.2046776},
	abstract = {We empirically analyzed sanitizer use in a shipping web ap- plication with over 400,000 lines of code and over 23,244 methods, the largest empirical analysis of sanitizer use of which we are aware. Our analysis reveals two novel classes of errors: context-mismatched sanitization and inconsistent multiple sanitization. Both of these arise not because sanitizers are incorrectly implemented, but rather because they are not placed in code correctly. Much of the work on crosssite scripting detection to date has focused on finding missing sanitizers in programs of average size. In large legacy applications, other sanitization issues leading to cross-site scripting emerge. To address these errors, we propose ScriptGard, a system for ASP.NET applications which can detect and repair the incorrect placement of sanitizers. ScriptGard serves both as a testing aid to developers as well as a runtime mitigation technique. While mitigations for cross site scripting attacks have seen intense prior research, we consider both server and browser context, none of them achieve the same degree of precision, and many other mitigation techniques require major changes to server side code or to browsers. Our approach, in contrast, can be incrementally retrofitted to legacy systems with no changes to the source code and no browser changes. With our optimizations, when used for mitigation, ScriptGard incurs virtually no statistically significant overhead.},
	booktitle = {Proceedings of the 18th {ACM} conference on {Computer} and communications security - {CCS} '11},
	author = {Saxena, Prateek and Molnar, David and Livshits, Benjamin},
	year = {2011},
	note = {00000},
	keywords = {READ, ST, VP, cross-site scripting, runtime analysis, web applications},
	pages = {601}
}

@article{medeiros_detecting_2016,
	title = {Detecting and {Removing} {Web} {Application} {Vulnerabilities} with {Static} {Analysis} and {Data} {Mining}},
	volume = {65},
	issn = {0018-9529},
	doi = {10.1109/TR.2015.2457411},
	abstract = {Although a large research effort on web application security has been going on for more than a decade, the security of web applications continues to be a challenging problem. An important part of that problem derives from vulnerable source code, often written in unsafe languages like PHP. Source code static analysis tools are a solution to find vulnerabilities, but they tend to generate false positives, and require considerable effort for programmers to manually fix the code. We explore the use of a combination of methods to discover vulnerabilities in source code with fewer false positives. We combine taint analysis, which finds candidate vulnerabilities, with data mining, to predict the existence of false positives. This approach brings together two approaches that are apparently orthogonal: humans coding the knowledge about vulnerabilities (for taint analysis), joined with the seemingly orthogonal approach of automatically obtaining that knowledge (with machine learning, for data mining). Given this enhanced form of detection, we propose doing automatic code correction by inserting fixes in the source code. Our approach was implemented in the WAP tool, and an experimental evaluation was performed with a large set of PHP applications. Our tool found 388 vulnerabilities in 1.4 million lines of code. Its accuracy and precision were approximately 5\% better than PhpMinerII's and 45\% better than Pixy's.},
	number = {1},
	journal = {IEEE Transactions on Reliability},
	author = {Medeiros, I. and Neves, N. and Correia, M.},
	month = mar,
	year = {2016},
	note = {00012},
	keywords = {Accuracy, Automatic protection, Computer architecture, Encoding, Internet, PHP applications, Security, Testing, WAP tool, Web application security, Web application vulnerabilities detection, Web application vulnerabilities removal, Wireless application protocol, automatic code correction, data mining, false positives, input validation vulnerabilities, learning (artificial intelligence), machine learning, orthogonal approach, program diagnostics, security of data, software security, source code (software), source code static analysis, source code static analysis tools, taint analysis, vulnerable source code, web applications},
	pages = {54--69}
}

@article{liu_software_2012,
	title = {Software vulnerability discovery techniques: {A} survey},
	doi = {10.1109/MINES.2012.202},
	journal = {Proceedings - 2012 4th International Conference on Multimedia and Security, MINES 2012},
	author = {Liu, Bingchang and Shi, Liang and Cai, Zhuhua and Li, Min},
	year = {2012},
	note = {00043},
	keywords = {Fuzzing, Software static analysis, Vulnerability, Vulnerability discovery model, penetration testing},
	pages = {152--156}
}

@article{shahbaz_automatic_2014,
	title = {Automatic generation of valid and invalid test data for string validation routines using web searches and regular expressions},
	volume = {97},
	issn = {01676423},
	url = {http://www.sciencedirect.com/science/article/pii/S0167642314001725},
	doi = {10.1016/j.scico.2014.04.008},
	abstract = {Classic approaches to automatic input data generation are usually driven by the goal of obtaining program coverage and the need to solve or find solutions to path constraints to achieve this. As inputs are generated with respect to the structure of the code, they can be ineffective, difficult for humans to read, and unsuitable for testing missing implementation. Furthermore, these approaches have known limitations when handling constraints that involve operations with string data types. This paper presents a novel approach for generating string test data for string validation routines, by harnessing the Internet. The technique uses program identifiers to construct web search queries for regular expressions that validate the format of a string type (such as an email address). It then performs further web searches for strings that match the regular expressions, producing examples of test cases that are both valid and realistic. Following this, our technique mutates the regular expressions to drive the search for invalid strings, and the production of test inputs that should be rejected by the validation routine. The paper presents the results of an empirical study evaluating our approach. The study was conducted on 24 string input validation routines collected from 10 open source projects. While dynamic symbolic execution and search-based testing approaches were only able to generate a very low number of values successfully, our approach generated values with an accuracy of 34\% on average for the case of valid strings, and 99\% on average for the case of invalid strings. Furthermore, whereas dynamic symbolic execution and search-based testing approaches were only capable of detecting faults in 8 routines, our approach detected faults in 17 out of the 19 validation routines known to contain implementation errors.},
	journal = {Science of Computer Programming},
	author = {Shahbaz, Muzammil and McMinn, Phil and Stevenson, Mark},
	year = {2014},
	note = {00013},
	keywords = {Regular expressions, Test data generation, Web searches},
	pages = {405--425}
}

@article{ayala-rivera_systematic_2014,
	title = {A {Systematic} {Comparison} and {Evaluation} of k-{Anonymization} {Algorithms} for {Practitioners}},
	volume = {7},
	issn = {1888-5063},
	url = {http://dl.acm.org/citation.cfm?id=2870614.2870620},
	abstract = {The vast amount of data being collected about individuals has brought new challenges in protecting their privacy when this data is disseminated. As a result, Privacy-Preserving Data Publishing has become an active research area, in which multiple anonymization algorithms have been proposed. However, given the large number of algorithms available and limited information regarding their performance, it is difficult to identify and select the most appropriate algorithm given a particular publishing scenario, especially for practitioners. In this paper, we perform a systematic comparison of three well-known k-anonymization algorithms to measure their efficiency (in terms of resources usage) and their effectiveness (in terms of data utility). We extend the scope of their original evaluation by employing a more comprehensive set of scenarios: different parameters, metrics and datasets. Using publicly available implementations of those algorithms, we conduct a series of experiments and a comprehensive analysis to identify the factors that influence their performance, in order to guide practitioners in the selection of an algorithm. We demonstrate through experimental evaluation, the conditions in which one algorithm outperforms the others for a particular metric, depending on the input dataset and privacy requirements. Our findings motivate the necessity of creating methodologies that provide recommendations about the best algorithm given a particular publishing scenario.},
	number = {3},
	journal = {Trans. Data Privacy},
	author = {Ayala-Rivera, Vanessa and McDonagh, Patrick and Cerqueus, Thomas and Murphy, Liam},
	month = dec,
	year = {2014},
	note = {00015},
	pages = {337--370}
}

@article{ganesh_taint-based_2009,
	title = {Taint-based directed whitebox fuzzing},
	issn = {02705257},
	doi = {10.1109/ICSE.2009.5070546},
	abstract = {We present a new automated white box fuzzing technique and a tool, BuzzFuzz, that implements this technique. Unlike standard fuzzing techniques, which randomly change parts of the input file with little or no information about the underlying syntactic structure of the file, BuzzFuzz uses dynamic taint tracing to automatically locate regions of original seed input files that influence values used at key program attack points (points where the program may contain an error). BuzzFuzz then automatically generates new fuzzed test input files by fuzzing these identified regions of the original seed input files. Because these new test files typically preserve the underlying syntactic structure of the original seed input files, they tend to make it past the initial input parsing components to exercise code deep within the semantic core of the computation. We have used BuzzFuzz to automatically find errors in two open-source applications: Swfdec (an Adobe Flash player) and MuPDF (a PDF viewer). Our results indicate that our new directed fuzzing technique can effectively expose errors located deep within large programs. Because the directed fuzzing technique uses taint to automatically discover and exploit information about the input file format, it is especially appropriate for testing programs that have complex, highly structured input file formats.},
	journal = {Proceedings - International Conference on Software Engineering},
	author = {Ganesh, Vijay and Leek, Tim and Rinard, Martin},
	year = {2009},
	note = {00170},
	keywords = {[Electronic Manuscript]},
	pages = {474--484}
}

@inproceedings{iyengar_transforming_2002,
	address = {New York, NY, USA},
	series = {{KDD} '02},
	title = {Transforming {Data} to {Satisfy} {Privacy} {Constraints}},
	isbn = {978-1-58113-567-1},
	url = {http://doi.acm.org/10.1145/775047.775089},
	doi = {10.1145/775047.775089},
	abstract = {Data on individuals and entities are being collected widely. These data can contain information that explicitly identifies the individual (e.g., social security number). Data can also contain other kinds of personal information (e.g., date of birth, zip code, gender) that are potentially identifying when linked with other available data sets. Data are often shared for business or legal reasons. This paper addresses the important issue of preserving the anonymity of the individuals or entities during the data dissemination process. We explore preserving the anonymity by the use of generalizations and suppressions on the potentially identifying portions of the data. We extend earlier works in this area along various dimensions. First, satisfying privacy constraints is considered in conjunction with the usage for the data being disseminated. This allows us to optimize the process of preserving privacy for the specified usage. In particular, we investigate the privacy transformation in the context of data mining applications like building classification and regression models. Second, our work improves on previous approaches by allowing more flexible generalizations for the data. Lastly, this is combined with a more thorough exploration of the solution space using the genetic algorithm framework. These extensions allow us to transform the data so that they are more useful for their intended purpose while satisfying the privacy constraints.},
	booktitle = {Proceedings of the {Eighth} {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Iyengar, Vijay S.},
	year = {2002},
	note = {00755},
	keywords = {data transformation, generalization, predictive modeling, privacy, suppression},
	pages = {279--288}
}

@article{tripp_finding_2013,
	title = {Finding your way in the testing jungle: a learning approach to web security testing},
	url = {http://dl.acm.org/citation.cfm?doid=2483760.2483776},
	doi = {10.1145/2483760.2483776},
	abstract = {Black-box testing of web applications for security vulnera- bilities is known as a hard problem. A major part of the diculty lies in the black-box assumption: The testing tool has limited insight into the workings of server-side defenses. This has traditionally led commercial as well as research vulnerability scanners toward heuristic approaches, such as testing each input point (e.g. HTTP parameter) with a short, prede ned list of e ective test payloads to enable a reasonable tradeo between coverage and performance. We take a fresh approach to the problem of security test- ing, casting it into a learning setting. In our approach, the testing algorithm has available a comprehensive database of test payloads, such that if the web application's defenses are broken, then with near certainty one of the candidate pay- loads is able to demonstrate the vulnerability. The question then becomes how to eciently search through the payload space to nd a good candidate. We have implemented our approach in XSS Analyzer, an industry-level XSS scanner that makes use of 500,000,000 di erent payloads, sending on average only 10 payloads per input point. This is thanks to a powerful learning algo- rithm, which infers from a failed test{\textbar}by analyzing the web- site's response{\textbar}which other payloads are also likely to fail, thereby pruning substantial portions of the search space. Evaluation of XSS Analyzer on 15,552 benchmarks shows solid results: XSS Analyzer achieves {\textgreater} 99\% coverage rel- ative to brute-force traversal over all payloads. Moreover, XSS Analyzer outperforms several competing testing algo- rithms, including a mature commercial algorithm{\textbar}featured in IBM Security AppScan Standard V8.5{\textbar}by a far margin. XSS Analyzer has recently been integrated into the latest version of AppScan (V8.6) instead of that algorithm.},
	journal = {Proceedings of the 2013 International Symposium on Software Testing and Analysis - ISSTA 2013},
	author = {Tripp, Omer and Weisman, Omri and Guy, Lotem},
	year = {2013},
	note = {00015},
	keywords = {FV, READ, cross-site scripting, online learning, vulnerability scanner},
	pages = {347}
}

@inproceedings{thome_search-based_2014,
	address = {New York, New York, USA},
	title = {Search-based security testing of web applications},
	isbn = {978-1-4503-2852-4},
	url = {http://dl.acm.org/citation.cfm?doid=2593833.2593835},
	doi = {10.1145/2593833.2593835},
	booktitle = {Proceedings of the 7th {International} {Workshop} on {Search}-{Based} {Software} {Testing} - {SBST} 2014},
	publisher = {ACM Press},
	author = {Thomé, Julian and Gorla, Alessandra and Zeller, Andreas},
	year = {2014},
	note = {00008},
	keywords = {FV, search-based testing, security testing, sql injections},
	pages = {5--14}
}

@article{robertson_using_2006,
	title = {Using generalization and characterization techniques in the anomaly-based detection of web attacks},
	doi = {10.1.1.109.2599},
	abstract = {The custom, ad hoc nature of web applicationsmakes learning-based anomaly detection systems a suitable approach to provide early warning about the exploita- tion of novel vulnerabilities. However, anomaly-based systems are known for producing a large number of false positives and for providing poor or non-existent infor- mation about the type of attack that is associated with an anomaly. This paper presents a novel approach to anomaly- based detection of web-based attacks. The approach uses an anomaly generalization technique that automat- ically translates suspicious web requests into anomaly signatures. These signatures are then used to group re- current or similar anomalous requests so that an admin- istrator can easily deal with a large number of similar alerts. In addition, the approach uses a heuristics-based technique to infer the type of attacks that generated the anomalies. This enables the prioritization of the at- tacks and provides better information to the adminis- trator. Our approach has been implemented and eval- uated experimentally on real-world data gathered from web servers at two universities.},
	journal = {Proceedings of the 13th Symposium on Network and Distributed System Security (NDSS)(February 2006)},
	author = {Robertson, William and Vigna, Giovanni and Kruegel, Christopher and Kemmerer, R.a. and {Others}},
	year = {2006},
	note = {00159},
	pages = {15}
}

@inproceedings{zhauniarovich_stadyna:_2015,
	address = {New York, NY, USA},
	series = {{CODASPY} '15},
	title = {{StaDynA}: {Addressing} the {Problem} of {Dynamic} {Code} {Updates} in the {Security} {Analysis} of {Android} {Applications}},
	isbn = {978-1-4503-3191-3},
	shorttitle = {{StaDynA}},
	url = {http://doi.acm.org/10.1145/2699026.2699105},
	doi = {10.1145/2699026.2699105},
	abstract = {Static analysis of Android applications can be hindered by the presence of the popular dynamic code update techniques: dynamic class loading and reflection. Recent Android malware samples do actually use these mechanisms to conceal their malicious behavior from static analyzers. These techniques defuse even the most recent static analyzers that usually operate under the "closed world" assumption (the targets of reflective calls can be resolved at analysis time; only classes reachable from the class path at analysis time are used at runtime). Our proposed solution allows existing static analyzers to remove this assumption. This is achieved by combining static and dynamic analysis of applications in order to reveal the hidden/updated behavior and extend static analysis results with this information. This paper presents design, implementation and preliminary evaluation results of our solution called StaDynA.},
	booktitle = {Proceedings of the 5th {ACM} {Conference} on {Data} and {Application} {Security} and {Privacy}},
	publisher = {ACM},
	author = {Zhauniarovich, Yury and Ahmad, Maqsood and Gadyatskaya, Olga and Crispo, Bruno and Massacci, Fabio},
	year = {2015},
	note = {00037},
	keywords = {Android, dynamic code updates, security analysis},
	pages = {37--48}
}

@article{kos_adversarial_2017,
	title = {Adversarial examples for generative models},
	url = {http://arxiv.org/abs/1702.06832},
	abstract = {We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks. Deep generative models have recently become popular due to their ability to model input data distributions and generate realistic examples from those distributions. We present three classes of attacks on the VAE and VAE-GAN architectures and demonstrate them against networks trained on MNIST, SVHN and CelebA. Our first attack leverages classification-based adversaries by attaching a classifier to the trained encoder of the target generative model, which can then be used to indirectly manipulate the latent representation. Our second attack directly uses the VAE loss function to generate a target reconstruction image from the adversarial example. Our third attack moves beyond relying on classification or the standard loss for the gradient and directly optimizes against differences in source and target latent representations. We also motivate why an attacker might be interested in deploying such techniques against a target generative network.},
	journal = {arXiv:1702.06832 [cs, stat]},
	author = {Kos, Jernej and Fischer, Ian and Song, Dawn},
	month = feb,
	year = {2017},
	note = {00010 
arXiv: 1702.06832},
	keywords = {Computer Science - Learning, Statistics - Machine Learning}
}

@article{nowozin_f-gan:_2016,
	title = {f-{GAN}: {Training} {Generative} {Neural} {Samplers} using {Variational} {Divergence} {Minimization}},
	shorttitle = {f-{GAN}},
	url = {http://arxiv.org/abs/1606.00709},
	abstract = {Generative neural samplers are probabilistic models that implement sampling using feedforward neural networks: they take a random input vector and produce a sample from a probability distribution defined by the network weights. These models are expressive and allow efficient computation of samples and derivatives, but cannot be used for computing likelihoods or for marginalization. The generative-adversarial training method allows to train such models through the use of an auxiliary discriminative neural network. We show that the generative-adversarial approach is a special case of an existing more general variational divergence estimation approach. We show that any f-divergence can be used for training generative neural samplers. We discuss the benefits of various choices of divergence functions on training complexity and the quality of the obtained generative models.},
	journal = {arXiv:1606.00709 [cs, stat]},
	author = {Nowozin, Sebastian and Cseke, Botond and Tomioka, Ryota},
	month = jun,
	year = {2016},
	note = {00075 
arXiv: 1606.00709},
	keywords = {Computer Science - Learning, Statistics - Machine Learning, Statistics - Methodology}
}

@misc{noauthor_how_nodate,
	title = {How browsers work},
	url = {http://taligarsiel.com/Projects/howbrowserswork1.htm#Parsing_general},
	urldate = {2016-11-23TZ},
	note = {00021}
}

@article{wang_structural_2010,
	title = {Structural {Learning} of {Attack} {Vectors} for {Generating} {Mutated} {XSS} {Attacks}},
	issn = {2075-2180},
	url = {http://arxiv.org/abs/1009.3711},
	doi = {10.4204/EPTCS.35.2},
	abstract = {Web applications suffer from cross-site scripting (XSS) attacks that resulting from incomplete or incorrect input sanitization. Learning the structure of attack vectors could enrich the variety of manifestations in generated XSS attacks. In this study, we focus on generating more threatening XSS attacks for the state-of-the-art detection approaches that can find potential XSS vulnerabilities in Web applications, and propose a mechanism for structural learning of attack vectors with the aim of generating mutated XSS attacks in a fully automatic way. Mutated XSS attack generation depends on the analysis of attack vectors and the structural learning mechanism. For the kernel of the learning mechanism, we use a Hidden Markov model (HMM) as the structure of the attack vector model to capture the implicit manner of the attack vector, and this manner is benefited from the syntax meanings that are labeled by the proposed tokenizing mechanism. Bayes theorem is used to determine the number of hidden states in the model for generalizing the structure model. The paper has the contributions as following: (1) automatically learn the structure of attack vectors from practical data analysis to modeling a structure model of attack vectors, (2) mimic the manners and the elements of attack vectors to extend the ability of testing tool for identifying XSS vulnerabilities, (3) be helpful to verify the flaws of blacklist sanitization procedures of Web applications. We evaluated the proposed mechanism by Burp Intruder with a dataset collected from public XSS archives. The results show that mutated XSS attack generation can identify potential vulnerabilities.},
	author = {Wang, Yi-Hsun and Mao, Ching-Hao and Lee, Hahn-Ming},
	year = {2010},
	note = {00011},
	keywords = {READ},
	pages = {15--26}
}
@article{godefroid_sage:_2012,
	title = {{SAGE}: {Whitebox} {Fuzzing} for {Security} {Testing}},
	volume = {10},
	issn = {15427730},
	doi = {10.1145/2090147.2094081},
	abstract = {SAGE has had a remarkable impact at Microsoft},
	number = {1},
	journal = {Queue},
	author = {Godefroid, Patrice and Levin, Michael Y. and Molnar, David},
	year = {2012},
	pmid = {73047227},
	note = {00284 },
	pages = {20}
}

@inproceedings{jovanovic_pixy:_2006,
	title = {Pixy: a static analysis tool for detecting {Web} application vulnerabilities},
	isbn = {978-0-7695-2574-7},
	shorttitle = {Pixy},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1624016},
	doi = {10.1109/SP.2006.29},
	urldate = {2016-03-02TZ},
	publisher = {IEEE},
	author = {Jovanovic, N. and Kruegel, C. and Kirda, E.},
	year = {2006},
	note = {00593},
	keywords = {READ, ST},
	pages = {6 pp.--263}
}

@inproceedings{li_t-closeness:_2007,
	title = {t-{Closeness}: {Privacy} {Beyond} k-{Anonymity} and l-{Diversity}},
	shorttitle = {t-{Closeness}},
	doi = {10.1109/ICDE.2007.367856},
	abstract = {The k-anonymity privacy requirement for publishing microdata requires that each equivalence class (i.e., a set of records that are indistinguishable from each other with respect to certain "identifying" attributes) contains at least k records. Recently, several authors have recognized that k-anonymity cannot prevent attribute disclosure. The notion of l-diversity has been proposed to address this; l-diversity requires that each equivalence class has at least l well-represented values for each sensitive attribute. In this paper we show that l-diversity has a number of limitations. In particular, it is neither necessary nor sufficient to prevent attribute disclosure. We propose a novel privacy notion called t-closeness, which requires that the distribution of a sensitive attribute in any equivalence class is close to the distribution of the attribute in the overall table (i.e., the distance between the two distributions should be no more than a threshold t). We choose to use the earth mover distance measure for our t-closeness requirement. We discuss the rationale for t-closeness and illustrate its advantages through examples and experiments.},
	booktitle = {2007 {IEEE} 23rd {International} {Conference} on {Data} {Engineering}},
	author = {Li, N. and Li, T. and Venkatasubramanian, S.},
	month = apr,
	year = {2007},
	note = {01943},
	keywords = {Computer science, Data security, Databases, Diseases, Earth, Motion measurement, Protection, Publishing, Remuneration, attribute disclosure, data privacy, database theory, earth mover distance measure, equivalence class, identifying attributes, k-anonymity privacy requirement, l-diversity, microdata publishing, privacy, t-closeness},
	pages = {106--115}
}

@inproceedings{xie_towards_2007,
	title = {Towards a {Framework} for {Differential} {Unit} {Testing} of {Object}-{Oriented} {Programs}},
	doi = {10.1109/AST.2007.15},
	abstract = {Software developers often face the task of determining how the behaviors of one version of a program unit differ from (or are the same as) the behaviors of a (slightly) different version of the same program unit. In such situations, developers would like to generate tests that exhibit the behavioral differences between the two versions, if any differences exist. We call this type of testing differential unit testing. Some examples of differential unit testing include regression testing, N-version testing, and mutation testing. We propose a framework, called Diffut, that enables differential unit testing of object-oriented programs. Diffut enables "simultaneous" execution of the pairs of corresponding methods from the two versions: methods can receive the same inputs (consisting of the object graph reachable from the receiver and method arguments), and Diffut compares their outputs (consisting of the object graph reachable from the receiver and method return values). Given two versions of a Java class, Diffut automatically synthesizes annotations (in the form of preconditions and postconditions) in the Java Modeling Language (JML) and inserts them into the unit under test to allow the simultaneous execution of the corresponding methods.},
	booktitle = {Second {International} {Workshop} on {Automation} of {Software} {Test} , 2007. {AST} '07},
	author = {Xie, T. and Taneja, K. and Kale, S. and Marinov, D.},
	month = may,
	year = {2007},
	note = {00026},
	keywords = {Automatic testing, Computer science, Diffut, File systems, Genetic mutations, Instruments, Java, Java Modeling Language, Java class, N-version testing, Object oriented modeling, Software systems, Software testing, System testing, configuration management, differential unit testing, mutation testing, object-oriented programs, program testing, regression testing, software developers, specification languages},
	pages = {5--5}
}

@techreport{noauthor_nist_nodate,
	title = {{NIST} {SP} 800-30, {Risk} {Management} {Guide} for {InformationTechnology} .},
	url = {http://csrc.nist.gov/publications/nistpubs/800-30/sp800-30.pdf},
	note = {01304 
bibtex: noauthor\_undated-ur}
}

@article{huang_history-based_2012,
	title = {A history-based cost-cognizant test case prioritization technique in regression testing},
	volume = {85},
	number = {3},
	journal = {Journal of Systems and Software},
	author = {Huang, Yu-Chi and Peng, Kuan-Li and Huang, Chin-Yu},
	year = {2012},
	note = {00051 
bibtex: huang2012history},
	pages = {626--637}
}

@inproceedings{samuel_context-sensitive_2011,
	title = {Context-sensitive auto-sanitization in web templating languages using type qualifiers},
	isbn = {978-1-4503-0948-6},
	url = {http://dl.acm.org/citation.cfm?id=2046775},
	doi = {10.1145/2046707.2046775},
	abstract = {Scripting vulnerabilities, such as cross-site scripting (XSS), plague web applications today. Most research on defense techniques has focused on securing existing legacy applica- tions written in general-purpose languages, such as Java and PHP. However, recent and emerging applications have widely adopted web templating frameworks that have received little attention in research. Web templating frameworks of- fer an ideal opportunity to ensure safety against scripting attacks by secure construction, but most of today’s frame- works fall short of achieving this goal. We propose a novel and principled type-qualifier based mech- anism that can be bolted onto existing web templating frame- works. Our solution permits rich expressiveness in the tem- plating language while achieving backwards compatibility, per- formance and formal security through a context-sensitive auto- sanitization (CSAS) engine. To demonstrate its practicality, we implement our mechanism in Google Closure Templates, a commercially used open-source templating framework that is used in GMail, Google Docs and other applications. Our approach is fast, precise and retrofits to existing commer- cially deployed template code without requiring any changes or annotations.},
	booktitle = {Proceedings of the 18th {ACM} conference on {Computer} and communications security},
	author = {Samuel, Mike and Saxena, P and Song, Dawn},
	year = {2011},
	note = {00057},
	keywords = {READ, ST, VP, cross-site scripting, type systems},
	pages = {587--600}
}

@inproceedings{popa_security_2013,
	title = {A security framework for mobile cloud applications},
	doi = {10.1109/RoEduNet.2013.6511724},
	abstract = {Mobile Cloud Computing is a new concept, which offers Cloud resources and services for mobile devices. It also brings several advantages to mobile devices and to the applications developed for them. However, it increases the security risks and privacy invasion due to the fact that it combines mobile devices with Cloud services and because there is not a well-defined application model. The security issues are treated independently and the existing security solutions are supplied separately by various providers. In this paper, we propose a framework to secure the data transmitted between the components of the same mobile cloud application; and to ensure the integrity of the applications at the installation on the mobile device and when being updated. Our framework allows applying different security properties to different kinds of data and not the same properties to all the data processed by the application. Also our approach takes into consideration the user preferences and the mobile device performances.},
	booktitle = {2013 11th {RoEduNet} {International} {Conference}},
	author = {Popa, D. and Cremene, M. and Borda, M. and Boudaoud, K.},
	month = jan,
	year = {2013},
	note = {00031},
	keywords = {Applications, Computational modeling, Computer applications, Mobile communication, Mobile handsets, Security, cloud computing, cloud resource, cloud services, data communication, data privacy, data processing, data transmission security, mobile cloud application, mobile cloud computing, mobile computing, mobile device performance, privacy invasion, risk management, security framework, security of data, security property, security risk, smart phones, telecommunication security, user preference},
	pages = {1--4}
}

@misc{noauthor_cascading_nodate,
	title = {Cascading {Style} {Sheets} {Level} 2 {Revision} 1 ({CSS} 2.1) {Specification}},
	url = {https://www.w3.org/TR/CSS21/},
	urldate = {2016-11-23TZ},
	note = {00140}
}

@article{bau_vulnerability_2012,
	title = {Vulnerability factors in new web applications: {Audit} tools, developer selection \& languages},
	journal = {Stanford, Tech. Rep},
	author = {Bau, Jason and Wang, Frank and Bursztein, Elie and Mutchler, Patrick and Mitchell, John C},
	year = {2012},
	note = {00007 
bibtex: bau2012vulnerability}
}

@inproceedings{pan_cspautogen:_2016,
	title = {{CSPAutoGen}: {Black}-box {Enforcement} of {Content} {Security} {Policy} upon {Real}-world {Websites}},
	booktitle = {Proceedings of the 2016 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Pan, Xiang and Cao, Yinzhi and Liu, Shuangping and Zhou, Yu and Chen, Yan and Zhou, Tingzhe},
	year = {2016},
	note = {00005 
bibtex: pan2016cspautogen},
	pages = {653--665}
}

@book{shar_auditing_2012,
	title = {Auditing the {XSS} defence features implemented in web application programs},
	volume = {6},
	abstract = {Cross site scripting (XSS) vulnerability is mainly caused by the failure of web applications in sanitising user inputs embedded in web pages. Even though state-of-the-art defensive coding methods and vulnerability detection methods are often used by developers and security auditors, XSS flaws still remain in many applications because of (i) the difficulty of adopting these methods, (ii) the inadequate implementation of these methods, and/or (iii) the lack of understanding of XSS problem. To address this issue, this study proposes a code-auditing approach that recovers the defence model implemented in program source code and suggests guidelines for checking the adequacy of recovered model against XSS attacks. On the basis of the possible implementation patterns of defensive coding methods, our approach extracts all such defences implemented for securing each potentially vulnerable HTML output. It then introduces a variant of control flow graph, called tainted-information flow graph, as},
	number = {4},
	author = {Shar, L.K. and Tan, H.B.K.},
	year = {2012},
	note = {00028}
}

@article{avgerinos_aeg_2011,
	title = {{AEG} : {Automatic} {Exploit} {Generation}},
	volume = {14},
	issn = {00010782},
	url = {http://dl.acm.org/citation.cfm?doid=2556647.2560219 http://security.ece.cmu.edu/aeg/aeg-current.pdf},
	doi = {10.1145/2560217.2560219},
	abstract = {The automatic exploit generation challenge is given a program, automatically find vulnerabilities and gener- ate exploits for them. In this paper we present AEG, the first end-to-end system for fully automatic exploit gener- ation. We used AEG to analyze 14 open-source projects and successfully generated 16 control flow hijacking ex- ploits. Two of the generated exploits (expect-5.43 and htget-0.93) are zero-day exploits against unknown vul- nerabilities. Our contributions are: 1) we show how exploit generation for control flow hijack attacks can be modeled as a formal verification problem, 2) we pro- pose preconditioned symbolic execution, a novel tech- nique for targeting symbolic execution, 3) we present a general approach for generating working exploits once a bug is found, and 4) we build the first end-to-end sys- tem that automatically finds vulnerabilities and gener- ates exploits that produce a shell.},
	number = {2},
	journal = {Memory},
	author = {Avgerinos, Thanassis and Cha, Sang Kil and Lim, Brent and Hao, Tze and Brumley, David},
	year = {2011},
	note = {00004},
	pages = {1--18}
}

@article{antunes_comparing_2009,
	title = {Comparing the effectiveness of penetration testing and static code analysis on the detection of {SQL} injection vulnerabilities in web services},
	doi = {10.1109/PRDC.2009.54},
	abstract = {Web services are becoming business-critical components that must provide a non-vulnerable interface to the client applications. However, previous research and practice show that many web services are deployed with critical vulnerabilities. SQL injection vulnerabilities are particularly relevant, as Web services frequently access a relational database using SQL commands. Penetration testing and static code analysis are two well-know techniques often used for the detection of security vulnerabilities. In this work we compare how effective these two techniques are on the detection of SQL injection vulnerabilities in Web services code. To understand the strengths and limitations of these techniques, we used several commercial and open source tools to detect vulnerabilities in a set of vulnerable services. Results suggest that, in general, static code analyzers are able to detect more SQL injection vulnerabilities than penetration testing tools. Another key observation is that tools implementing the same detection approach frequently detect different vulnerabilities. Finally, many tools provide a low coverage and a high false positives rate, making them a bad option for programmers.},
	journal = {2009 15th IEEE Pacific Rim International Symposium on Dependable Computing, PRDC 2009},
	author = {Antunes, Nuno and Vieira, Marco},
	year = {2009},
	note = {00059},
	keywords = {SQL injection, Security, Vulnerabilities, Web services, penetration testing, static code analysis},
	pages = {301--306}
}

@article{papernot_limitations_2015,
	title = {The {Limitations} of {Deep} {Learning} in {Adversarial} {Settings}},
	url = {http://arxiv.org/abs/1511.07528},
	abstract = {Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97\% adversarial success rate while only modifying on average 4.02\% of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification.},
	journal = {arXiv:1511.07528 [cs, stat]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Jha, Somesh and Fredrikson, Matt and Celik, Z. Berkay and Swami, Ananthram},
	month = nov,
	year = {2015},
	note = {00120 
arXiv: 1511.07528},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning}
}

@article{hopcroft_automata_2006,
	title = {Automata theory, languages, and computation},
	volume = {24},
	journal = {International Edition},
	author = {Hopcroft, John E and Motwani, Rajeev and Ullman, Jeffrey D},
	year = {2006},
	note = {17670 
bibtex: hopcroft2006automata}
}

@incollection{cova_swaddler:_2007,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Swaddler: {An} {Approach} for the {Anomaly}-{Based} {Detection} of {State} {Violations} in {Web} {Applications}},
	copyright = {©2007 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-74319-4 978-3-540-74320-0},
	shorttitle = {Swaddler},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-74320-0_4},
	abstract = {In recent years, web applications have become tremendously popular, and nowadays they are routinely used in security-critical environments, such as medical, financial, and military systems. As the use of web applications for critical services has increased, the number and sophistication of attacks against these applications have grown as well. Most approaches to the detection of web-based attacks analyze the interaction of a web application with its clients and back-end servers. Even though these approaches can effectively detect and block a number of attacks, there are attacks that cannot be detected only by looking at the external behavior of a web application. In this paper, we present Swaddler, a novel approach to the anomaly-based detection of attacks against web applications. Swaddler analyzes the internal state of a web application and learns the relationships between the application’s critical execution points and the application’s internal state. By doing this, Swaddler is able to identify attacks that attempt to bring an application in an inconsistent, anomalous state, such as violations of the intended workflow of a web application. We developed a prototype of our approach for the PHP language and we evaluated it with respect to several real-world applications.},
	language = {en},
	number = {4637},
	urldate = {2016-03-21TZ},
	booktitle = {Recent {Advances} in {Intrusion} {Detection}},
	publisher = {Springer Berlin Heidelberg},
	author = {Cova, Marco and Balzarotti, Davide and Felmetsger, Viktoria and Vigna, Giovanni},
	editor = {Kruegel, Christopher and Lippmann, Richard and Clark, Andrew},
	month = sep,
	year = {2007},
	note = {00186 
DOI: 10.1007/978-3-540-74320-0\_4},
	keywords = {Anomaly Detection, Code Instrumentation, Computer Communication Networks, Computers and Society, Data Encryption, Dynamic Analysis, Management of Computing and Information Systems, Operating Systems, Web Attacks},
	pages = {63--86}
}

@inproceedings{medeiros_automatic_2014,
	address = {New York, NY, USA},
	series = {{WWW} '14},
	title = {Automatic {Detection} and {Correction} of {Web} {Application} {Vulnerabilities} {Using} {Data} {Mining} to {Predict} {False} {Positives}},
	isbn = {978-1-4503-2744-2},
	url = {http://doi.acm.org/10.1145/2566486.2568024},
	doi = {10.1145/2566486.2568024},
	abstract = {Web application security is an important problem in today's internet. A major cause of this status is that many programmers do not have adequate knowledge about secure coding, so they leave applications with vulnerabilities. An approach to solve this problem is to use source code static analysis to find these bugs, but these tools are known to report many false positives that make hard the task of correcting the application. This paper explores the use of a hybrid of methods to detect vulnerabilities with less false positives. After an initial step that uses taint analysis to flag candidate vulnerabilities, our approach uses data mining to predict the existence of false positives. This approach reaches a trade-off between two apparently opposite approaches: humans coding the knowledge about vulnerabilities (for taint analysis) versus automatically obtaining that knowledge (with machine learning, for data mining). Given this more precise form of detection, we do automatic code correction by inserting fixes in the source code. The approach was implemented in the WAP tool and an experimental evaluation was performed with a large set of open source PHP applications.},
	booktitle = {Proceedings of the 23rd {International} {Conference} on {World} {Wide} {Web}},
	publisher = {ACM},
	author = {Medeiros, Ibéria and Neves, Nuno F. and Correia, Miguel},
	year = {2014},
	note = {00025},
	keywords = {Automatic protection, Security, data mining, false positives, input validation vulnerabilities, software security, source code analysis, web applications},
	pages = {63--74}
}

@misc{noauthor_html_nodate,
	title = {{HTML} {Standard}},
	url = {https://html.spec.whatwg.org/multipage/},
	urldate = {2016-08-04TZ},
	note = {00041}
}

@article{bostani_hybrid_2017,
	title = {Hybrid of anomaly-based and specification-based {IDS} for {Internet} of {Things} using unsupervised {OPF} based on {MapReduce} approach},
	volume = {98},
	issn = {0140-3664},
	doi = {10.1016/j.comcom.2016.12.001},
	abstract = {Internet of Things (IoT) is a novel paradigm in computer networks in which resource-constrained objects connect to unreliable Internet by using a wide range of technologies. The insecure nature of the Internet and wireless sensor networks, that are the main components of loT, make loT vulnerable to different attacks, especially routing attacks (as insider attacks). A novel real-time hybrid intrusion detection framework is proposed in this study that consists of anomaly-based and specification-based intrusion detection modules for detecting two well-known routing attacks in IoT called sinkhole and selective-forwarding attacks. For this purpose, the specification-based intrusion detection agents, that are located in the router nodes, analyze the behavior of their host nodes and send their local results to the root node through normal data packets. In addition, an anomaly-based intrusion detection agent, that is located in the root node, employs the unsupervised optimum-path forest algorithm for projecting clustering models by using incoming data packets. This agent, which is based on the MapReduce architecture, can work in a distributed platform for projecting clustering models and consequently parallel detecting of anomalies as a global detection approach. The proposed method makes decision about suspicious behavior by using a voting mechanism. Notably, the proposed method is also extended to detect wormhole attack. The deployment of the hybrid proposed model is investigated in a smart-city scenario by an existing platform, as well. The free network's scale and the ability to identify malicious nodes are two key features of the proposed framework that are evaluated through different experiments in this study. The experimental results of simulated scenarios showed that the proposed hybrid method can achieve true positive rate of 76.19\% and false positive rate of 5.92\% when both sinkhole and selective-forwarding attacks were launched simultaneously. These rates in detecting wormhole attack are 96.02\% and 2.08\%, respectively. (C) 2016 Elsevier B.V. All rights reserved.},
	language = {English},
	journal = {Computer Communications},
	author = {Bostani, Hamid and Sheikhan, Mansour},
	month = jan,
	year = {2017},
	note = {00001 
WOS:000393012700006},
	keywords = {Anomaly-based   intrusion detection, Internet of Things, MapReduce, Security, Sensor networks, Specification-based intrusion detection, Unsupervised optimum-path forest, attacks, challenges, intrusion detection, issues, privacy},
	pages = {52--71}
}

@article{zhou_statistical_2008,
	title = {A {Statistical} {Language} {Modeling} {Approach} to {Online} {Deception} {Detection}},
	volume = {20},
	issn = {1041-4347},
	doi = {10.1109/TKDE.2007.190624},
	abstract = {Online deception is disrupting our daily life, organizational process, and even national security. Existing approaches to online deception detection follow a traditional paradigm by using a set of cues as antecedents for deception detection, which may be hindered by ineffective cue identification. Motivated by the strength of statistical language models (SLMs) in capturing the dependency of words in text without explicit feature extraction, we developed SLMs to detect online deception. We also addressed the data sparsity problem in building SLMs in general and in deception detection in specific using smoothing and vocabulary pruning techniques. The developed SLMs were evaluated empirically with diverse datasets. The results showed that the proposed SLM approach to deception detection outperformed a state-of-the-art text categorization method as well as traditional feature-based methods.},
	number = {8},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Zhou, L. and Shi, Y. and Zhang, D.},
	month = aug,
	year = {2008},
	note = {00060},
	keywords = {Data security, Electronic mail, Face detection, Feature extraction, Knowledge management, National security, Security, Smoothing methods, Text categorization, Text mining, Vocabulary, classification, cue identification, data mining, data sparsity problem, knowledge management applications, language models, learning (artificial intelligence), machine learning, online deception detection, organizational process, programming languages, security of data, smoothing-vocabulary pruning techniques, statistical language modeling approach},
	pages = {1077--1081}
}

@inproceedings{portner_moving_2014,
	title = {Moving {Target} {Defense} {Against} {Cross}-{Site} {Scripting} {Attacks} ({Position} {Paper})},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-17040-4_6},
	doi = {10.1007/978-3-319-17040-4_6},
	abstract = {We present a new method to defend against cross-site scripting (XSS) attacks. Our approach is based on mutating symbols in the JavaScript language and leveraging commonly used load-balancing mechanisms to deliver multiple copies of a website using different versions of the JavaScript language. A XSS attack that injects unauthorized JavaScript code can thus be easily detected. Our solution achieves similar benefits in XSS protection as Content Security Policy (CSP), a leading web standard to prevent cross site scripting, but can be much more easily adopted because refactoring of websites is not required.},
	language = {en},
	urldate = {2017-03-05TZ},
	booktitle = {Foundations and {Practice} of {Security}},
	publisher = {Springer, Cham},
	author = {Portner, Joe and Kerr, Joel and Chu, Bill},
	month = nov,
	year = {2014},
	note = {00003},
	pages = {85--91}
}

@inproceedings{sekar_efficient_2009,
	title = {An {Efficient} {Black}-box {Technique} for {Defeating} {Web} {Application} {Attacks}.},
	booktitle = {{NDSS}},
	author = {Sekar, R},
	year = {2009},
	note = {00148 
bibtex: sekar2009efficient}
}

@article{li_hamsa:_2006,
	title = {Hamsa: {Fast} signature generation for zero-day polymorphic worms with provable attack resilience},
	volume = {2006},
	issn = {10816011},
	doi = {10.1109/SP.2006.18},
	abstract = {Zero-day polymorphic worms pose a serious threat to the security of Internet infrastructures. Given their rapid propagation, it is crucial to detect them at edge networks and automatically generate signatures in the early stages of infection. Most existing approaches for automatic signature generation need host information and are thus not applicable for deployment on high-speed network links. In this paper, we propose Hamsa, a network-based automated signature generation system for polymorphic worms which is fast, noise-tolerant and attack-resilient. Essentially, we propose a realistic model to analyze the invariant content of polymorphic worms which allows us to make analytical attack-resilience guarantees for the signature generation algorithm. Evaluation based on a range of polymorphic worms and polymorphic engines demonstrates that Hamsa significantly outperforms Polygraph 16 in terms of efficiency, accuracy, and attack resilience.},
	journal = {Proceedings - IEEE Symposium on Security and Privacy},
	author = {Li, Zhichun and Sanghi, Manan and Chen, Yan and Kao, Ming Yang and Chavez, Brian},
	year = {2006},
	note = {00336},
	pages = {32--46}
}

@article{choi_static_1994,
	title = {Static {Slicing} in the {Presence} of {Goto} {Statements}},
	volume = {16},
	issn = {0164-0925},
	url = {http://doi.acm.org/10.1145/183432.183438},
	doi = {10.1145/183432.183438},
	abstract = {A static program slice is an extract of a program which can help our understanding of the behavior of the program; it has been proposed for use in debugging, optimization, parallelization, and integration of programs. This article considers two types of static slices: executable and nonexecutable. Efficient and well-founded methods have been developed to construct executable slices for programs without goto statements; it would be tempting to assume these methods would apply as well in programs with arbitrary goto statements. We show why previous methods do not work in this more general setting, and describe our solutions that correctly and efficiently compute executable slices for programs even with arbitrary goto statements. Our conclusion is that goto statements can be  accommodated in generating executable static slices.},
	number = {4},
	urldate = {2016-09-30TZ},
	journal = {ACM Trans. Program. Lang. Syst.},
	author = {Choi, Jong-Deok and Ferrante, Jeanne},
	month = jul,
	year = {1994},
	note = {00138},
	keywords = {Testing, debugging, program analysis, slicing},
	pages = {1097--1113}
}

@inproceedings{hills_empirical_2013,
	address = {New York, NY, USA},
	series = {{ISSTA} 2013},
	title = {An {Empirical} {Study} of {PHP} {Feature} {Usage}: {A} {Static} {Analysis} {Perspective}},
	isbn = {978-1-4503-2159-4},
	shorttitle = {An {Empirical} {Study} of {PHP} {Feature} {Usage}},
	url = {http://doi.acm.org/10.1145/2483760.2483786},
	doi = {10.1145/2483760.2483786},
	abstract = {PHP is one of the most popular languages for server-side application development. The language is highly dynamic, providing programmers with a large amount of flexibility. However, these dynamic features also have a cost, making it difficult to apply traditional static analysis techniques used in standard code analysis and transformation tools. As part of our work on creating analysis tools for PHP, we have conducted a study over a significant corpus of open-source PHP systems, looking at the sizes of actual PHP programs, which features of PHP are actually used, how often dynamic features appear, and how distributed these features are across the files that make up a PHP website. We have also looked at whether uses of these dynamic features are truly dynamic or are, in some cases, statically understandable, allowing us to identify specific patterns of use which can then be taken into account to build more precise tools. We believe this work will be of interest to creators of analysis tools for PHP, and that the methodology we present can be leveraged for other dynamic languages with similar features.},
	booktitle = {Proceedings of the 2013 {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {ACM},
	author = {Hills, Mark and Klint, Paul and Vinju, Jurgen},
	year = {2013},
	note = {00033},
	keywords = {PHP, Static analysis, Static metrics, Static program behavior, dynamic language features},
	pages = {325--335}
}

@article{liang_fast_2005,
	title = {Fast and automated generation of attack signatures: a basis for building self-protecting servers},
	issn = {15437221},
	url = {http://portal.acm.org/citation.cfm?id=1102150},
	doi = {10.1145/1102120.1102150},
	abstract = {Large-scale attacks, such as those launched by worms and zombie farms, pose a serious threat to our network-centric society. Existing approaches such as software patches are simply unable to cope with the volume and speed with which new vulnerabilities are being discovered. In this paper, we develop a new approach that can provide effective protection against a vast majority of these attacks that exploit memory errors in C/C++ programs. Our approach, called COVERS, uses a forensic analysis of a victim server's memory to correlate attacks to inputs received over the network, and automatically develop a signature that characterizes inputs that carry attacks. The signatures tend to capture characteristics of the underlying vulnerability (e.g., a message field being too long) rather than the characteristics of an attack, which makes them effective against variants of attacks. Our approach introduces low overheads (under 10\%), does not require access to source code of the protected server, and has successfully generated signatures for the attacks studied in our experiments, without producing false positives. Since the signatures are generated in tens of milliseconds, they can potentially be distributed quickly over the Internet to filter out (and thus stop) fast-spreading worms. Another interesting aspect of our approach is that it can defeat guessing attacks reported against address-space randomization and instruction set randomization techniques. Finally, it increases the capacity of servers to withstand repeated attacks by a factor of 10 or more.},
	journal = {Conference on Computer and Communications Security},
	author = {Liang, Zhenkai and Sekar, R.},
	year = {2005},
	note = {00210},
	keywords = {buffer overflow, denial-of-service protection, memory error, signature generation, worm defense},
	pages = {213}
}

@inproceedings{bozic_attack_2014,
	address = {New York, NY, USA},
	series = {{AST} 2014},
	title = {Attack {Pattern}-based {Combinatorial} {Testing}},
	isbn = {978-1-4503-2858-6},
	url = {http://doi.acm.org/10.1145/2593501.2593502},
	doi = {10.1145/2593501.2593502},
	booktitle = {Proceedings of the 9th {International} {Workshop} on {Automation} of {Software} {Test}},
	publisher = {ACM},
	author = {Bozic, Josip and Simos, Dimitris E and Wotawa, Franz},
	year = {2014},
	note = {00012},
	keywords = {Combinatorial testing, DN, READ, attack patterns, model-based testing, security testing},
	pages = {1--7}
}

@article{godefroid_automating_2008,
	title = {Automating {Software} {Testing} {Using} {Program} {Analysis}},
	volume = {25},
	issn = {0740-7459},
	doi = {10.1109/MS.2008.109},
	abstract = {During the last 10 years, code inspection for standard programming errors has largely been automated with static code analysis. During the next 10 years, we expect to see similar progress in automating testing, and specifically test generation, thanks to advances in program analysis, efficient constraint solvers, and powerful computers. Three new tools from Microsoft combine techniques from static program analysis, dynamic analysis, model checking, and automated constraint solving while targeting different application domains.},
	number = {5},
	journal = {Software, IEEE},
	author = {Godefroid, P and de Halleux, P and Nori, a V and Rajamani, S K and Schulte, W and Tillmann, N and Levin, M Y},
	year = {2008},
	note = {00109},
	keywords = {Application software, Automatic programming, Automatic testing, Code standards, Computer bugs, Dynamic analysis, FV, Inspection, Model checking, Power generation, Program Verification, Reliability, Research and development, Security, Software testing, Software tools, automated constraint solving, automatic test generation, code inspection, program analysis, program testing, software engineering, static code analysis, test generation},
	pages = {30--37}
}

@article{ahmed_multiple-path_2016,
	series = {Real-{Time} {Signal} {Processing} in {Embedded} {Systems}},
	title = {Multiple-path testing for cross site scripting using genetic algorithms},
	volume = {64},
	issn = {1383-7621},
	url = {http://www.sciencedirect.com/science/article/pii/S1383762115001332},
	doi = {10.1016/j.sysarc.2015.11.001},
	abstract = {Web applications suffer from different security vulnerabilities that could be exploited by hackers to cause harm in a variety of ways. A number of approaches have been proposed to test for such vulnerabilities. However, some gaps are still to be addressed. In this paper, we address one of such gaps: the problem of automatically generating test data (i.e., possible attacks) to test for cross site scripting (XSS) type of vulnerability. The objective is to generate a set of test data to exercise candidate security-vulnerable paths in a given script. The desirable set of test data must be effective in the sense that it uncovers whether any path can indeed be exploited to launch an attack. We designed a genetic algorithm-based test data generator that uses a database of XSS attack patterns to generate possible attacks and assess whether the attack is successful. We considered different types of XSS vulnerability: stored, reflected and DOM based. We empirically validated our test data generator using case studies of Web applications developed using PHP and MySQL. Empirical results show that our test data generator is effective in generating, in one run, multiple test data to cover multiple target paths.},
	journal = {Journal of Systems Architecture},
	author = {Ahmed, Moataz A. and Ali, Fakhreldin},
	month = mar,
	year = {2016},
	note = {00002},
	keywords = {Security testing, Web testing, cross-site scripting, genetic algorithms},
	pages = {50--62}
}

@inproceedings{xie_advances_2016,
	title = {Advances in unit testing: theory and practice},
	doi = {http://dx.doi.org/10.1145/2889160.2891056},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Software} {Engineering} {Companion}},
	publisher = {ACM},
	author = {Xie, Tao and Tillmann, Nikolai and Lakshman, Pratap},
	year = {2016},
	note = {00001 
bibtex: xie2016advances},
	pages = {904--905}
}

@book{shar_automated_2012,
	title = {Automated removal of cross site scripting vulnerabilities in web applications},
	volume = {54},
	isbn = {0-7695-2224-6},
	url = {http://dx.doi.org/10.1016/j.infsof.2011.12.006},
	abstract = {Context: Cross site scripting (XSS) vulnerability is among the top web application vulnerabilities according to recent surveys. This vulnerability occurs when a web application uses inputs received from users in web pages without properly checking them. This allows an attacker to inject malicious scripts in web pages via such inputs such that the scripts perform malicious actions when a client visits the exploited web pages. Such an attack may cause serious security violations such as account hijacking and cookie theft. Current approaches to mitigate this problem mainly focus on effective detection of XSS vulnerabilities in the programs or prevention of real time XSS attacks. As more sophisticated attack vectors are being discovered, vulnerabilities if not removed could be exploited anytime. Objective: To address this issue, this paper presents an approach for removing XSS vulnerabilities in web applications. Method: Based on static analysis and pattern matching techniques, our approach identifies potential XSS vulnerabilities in program source code and secures them with appropriate escaping mechanisms which prevent input values from causing any script execution. Results: We developed a tool, saferXSS, to implement the proposed approach. Using the tool, we evaluated the applicability and effectiveness of the proposed approach based on the experiments on five Java-based web applications. Conclusion: Our evaluation has shown that the tool can be applied to real-world web applications and it automatically removed all the real XSS vulnerabilities in the test subjects. ?? 2011 Elsevier B.V. All rights reserved.},
	number = {5},
	publisher = {Elsevier B.V.},
	author = {Shar, Lwin Khin and Tan, Hee Beng Kuan},
	year = {2012},
	note = {00048},
	keywords = {Automated bug fixing, Character escaping, Encoding, Injection vulnerability, Web security, cross site scripting}
}

@article{kurakin_adversarial_2016,
	title = {Adversarial {Machine} {Learning} at {Scale}},
	url = {http://arxiv.org/abs/1611.01236},
	abstract = {Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a "label leaking" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.},
	journal = {arXiv:1611.01236 [cs, stat]},
	author = {Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
	month = nov,
	year = {2016},
	note = {00021 
arXiv: 1611.01236},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Learning, Statistics - Machine Learning}
}

@article{huang_testing_2005,
	title = {A testing framework for {Web} application security assessment},
	volume = {48},
	issn = {13891286},
	doi = {10.1016/j.comnet.2005.01.003},
	abstract = {The rapid development phases and extremely short turnaround time of Webapplications make it difficult to eliminate their vulnerabilities. Here we study how software testing techniques such as fault injection and runtime monitoring can be applied to Webapplications. We implemented our proposed mechanisms in the WebApplication Vulnerability and Error Scanner (WAVES)—a black-box testingframework for automated Webapplicationsecurityassessment. Real-world situations are used to test WAVES and to compare it with other tools. Our results show that WAVES is a feasible platform for assessing Webapplicationsecurity.},
	number = {5},
	journal = {Computer Networks},
	author = {Huang, Yao-Wen and Tsai, Chung-Hung and Lin, Tsung-Po and Huang, Shih-Kun and Lee, D.T. and Kuo, Sy-Yen},
	year = {2005},
	note = {00071},
	keywords = {910444, affairs, and by the national, black-box testing, complete crawling, economic, fault injection, for information industry, grants, no, of institute, project, science council under the, security assessment, sponsored by ministry of, supported by the ncert, web application testing},
	pages = {739--761}
}

@inproceedings{schwartz_all_2010,
	title = {All you ever wanted to know about dynamic taint analysis and forward symbolic execution (but might have been afraid to ask)},
	isbn = {978-0-7695-4035-1},
	doi = {10.1109/SP.2010.26},
	abstract = {Dynamic taint analysis and forward symbolic execution are quickly becoming staple techniques in security analyses. Example applications of dynamic taint analysis and forward symbolic execution include malware analysis, input filter generation, test case generation, and vulnerability discovery. Despite the widespread usage of these two techniques, there has been little effort to formally define the algorithms and summarize the critical issues that arise when these techniques are used in typical security contexts. The contributions of this paper are two-fold. First, we precisely describe the algorithms for dynamic taint analysis and forward symbolic execution as extensions to the run-time semantics of a general language. Second, we highlight important implementation choices, common pitfalls, and considerations when using these techniques in a security context.},
	booktitle = {Proceedings - {IEEE} {Symposium} on {Security} and {Privacy}},
	author = {Schwartz, Edward J and Avgerinos, Thanassis and Brumley, David},
	year = {2010},
	pmid = {1511570},
	note = {00455 },
	keywords = {Dynamic analysis, symbolic execution, taint analysis},
	pages = {317--331}
}

@article{papernot_practical_2016,
	title = {Practical {Black}-{Box} {Attacks} against {Machine} {Learning}},
	url = {http://arxiv.org/abs/1602.02697},
	abstract = {Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24\% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19\% and 88.94\%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.},
	journal = {arXiv:1602.02697 [cs]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z. Berkay and Swami, Ananthram},
	month = feb,
	year = {2016},
	note = {00022 
arXiv: 1602.02697},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Learning}
}

@article{gkoulalas-divanis_publishing_2014,
	series = {Special {Issue} on {Informatics} {Methods} in {Medical} {Privacy}},
	title = {Publishing data from electronic health records while preserving privacy: {A} survey of algorithms},
	volume = {50},
	issn = {1532-0464},
	shorttitle = {Publishing data from electronic health records while preserving privacy},
	url = {http://www.sciencedirect.com/science/article/pii/S1532046414001403},
	doi = {10.1016/j.jbi.2014.06.002},
	abstract = {The dissemination of Electronic Health Records (EHRs) can be highly beneficial for a range of medical studies, spanning from clinical trials to epidemic control studies, but it must be performed in a way that preserves patients’ privacy. This is not straightforward, because the disseminated data need to be protected against several privacy threats, while remaining useful for subsequent analysis tasks. In this work, we present a survey of algorithms that have been proposed for publishing structured patient data, in a privacy-preserving way. We review more than 45 algorithms, derive insights on their operation, and highlight their advantages and disadvantages. We also provide a discussion of some promising directions for future research in this area.},
	journal = {Journal of Biomedical Informatics},
	author = {Gkoulalas-Divanis, Aris and Loukides, Grigorios and Sun, Jimeng},
	month = aug,
	year = {2014},
	note = {00064},
	keywords = {Algorithms, Anonymization, Electronic health records, privacy, survey},
	pages = {4--19}
}

@inproceedings{dallmeier_webmate:_2014,
	title = {{WebMate}: {Web} {Application} {Test} {Generation} in the {Real} {World}},
	isbn = {978-1-4799-5790-3},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6825696},
	doi = {10.1109/ICSTW.2014.65},
	booktitle = {2014 {IEEE} {Seventh} {International} {Conference} on {Software} {Testing}, {Verification} and {Validation} {Workshops}},
	author = {Dallmeier, Valentin and Pohl, Bernd and Burger, Martin and Mirold, Michael and Zeller, Andreas},
	year = {2014},
	note = {00008},
	keywords = {-software testing, 0, Software testing, Web 2.0, web 2},
	pages = {413--418}
}

@misc{noauthor_rfc_nodate,
	title = {{RFC} 3986-{Uniform} {Resource} {Identifier} ({URI}): {Generic} {Syntax}},
	url = {https://www.ietf.org/rfc/rfc3986.txt},
	urldate = {2016-08-04TZ},
	note = {00033}
}

@inproceedings{wassermann_static_2008,
	title = {Static detection of cross-site scripting vulnerabilities},
	doi = {10.1145/1368088.1368112},
	abstract = {Web applications support many of our daily activities, but they often have security problems, and their accessibility makes them easy to exploit. In cross-site scripting (XSS), an attacker exploits the trust a Web client (browser) has for a trusted server and executes injected script on the browser with the server's privileges. In 2006, XSS constituted the largest class of newly reported vulnerabilities making it the most prevalent class of attacks today. Web applications have XSS vulnerabilities because the validation they perform on untrusted input does not suffice to prevent that input from invoking a browser's JavaScript interpreter, and this validation is particularly difficult to get right if it must admit some HTML mark-up. Most existing approaches to finding XSS vulnerabilities are taint-based and assume input validation functions to be adequate, so they either miss real vulnerabilities or report many false positives. This paper presents a static analysis for finding XSS vulnerabilities that directly addresses weak or absent input validation. Our approach combines work on tainted information flow with string analysis. Proper input validation is difficult largely because of the many ways to invoke the JavaScript interpreter; we face the same obstacle checking for vulnerabilities statically, and we address it by formalizing a policy based on the W3C recommendation, the Firefox source code, and online tutorials about closed-source browsers. We provide effective checking algorithms based on our policy. We implement our approach and provide an extensive evaluation that finds both known and unknown vulnerabilities in real-world web applications.},
	booktitle = {2008 {ACM}/{IEEE} 30th {International} {Conference} on {Software} {Engineering}},
	author = {Wassermann, G. and Su, Zhendong Su Zhendong},
	year = {2008},
	note = {00356},
	keywords = {READ, ST, cross-site scripting, input validation, static analysis, web applications},
	pages = {171--180}
}

@article{machanavajjhala_l-diversity:_2007,
	title = {L-diversity: {Privacy} {Beyond} {K}-anonymity},
	volume = {1},
	issn = {1556-4681},
	shorttitle = {L-diversity},
	url = {http://doi.acm.org/10.1145/1217299.1217302},
	doi = {10.1145/1217299.1217302},
	abstract = {Publishing data about individuals without revealing sensitive information about them is an important problem. In recent years, a new definition of privacy called k-anonymity has gained popularity. In a k-anonymized dataset, each record is indistinguishable from at least k − 1 other records with respect to certain identifying attributes. In this article, we show using two simple attacks that a k-anonymized dataset has some subtle but severe privacy problems. First, an attacker can discover the values of sensitive attributes when there is little diversity in those sensitive attributes. This is a known problem. Second, attackers often have background knowledge, and we show that k-anonymity does not guarantee privacy against attackers using background knowledge. We give a detailed analysis of these two attacks, and we propose a novel and powerful privacy criterion called ℓ-diversity that can defend against such attacks. In addition to building a formal foundation for ℓ-diversity, we show in an experimental evaluation that ℓ-diversity is practical and can be implemented efficiently.},
	number = {1},
	journal = {ACM Trans. Knowl. Discov. Data},
	author = {Machanavajjhala, Ashwin and Kifer, Daniel and Gehrke, Johannes and Venkitasubramaniam, Muthuramakrishnan},
	month = mar,
	year = {2007},
	note = {01962},
	keywords = {\textit{k}-anonymity, data privacy, privacy-preserving data publishing, ℓ-diversity}
}

@article{emanuelsson_comparative_2008,
	series = {Proceedings of the 3rd {International} {Workshop} on {Systems} {Software} {Veriﬁcation} ({SSV} 2008)},
	title = {A {Comparative} {Study} of {Industrial} {Static} {Analysis} {Tools}},
	volume = {217},
	issn = {1571-0661},
	url = {http://www.sciencedirect.com/science/article/pii/S1571066108003824},
	doi = {10.1016/j.entcs.2008.06.039},
	abstract = {Tools based on static analysis can be used to find defects in programs. Tools that do shallow analyses based on pattern matching have existed since the 1980's and although they can analyze large programs they have the drawback of producing a massive amount of warnings that have to be manually analyzed to see if they are real defects or not. Recent technology advances has brought forward tools that do deeper analyses that discover more defects and produce a limited amount of false warnings. These tools can still handle large industrial applications with millions lines of code. This article surveys the underlying supporting technology of three state-of-the-art static analysis tools. The survey relies on information in research articles and manuals, and includes the types of defects checked for (such as memory management, arithmetics, security vulnerabilities), soundness, value and aliasing analyses, incrementality and IDE integration. This survey is complemented by practical experiences from evaluations at the Ericsson telecom company.},
	urldate = {2016-03-02TZ},
	journal = {Electronic Notes in Theoretical Computer Science},
	author = {Emanuelsson, Pär and Nilsson, Ulf},
	month = jul,
	year = {2008},
	note = {00153},
	keywords = {READ, ST, Static analysis, dataflow analysis, defects, security vulnerabilities},
	pages = {5--21}
}

@article{gil_internet_2016,
	title = {Internet of {Things}: {A} {Review} of {Surveys} {Based} on {Context} {Aware} {Intelligent} {Services}},
	volume = {16},
	issn = {1424-8220},
	shorttitle = {Internet of {Things}},
	doi = {10.3390/s16071069},
	abstract = {The Internet of Things (IoT) has made it possible for devices around the world to acquire information and store it, in order to be able to use it at a later stage. However, this potential opportunity is often not exploited because of the excessively big interval between the data collection and the capability to process and analyse it. In this paper, we review the current IoT technologies, approaches and models in order to discover what challenges need to be met to make more sense of data. The main goal of this paper is to review the surveys related to IoT in order to provide well integrated and context aware intelligent services for IoT. Moreover, we present a state-of-the-art of IoT from the context aware perspective that allows the integration of IoT and social networks in the emerging Social Internet of Things (SIoT) term.},
	language = {English},
	number = {7},
	journal = {Sensors},
	author = {Gil, David and Ferrandez, Antonio and Mora-Mora, Higinio and Peral, Jesus},
	month = jul,
	year = {2016},
	note = {00020 
WOS:000380967000126},
	keywords = {Algorithms, Big Data, Internet of Things, Semantics, Sensor networks, analytics, challenges, cloud computing, data mining with big   data, directions, information, integration, ontology, services for big data, smart cities, social internet of things, systems},
	pages = {1069}
}

@misc{noauthor_closure_nodate,
	title = {Closure {Templates}},
	url = {https://developers.google.com/closure/templates/},
	abstract = {Create powerful and efficient JavaScript.},
	urldate = {2016-03-02TZ},
	journal = {Google Closure Templates},
	note = {00004},
	keywords = {READ, VP}
}

@inproceedings{kern_securing_2014,
	title = {Securing the tangled web},
	volume = {57},
	url = {http://dl.acm.org/citation.cfm?id=2643134},
	booktitle = {Communications of the {ACM}},
	publisher = {ACM},
	author = {Kern, Christoph},
	year = {2014},
	note = {00012},
	pages = {38--47}
}

@article{jhala_software_2009,
	title = {Software {Model} {Checking}},
	volume = {41},
	issn = {0360-0300},
	url = {http://doi.acm.org/10.1145/1592434.1592438},
	doi = {10.1145/1592434.1592438},
	abstract = {We survey recent progress in software model checking.},
	number = {4},
	urldate = {2016-03-07TZ},
	journal = {ACM Comput. Surv.},
	author = {Jhala, Ranjit and Majumdar, Rupak},
	month = oct,
	year = {2009},
	note = {00315},
	keywords = {Software model checking, abstraction, counterexample-guided refinement, enumerative and symbolic model checking, liveness, safety},
	pages = {21:1--21:54}
}

@misc{noauthor_html5_nodate,
	title = {{HTML}5},
	url = {https://www.w3.org/TR/html5/},
	urldate = {2016-11-23TZ},
	note = {00512}
}

@article{guha_using_2009,
	title = {Using {Static} {Analysis} for {Ajax} {Intrusion} {Detection}},
	author = {Guha, Arjun and Jim, Trevor},
	year = {2009},
	note = {00166},
	keywords = {ajax, control-flow analysis, intrusion detec-, javascript},
	pages = {561--570}
}

@incollection{boyd_sqlrand:_2004,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{SQLrand}: {Preventing} {SQL} {Injection} {Attacks}},
	copyright = {©2004 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-22217-0 978-3-540-24852-1},
	shorttitle = {{SQLrand}},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-24852-1_21},
	abstract = {We present a practical protection mechanism against SQL injection attacks. Such attacks target databases that are accessible through a web front-end, and take advantage of flaws in the input validation logic of Web components such as CGI scripts. We apply the concept of instruction-set randomization to SQL, creating instances of the language that are unpredictable to the attacker. Queries injected by the attacker will be caught and terminated by the database parser. We show how to use this technique with the MySQL database using an intermediary proxy that translates the random SQL to its standard language. Our mechanism imposes negligible performance overhead to query processing and can be easily retrofitted to existing systems.},
	language = {en},
	number = {3089},
	urldate = {2016-03-21TZ},
	booktitle = {Applied {Cryptography} and {Network} {Security}},
	publisher = {Springer Berlin Heidelberg},
	author = {Boyd, Stephen W. and Keromytis, Angelos D.},
	editor = {Jakobsson, Markus and Yung, Moti and Zhou, Jianying},
	month = jun,
	year = {2004},
	note = {00462 
DOI: 10.1007/978-3-540-24852-1\_21},
	keywords = {Computer Communication Networks, Data Encryption, Information Storage and Retrieval, Information Systems Applications (incl. Internet), Management of Computing and Information Systems, Operating Systems},
	pages = {292--302}
}

@inproceedings{younis_using_2014,
	title = {Using {Software} {Structure} to {Predict} {Vulnerability} {Exploitation} {Potential}},
	booktitle = {Software {Security} and {Reliability}-{Companion} ({SERE}-{C}), 2014 {IEEE} {Eighth} {International} {Conference} on},
	publisher = {IEEE},
	author = {Younis, Awad A and Malaiya, Yashwant K},
	year = {2014},
	note = {00006 
bibtex: younis2014using},
	pages = {13--18}
}

@article{elbaum_leveraging_2005,
	title = {Leveraging user-session data to support {Web} application testing},
	volume = {31},
	issn = {0098-5589},
	doi = {10.1109/TSE.2005.36},
	abstract = {Web applications are vital components of the global information infrastructure, and it is important to ensure their dependability. Many techniques and tools for validating Web applications have been created, but few of these have addressed the need to test Web application functionality and none have attempted to leverage data gathered in the operation of Web applications to assist with testing. In this paper, we present several techniques for using user session data gathered as users operate Web applications to help test those applications from a functional standpoint. We report results of an experiment comparing these new techniques to existing white-box techniques for creating test cases for Web applications, assessing both the adequacy of the generated test cases and their ability to detect faults on a point-of-sale Web application. Our results show that user session data can be used to produce test suites more effective overall than those produced by the white-box techniques considered; however, the faults detected by the two classes of techniques differ, suggesting that the techniques are complementary.},
	number = {3},
	journal = {IEEE Transactions on Software Engineering},
	author = {Elbaum, S. and Rothermel, G. and Karre, S. and II, M. Fisher},
	month = mar,
	year = {2005},
	note = {00273},
	keywords = {Application software, Automatic testing, Computer Society, Face detection, Fault detection, Index Terms- Software testing, Internet, Marketing and sales, Medical diagnostic imaging, Proposals, Software systems, Software testing, System testing, Web application testing, empirical studies., program testing, test data generation, user session data, web applications, white-box technique},
	pages = {187--202}
}

@article{cadar_symbolic_2011,
	title = {Symbolic execution for software testing in practice: preliminary assessment},
	issn = {0270-5257},
	doi = {10.1145/1985793.1985995},
	abstract = {We present results for the "Impact Project Focus Area" on the topic of symbolic execution as used in software testing. Symbolic execution is a program analysis technique introduced in the 70s that has received renewed interest in recent years, due to algorithmic advances and increased availability of computational power and constraint solving technology. We review classical symbolic execution and some modern extensions such as generalized symbolic execution and dynamic test generation. We also give a preliminary assessment of the use in academia, research labs, and industry.},
	journal = {2011 33rd International Conference on Software Engineering (ICSE)},
	author = {Cadar, Cristian and Godefroid, Patrice and Khurshid, Sarfraz and Pasareanu, Corina S. and Sen, Koushik and Tillmann, Nikolai and Visser, Willem},
	year = {2011},
	note = {00256},
	keywords = {dynamic test generation, generalized symbolic execution},
	pages = {1066--1071}
}

@article{goseva-popstojanova_capability_2015,
	title = {On the capability of static code analysis to detect security vulnerabilities},
	volume = {68},
	issn = {0950-5849},
	url = {http://www.sciencedirect.com/science/article/pii/S0950584915001366},
	doi = {10.1016/j.infsof.2015.08.002},
	abstract = {Context:
Static analysis of source code is a scalable method for discovery of software faults and security vulnerabilities. Techniques for static code analysis have matured in the last decade and many tools have been developed to support automatic detection.
Objective: This research work is focused on empirical evaluation of the ability of static code analysis tools to detect security vulnerabilities with an objective to better understand their strengths and shortcomings.
Method: We conducted an experiment which consisted of using the benchmarking test suite Juliet to evaluate three widely used commercial tools for static code analysis. Using design of experiments approach to conduct the analysis and evaluation and including statistical testing of the results are unique characteristics of this work. In addition to the controlled experiment, the empirical evaluation included case studies based on three open source programs.
Results: Our experiment showed that 27\% of C/C++ vulnerabilities and 11\% of Java vulnerabilities were missed by all three tools. Some vulnerabilities were detected by only one or combination of two tools; 41\% of C/C++ and 21\% of Java vulnerabilities were detected by all three tools. More importantly, static code analysis tools did not show statistically significant difference in their ability to detect security vulnerabilities for both C/C++ and Java. Interestingly, all tools had median and mean of the per CWE recall values and overall recall across all CWEs close to or below 50\%, which indicates comparable or worse performance than random guessing. While for C/C++ vulnerabilities one of the tools had better performance in terms of probability of false alarm than the other two tools, there was no statistically significant difference among tools’ probability of false alarm for Java test cases.
Conclusions: Despite recent advances in methods for static code analysis, the state-of-the-art tools are not very effective in detecting security vulnerabilities.},
	urldate = {2017-05-30TZ},
	journal = {Information and Software Technology},
	author = {Goseva-Popstojanova, Katerina and Perhinschi, Andrei},
	month = dec,
	year = {2015},
	note = {00011},
	keywords = {Case studies, Common Weakness Enumeration (CWE), Experiment, Static code analysis evaluation, security vulnerabilities},
	pages = {18--33}
}

@inproceedings{zhang_secure_2015,
	title = {Secure and resilient distributed machine learning under adversarial environments},
	abstract = {With a large number of sensors and control units in networked systems, the decentralized computing algorithms play a key role in scalable and efficient data processing for detection and estimation. The well-known algorithms are vulnerable to adversaries who can modify and generate data to deceive the system to misclassify or misestimate the information from the distributed data processing. This work aims to develop secure, resilient and distributed machine learning algorithms under adversarial environment. We establish a game-theoretic framework to capture the conflicting interests between the adversary and a set of distributed data processing units. The Nash equilibrium of the game allows predicting the outcome of learning algorithms in adversarial environment, and enhancing the resilience of the machine learning through dynamic distributed learning algorithms. We use Spambase Dataset to illustrate and corroborate our results.},
	booktitle = {2015 18th {International} {Conference} on {Information} {Fusion} ({Fusion})},
	author = {Zhang, R. and Zhu, Q.},
	month = jul,
	year = {2015},
	note = {00005},
	keywords = {Games, Heuristic algorithms, Machine learning algorithms, Nash equilibrium, Security, Spambase Dataset, Training, Training data, adversarial environments, decentralized computing algorithms, distributed data processing units, distributed machine learning algorithms, distributed processing, dynamic distributed learning algorithm, game theory, game-theoretic framework, information misclassification, information misestimation, learning (artificial intelligence), networked systems, sensors},
	pages = {644--651}
}

@article{catal_test_2012,
	title = {Test case prioritization: a systematic mapping study},
	volume = {21},
	issn = {0963-9314, 1573-1367},
	shorttitle = {Test case prioritization},
	url = {http://link.springer.com/article/10.1007/s11219-012-9181-z},
	doi = {10.1007/s11219-012-9181-z},
	abstract = {Test case prioritization techniques, which are used to improve the cost-effectiveness of regression testing, order test cases in such a way that those cases that are expected to outperform others in detecting software faults are run earlier in the testing phase. The objective of this study is to examine what kind of techniques have been widely used in papers on this subject, determine which aspects of test case prioritization have been studied, provide a basis for the improvement of test case prioritization research, and evaluate the current trends of this research area. We searched for papers in the following five electronic databases: IEEE Explorer, ACM Digital Library, Science Direct, Springer, and Wiley. Initially, the search string retrieved 202 studies, but upon further examination of titles and abstracts, 120 papers were identified as related to test case prioritization. There exists a large variety of prioritization techniques in the literature, with coverage-based prioritization techniques (i.e., prioritization in terms of the number of statements, basic blocks, or methods test cases cover) dominating the field. The proportion of papers on model-based techniques is on the rise, yet the growth rate is still slow. The proportion of papers that use datasets from industrial projects is found to be 64 \%, while those that utilize public datasets for validation are only 38 \%. On the basis of this study, the following recommendations are provided for researchers: (1) Give preference to public datasets rather than proprietary datasets; (2) develop more model-based prioritization methods; (3) conduct more studies on the comparison of prioritization methods; (4) always evaluate the effectiveness of the proposed technique with well-known evaluation metrics and compare the performance with the existing methods; (5) publish surveys and systematic review papers on test case prioritization; and (6) use datasets from industrial projects that represent real industrial problems.},
	language = {en},
	number = {3},
	urldate = {2016-10-07TZ},
	journal = {Software Quality Journal},
	author = {Catal, Cagatay and Mishra, Deepti},
	month = jul,
	year = {2012},
	note = {00050},
	pages = {445--478}
}

@article{heartfield_taxonomy_2015,
	title = {A {Taxonomy} of {Attacks} and a {Survey} of {Defence} {Mechanisms} for {Semantic} {Social} {Engineering} {Attacks}},
	volume = {48},
	issn = {0360-0300},
	url = {http://doi.acm.org/10.1145/2835375},
	doi = {10.1145/2835375},
	abstract = {Social engineering is used as an umbrella term for a broad spectrum of computer exploitations that employ a variety of attack vectors and strategies to psychologically manipulate a user. Semantic attacks are the specific type of social engineering attacks that bypass technical defences by actively manipulating object characteristics, such as platform or system applications, to deceive rather than directly attack the user. Commonly observed examples include obfuscated URLs, phishing emails, drive-by downloads, spoofed websites and scareware to name a few. This article presents a taxonomy of semantic attacks, as well as a survey of applicable defences. By contrasting the threat landscape and the associated mitigation techniques in a single comparative matrix, we identify the areas where further research can be particularly beneficial.},
	number = {3},
	urldate = {2016-03-07TZ},
	journal = {ACM Comput. Surv.},
	author = {Heartfield, Ryan and Loukas, George},
	month = dec,
	year = {2015},
	note = {00026},
	keywords = {Computer crime, semantic attacks, social engineering attacks, survey},
	pages = {37:1--37:39}
}

@article{artzi_finding_2010,
	title = {Finding {Bugs} in {Web} {Applications} {Using} {Dynamic} {Test} {Generation} and {Explicit}-{State} {Model} {Checking}},
	volume = {36},
	issn = {0098-5589},
	doi = {10.1109/TSE.2010.31},
	abstract = {Web script crashes and malformed dynamically generated webpages are common errors, and they seriously impact the usability of Web applications. Current tools for webpage validation cannot handle the dynamically generated pages that are ubiquitous on today's Internet. We present a dynamic test generation technique for the domain of dynamic Web applications. The technique utilizes both combined concrete and symbolic execution and explicit-state model checking. The technique generates tests automatically, runs the tests capturing logical constraints on inputs, and minimizes the conditions on the inputs to failing tests so that the resulting bug reports are small and useful in finding and fixing the underlying faults. Our tool Apollo implements the technique for the PHP programming language. Apollo generates test inputs for a Web application, monitors the application for crashes, and validates that the output conforms to the HTML specification. This paper presents Apollo's algorithms and implementation, and an experimental evaluation that revealed 673 faults in six PHP Web applications.},
	number = {4},
	journal = {IEEE Transactions on Software Engineering},
	author = {Artzi, S. and Kiezun, A. and Dolby, J. and Tip, F. and Dig, D. and Paradkar, A. and Ernst, M. D.},
	month = jul,
	year = {2010},
	note = {00146},
	keywords = {Apollo tool, Dynamic Analysis, HTML specification, Internet, PHP, PHP Web applications, PHP programming language, Software testing, Web pages, Web script, bugs, dynamic test generation, explicit state model checking, program debugging, program testing, program verification, reliability, software tools, verification., web applications},
	pages = {474--494}
}

@misc{noauthor_noscript_nodate,
	title = {{NoScript} - {JavaScript}/{Java}/{Flash} blocker for a safer {Firefox} experience! - what is it? - {InformAction}},
	url = {https://noscript.net/},
	urldate = {2017-03-30TZ},
	note = {00004}
}

@inproceedings{avancini_grammar_2012,
	title = {Grammar based oracle for security testing of web applications},
	isbn = {978-1-4673-1822-8},
	doi = {10.1109/IWAST.2012.6228984},
	abstract = {The goal of security testing is to detect those defects that could be exploited to conduct attacks. Existing works, however, address security testing mostly from the point of view of automatic generation of test cases. Less attention is paid to the problem of developing and integrating with a security oracle. In this paper we address the problem of the security oracle, in particular for Cross-Site Scripting vulnerabilities. We rely on existing test cases to collect HTML pages in safe conditions, i.e. when no attack is run. Pages are then used to construct the safe model of the application under analysis, a model that describes the structure of an application response page for safe input values. The oracle eventually detects a successful attack when a test makes the application display a web page that is not compliant with the safe model.},
	booktitle = {2012 7th {International} {Workshop} on {Automation} of {Software} {Test}, {AST} 2012 - {Proceedings}},
	author = {Avancini, Andrea and Ceccato, Mariano},
	year = {2012},
	note = {00007},
	keywords = {FV, cross site scripting, security testing, test oracle},
	pages = {15--21}
}

@inproceedings{younis_using_2014-1,
	title = {Using {Attack} {Surface} {Entry} {Points} and {Reachability} {Analysis} to {Assess} the {Risk} of {Software} {Vulnerability} {Exploitability}},
	doi = {10.1109/HASE.2014.10},
	abstract = {An unpatched vulnerability can lead to security breaches. When a new vulnerability is discovered, it needs to be assessed so that it can be prioritized. A major challenge in software security is the assessment of the potential risk due to vulnerability exploitability. CVSS metrics have become a de facto standard that is commonly used to assess the severity of a vulnerability. The CVSS Base Score measures severity based on exploitability and impact measures. CVSS exploitability is measured based on three metrics: Access Vector, Authentication, and Access Complexity. However, CVSS exploitability measures assign subjective numbers based on the views of experts. Two of its factors, Access Vector and Authentication, are the same for almost all vulnerabilities. CVSS does not specify how the third factor, Access Complexity, is measured, and hence we do not know if it considers software properties as a factor. In this paper, we propose an approach that assesses the risk of vulnerability exploitability based on two software properties - attack surface entry points and reach ability analysis. A vulnerability is reachable if it is located in one of the entry points or is located in a function that is called either directly or indirectly by the entry points. The likelihood of an entry point being used in an attack can be assessed by using damage potential-effort ratio in the attack surface metric and the presence of system calls deemed dangerous. To illustrate the proposed method, five reported vulnerabilities of Apache HTTP server 1.3.0 have been examined at the source code level. The results show that the proposed approach, which uses more detailed information, can yield a risk assessment that can be different from the CVSS Base Score.},
	booktitle = {2014 {IEEE} 15th {International} {Symposium} on {High}-{Assurance} {Systems} {Engineering}},
	author = {Younis, A. A. and Malaiya, Y. K. and Ray, I.},
	month = jan,
	year = {2014},
	note = {00019},
	keywords = {Apache HTTP server 1.3.0, Attack Surface, Authentication, CVSS base score, CVSS exploitability, CVSS metrics, Complexity theory, Measurement, Servers, Software, Software Security Metrics, Software Vulnerability, Vectors, access complexity, access vector, attack surface entry point, attack surface metric, damage potential-effort ratio, reachability analysis, risk assessment, risk management, security breach, security of data, severity measurement, software metrics, software security, software vulnerability exploitability},
	pages = {1--8}
}

@inproceedings{medeiros_dekant:_2016,
	address = {New York, NY, USA},
	series = {{ISSTA} 2016},
	title = {{DEKANT}: {A} {Static} {Analysis} {Tool} {That} {Learns} to {Detect} {Web} {Application} {Vulnerabilities}},
	isbn = {978-1-4503-4390-9},
	shorttitle = {{DEKANT}},
	url = {http://doi.acm.org/10.1145/2931037.2931041},
	doi = {10.1145/2931037.2931041},
	abstract = {The state of web security remains troubling as web applications continue to be favorite targets of hackers. Static analysis tools are important mechanisms for programmers to deal with this problem as they search for vulnerabilities automatically in the application source code, allowing programmers to remove them. However, developing these tools requires explicitly coding knowledge about how to discover each kind of vulnerability. This paper presents a new approach in which static analysis tools learn to detect vulnerabilities automatically using machine learning. The approach uses a sequence model to learn to characterize vulnerabilities based on a set of annotated source code slices. This model takes into consideration the order in which the code elements appear and are executed in the slices. The model created can then be used as a static analysis tool to discover and identify vulnerabilities in source code. The approach was implemented in the DEKANT tool and evaluated experimentally with a set of open source PHP applications and WordPress plugins, finding 16 zero-day vulnerabilities.},
	booktitle = {Proceedings of the 25th {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {ACM},
	author = {Medeiros, Ibéria and Neves, Nuno and Correia, Miguel},
	year = {2016},
	note = {00001},
	keywords = {Static analysis, machine learning, sequence models, software security, vulnerabilities, web application},
	pages = {1--11}
}

@inproceedings{pelizzi_protection_2012,
	title = {Protection, usability and improvements in reflected {XSS} filters.},
	booktitle = {{ASIACCS}},
	author = {Pelizzi, Riccardo and Sekar, R},
	year = {2012},
	note = {00028 
bibtex: pelizzi2012protection},
	pages = {5}
}

@article{manadhata_attack_2011,
	title = {An {Attack} {Surface} {Metric}},
	volume = {37},
	issn = {0098-5589},
	doi = {10.1109/TSE.2010.60},
	abstract = {Measurement of software security is a long-standing challenge to the research community. At the same time, practical security metrics and measurements are essential for secure software development. Hence, the need for metrics is more pressing now due to a growing demand for secure software. In this paper, we propose using a software system's attack surface measurement as an indicator of the system's security. We formalize the notion of a system's attack surface and introduce an attack surface metric to measure the attack surface in a systematic manner. Our measurement method is agnostic to a software system's implementation language and is applicable to systems of all sizes; we demonstrate our method by measuring the attack surfaces of small desktop applications and large enterprise systems implemented in C and Java. We conducted three exploratory empirical studies to validate our method. Software developers can mitigate their software's security risk by measuring and reducing their software's attack surfaces. Our attack surface reduction approach complements the software industry's traditional code quality improvement approach for security risk mitigation and is useful in multiple phases of the software development lifecycle. Our collaboration with SAP demonstrates the use of our metric in the software development process.},
	number = {3},
	journal = {IEEE Transactions on Software Engineering},
	author = {Manadhata, P. K. and Wing, J. M.},
	month = may,
	year = {2011},
	note = {00427},
	keywords = {Application software, C language, Code design, Java, Java language, Pressing, Programming, Security, Size measurement, Software measurement, Software quality, Software systems, Time measurement, attack surface metric, implementation language, life cycle, product metrics, protection mechanisms, risk mitigation, security metrics, software development, software metrics, software security, software security.},
	pages = {371--386}
}

@techreport{gennari_measuring_2012,
	title = {Measuring attack surface in software architecture},
	institution = {Technical Report CMU-ISR-11-121, Carnegie Mellon University},
	author = {Gennari, Jeffrey and Garlan, David},
	year = {2012},
	note = {00007 
bibtex: gennari2012measuring}
}

@article{huang_web_2003,
	title = {Web application security assessment by fault injection and behavior monitoring},
	url = {http://portal.acm.org/citation.cfm?doid=775152.775174},
	doi = {10.1145/775173.775174},
	journal = {Proceedings of the twelfth international conference on World Wide Web - WWW '03},
	author = {Huang, Yao-Wen and Huang, Shih-Kun and Lin, Tsung-Po and Tsai, Chung-Hung},
	year = {2003},
	note = {00402},
	keywords = {DN, READ, and behavior monitoring, application security assessment by, fault injection, shih-kun huang, yao-wen huang},
	pages = {148}
}

@misc{godefroid_random_2007,
	title = {Random testing for security: blackbox vs. whitebox fuzzing},
	url = {http://portal.acm.org/citation.cfm?id=1292414.1292416},
	abstract = {Fuzz testing is an effective technique for finding security vulnerabilities in software. Fuzz testing is a form of blackbox random testing which randomly mutates well-formed inputs and tests the program on the resulting data. In some cases, grammars are used to randomly generate the well-formed inputs. This also allows the tester to encode application-specific knowledge (such as corner cases of particular interest) as part of the grammar, and to specify test heuristics by assigning probabilistic weights to production rules. Although fuzz testing can be remarkably effective, the limitations of blackbox random testing are well-known. For instance, the then branch of the conditional statement "if (x==10) then" has only one in 232 chances of being exercised if x is a randomly chosen 32-bit input value. This intuitively explains why random testing usually provides low code coverage. Recently, we have proposed an alternative approach of whitebox fuzz testing 4, building upon recent advances in dynamic symbolic execution and test generation 2. Starting with a well-formed input, our approach symbolically executes the program dynamically and gathers constraints on inputs from conditional statements encountered along the way. The collected constraints are then systematically negated and solved with a constraint solver, yielding new inputs that exercise different execution paths in the program. This process is repeated using a novel search algorithm with a coverage-maximizing heuristic designed to find defects as fast as possible in large search spaces. For example, symbolic execution of the above code fragment on the input x = 0 generates the constraint x 10. Once this constraint is negated and solved, it yields x = 10, which gives us a new input that causes the program to follow the then branch of the given conditional statement. We have implemented this approach in SAGE (Scalable, Automated, Guided Execution), a tool based on x86 instruction-level tracing and emulation for whitebox fuzzing of file-reading Windows applications. While still in an early stage of development and deployment, SAGE has already discovered more than 30 new bugs in large shipped Windows applications including image processors, media players and file decoders. Several of these bugs are potentially exploitable memory access violations. In this talk, I will briefly review blackbox fuzzing for security testing. Then, I will present an overview of our recent work on whitebox fuzzing 4 (joint work with Michael Y. Levin and David Molnar), with an emphasis on the key algorithms and techniques needed to make this approach effective and scalable (see also 1, 3).},
	author = {Godefroid, Patrice},
	year = {2007},
	note = {00051}
}

@article{robertson_static_2009,
	title = {Static {Enforcement} of {Web} {Application} {Integrity} {Through} {Strong} {Typing}},
	volume = {SSYM'09 Pr},
	url = {http://portal.acm.org/citation.cfm?id=1855768.1855786},
	abstract = {Security vulnerabilities continue to plague web applications, allowing attackers to access sensitive data and co-opt legitimate web sites as a hosting ground for malware. Accordingly, researchers have focused on various approaches to detecting and preventing common classes of security vulnerabilities in web applications, including anomaly-based detection mechanisms, static and dynamic analyses of server-side web application code, and client-side security policy enforcement. This paper presents a different approach to web application security. In this work, we present a web application framework that leverages existing work on strong type systems to statically enforce a separation between the structure and content of both web documents and database queries generated by a web application, and show how this approach can automatically prevent the introduction of both server-side cross-site scripting and SQL injection vulnerabilities. We present an evaluation of the framework, and demonstrate both the coverage and correctness of our sanitization functions. Finally, experimental results suggest that web applications developed using this framework perform competitively with applications developed using traditional frameworks.},
	journal = {Proceedings of the 18th conference on},
	author = {Robertson, William and Vigna, Giovanni},
	year = {2009},
	note = {00078},
	keywords = {cross site scripting, functional languages, sql, strongly typed languages, web applications},
	pages = {283--298}
}

@article{buchler_semi-automatic_2012,
	title = {Semi-automatic security testing of web applications from a secure model},
	doi = {10.1109/SERE.2012.38},
	abstract = {Web applications are a major target of attackers. The increasing complexity of such applications and the subtlety of today's attacks make it very hard for developers to manually secure their web applications. Penetration testing is considered an art, the success of a penetration tester in detecting vulnerabilities mainly depends on his skills. Recently, model-checkers dedicated to security analysis have proved their ability to identify complex attacks on web-based security protocols. However, bridging the gap between an abstract attack trace output by a model-checker and a penetration test on the real web application is still an open issue. We present here a methodology for testing web applications starting from a secure model. First, we mutate the model to introduce specific vulnerabilities present in web applications. Then, a model-checker outputs attack traces that exploit those vulnerabilities. Next, the attack traces are translated into concrete test cases by using a 2-step mapping. Finally, the tests are executed on the real system using an automatic procedure that may request the help of a test expert from time to time. A prototype has been implemented and evaluated on Web Goat, an insecure web application maintained by OWASP. It successfully reproduced Role-Based Access Control (RBAC) and Cross-Site Scripting (XSS) attacks.},
	journal = {Proceedings of the 2012 IEEE 6th International Conference on Software Security and Reliability, SERE 2012},
	author = {Büchler, Matthias and Oudinet, Johan and Pretschner, Alexander},
	year = {2012},
	note = {00043},
	keywords = {FV},
	pages = {253--262}
}

@article{bezemer_automated_2009,
	title = {Automated security testing of web widget interactions},
	url = {http://dx.doi.org/10.1145/1595696.1595711},
	doi = {10.1145/1595696.1595711},
	abstract = {We present a technique for automatically detecting security vulnerabilities in client-side self-contained components, called web widgets, that can co-exist independently on a single web page. In this paper we focus on two security scenarios, namely the case in which (1) a malicious widget changes the content (DOM) of another widget, and (2) a widget steals data from another widget and sends it to the server via an HTTP request. We propose a dynamic analysis approach for automatically executing the web application and analyzing the runtime changes in the user interface, as well as the outgoing HTTP calls, to detect inter-widget interaction violations. Our approach, implemented in a number of open source ATUSA plugins, called DIVA, requires no modification of application code, and has few false positives. We discuss the results of an empirical evaluation of the violation revealing capabilities, performance, and scalability of our approach, by means of two case studies, on the Exact Widget Framework and Pageflakes, a commercial, widely used widget framework. Copyright 2009 ACM.},
	journal = {ESEC-FSE'09 - Proceedings of the Joint 12th European Software Engineering Conference and 17th ACM SIGSOFT Symposium on the Foundations of Software Engineering},
	author = {Bezemer, Cor-Paul and Mesbah, Ali and Van Deursen, Arie},
	year = {2009},
	note = {00041},
	keywords = {Computer software, Dynamic analysis, HTTP, User inter},
	pages = {81--90}
}

@article{loukides_utility-aware_2013,
	title = {Utility-{Aware} {Anonymization} of {Diagnosis} {Codes}},
	volume = {17},
	issn = {2168-2194},
	doi = {10.1109/TITB.2012.2212281},
	abstract = {The growing need for performing large-scale and low-cost biomedical studies has led organizations to promote the reuse of patient data. For instance, the National Institutes of Health in the U.S. requires patient-specific data collected and analyzed in the context of Genome-wide Association Studies (GWAS) to be deposited into a biorepository and broadly disseminated. While essential to comply with regulations, disseminating such data risks privacy breaches because patients' genomic sequences can be linked to their identities through diagnosis codes. This paper proposes a novel approach that prevents this type of data linkage by modifying diagnosis codes to limit the probability of associating a patient's identity to their genomic sequence. Our approach employs an effective algorithm that uses generalization and suppression of diagnosis codes to preserve privacy and takes into account the intended uses of the disseminated data to guarantee utility. We also present extensive experiments using several datasets derived from the electronic medical record (EMR) system of the Vanderbilt University Medical Center, as well as a large-scale case study using the EMRs of 79K patients, which are linked to DNA contained in the Vanderbilt University biobank. Our results verify that our approach generates anonymized data that permit accurate biomedical analysis in tasks including case count studies and GWAS.},
	number = {1},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Loukides, G. and Gkoulalas-Divanis, A.},
	month = jan,
	year = {2013},
	note = {00011},
	keywords = {Anonymization, Bioinformatics, Clinical Coding, Cluster Analysis, Computer security, DNA, Databases, Factual, Disease, Educational institutions, Electronic health records, GWAS, Genome-Wide Association Study, Genome-wide Association Studies, Humans, Loss measurement, Medical Informatics, National Institutes of Health, United States, Vanderbilt University Medical Center, biomedical analysis, biorepository, data analysis, data linkage, data privacy, data risk privacy, diagnosis code generalization, diagnosis code suppression, diagnosis codes, electronic medical record system, genomics, large-scale biomedical study, low-cost biomedical study, medical information systems, patient data, patient diagnosis, patient genomic sequence, patient identity, patient-specific data collection, privacy, utility-aware anonymization},
	pages = {60--70}
}

@inproceedings{dahse_simulation_2014,
	title = {Simulation of {Built}-in {PHP} {Features} for {Precise} {Static} {Code} {Analysis}.},
	booktitle = {{NDSS}},
	author = {Dahse, Johannes and Holz, Thorsten},
	year = {2014},
	note = {00033}
}

@inproceedings{tu_beyond_2017,
	title = {Beyond {K}-{Anonymity}: {Protect} {Your} {Trajectory} from {Semantic} {Attack}},
	shorttitle = {Beyond {K}-{Anonymity}},
	doi = {10.1109/SAHCN.2017.7964921},
	abstract = {Nowadays, human trajectories are widely collected and utilized for scientific research and business purpose. However, publishing trajectory data without proper handling might cause severe privacy leakage. A large body of works are dedicated to merging one's trajectory with others', so as to avoid any individual trajectory being re-identified. Yet their solutions do not provide enough protection since they cannot prevent semantic attack, which means the attackers are able to acquire individual's private information by using the semantics features of frequently- visited locations in the trajectory even without re- identification. In this work, we are the first to recognize the semantic attack, which is another severe privacy problem in publishing trajectory datasets. We propose an algorithm providing strong privacy protection against both the semantic and re- identification attack while reserving high data utility. Extensive evaluations based on two real-world datasets demonstrate that our solution improves the quality of privacy protection by 3 times, sacrificing only 36\% and 30\% of spatial and temporal resolution, respectively.},
	booktitle = {2017 14th {Annual} {IEEE} {International} {Conference} on {Sensing}, {Communication}, and {Networking} ({SECON})},
	author = {Tu, Z. and Zhao, K. and Xu, F. and Li, Y. and Su, L. and Jin, D.},
	month = jun,
	year = {2017},
	note = {00001},
	keywords = {Base stations, Publishing, Semantics, Trajectory, Urban areas, data privacy, data protection, data utility, frequently-visited locations, human trajectories, k-anonymity, mobile computing, privacy, privacy leakage, privacy protection, reidentification attack, semantic attack, spatial resolution, temporal resolution},
	pages = {1--9}
}

@inproceedings{bandhakavi_candid:_2007,
	address = {New York, NY, USA},
	series = {{CCS} '07},
	title = {{CANDID}: {Preventing} {Sql} {Injection} {Attacks} {Using} {Dynamic} {Candidate} {Evaluations}},
	isbn = {978-1-59593-703-2},
	shorttitle = {{CANDID}},
	url = {http://doi.acm.org/10.1145/1315245.1315249},
	doi = {10.1145/1315245.1315249},
	abstract = {SQL injection attacks are one of the topmost threats for applications written for the Web. These attacks are launched through specially crafted user input on web applications that use low level string operations to construct SQL queries. In this work, we exhibit a novel and powerful scheme for automatically transforming web applications to render them safe against all SQL injection attacks. A characteristic diagnostic feature of SQL injection attacks is that they change the intended structure of queries issued. Our technique for detecting SQL injection is to dynamically mine the programmer-intended query structure on any input, and detect attacks by comparing it against the structure of the actual query issued. We propose a simple and novel mechanism, called C{\textless}scp{\textgreater}ANDID{\textless}/scp{\textgreater}, for mining programmer intended queries by dynamically evaluating runs over benign candidate inputs. This mechanism is theoretically well founded and is based on inferring intended queries by considering the symbolic query computed on a program run. Our approach has been implemented in a tool called C{\textless}scp{\textgreater}ANDID{\textless}/scp{\textgreater}, that retrofits Web applications written in Java to defend them against SQL injection attacks. We report extensive experimental results that show that our approach performs remarkably well in practice.},
	urldate = {2016-03-21TZ},
	booktitle = {Proceedings of the 14th {ACM} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Bandhakavi, Sruthi and Bisht, Prithvi and Madhusudan, P. and Venkatakrishnan, V. N.},
	year = {2007},
	note = {00141},
	keywords = {SQL Injection attacks, dynamic monitoring, retrofitting code, symbolic evaluation},
	pages = {12--24}
}

@inproceedings{maier_divide-and-conquer:_2014,
	title = {Divide-and-{Conquer}: {Why} {Android} {Malware} {Cannot} {Be} {Stopped}},
	shorttitle = {Divide-and-{Conquer}},
	doi = {10.1109/ARES.2014.12},
	abstract = {In this paper, we demonstrate that Android malware can bypass all automated analysis systems, including AV solutions, mobile sandboxes, and the Google Bouncer. We propose a tool called Sand-Finger for the fingerprinting of Android-based analysis systems. By analyzing the fingerprints of ten unique analysis environments from different vendors, we were able to find characteristics in which all tested environments differ from actual hardware. Depending on the availability of an analysis system, malware can either behave benignly or load malicious code at runtime. We classify this group of malware as Divide-and-Conquer attacks that are efficiently obfuscated by a combination of fingerprinting and dynamic code loading. In this group, we aggregate attacks that work against dynamic as well as static analysis. To demonstrate our approach, we create proof-of-concept malware that surpasses up-to-date malware scanners for Android. We also prove that known malware samples can enter the Google Play Store by modifying them only slightly. Due to Android's lack of an API for malware scanning at runtime, it is impossible for AV solutions to secure Android devices against these attacks.},
	booktitle = {2014 {Ninth} {International} {Conference} on {Availability}, {Reliability} and {Security}},
	author = {Maier, D. and Müller, T. and Protsenko, M.},
	month = sep,
	year = {2014},
	note = {00026},
	keywords = {API, AV, AV solutions, Android (operating system), Android malware, Android-based analysis system fingerprinting, Androids, Google, Google Bouncer, Google Play Store, Hardware, Humanoid robots, Mobile communication, Sand-Finger, Static analysis, Static and Dynamic Analysis, automated analysis systems, divide and conquer methods, divide-and-conquer attacks, dynamic code loading, invasive software, malicious code, malware, malware scanning, mobile computing, mobile sandboxes, obfuscation, program diagnostics, proof-of-concept malware, smart phones},
	pages = {30--39}
}

@inproceedings{calzavara_content_2016,
	title = {Content {Security} {Problems}?: {Evaluating} the {Effectiveness} of {Content} {Security} {Policy} in the {Wild}},
	booktitle = {Proceedings of the 2016 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Calzavara, Stefano and Rabitti, Alvise and Bugliesi, Michele},
	year = {2016},
	note = {00004 
bibtex: calzavara2016content},
	pages = {1365--1375}
}

@inproceedings{taneja_diffgen:_2008,
	title = {{DiffGen}: {Automated} {Regression} {Unit}-{Test} {Generation}},
	shorttitle = {{DiffGen}},
	doi = {10.1109/ASE.2008.60},
	abstract = {Software programs continue to evolve throughout their lifetime. Maintenance of such evolving programs, including regression testing, is one of the most expensive activities in software development. We present an approach and its implementation called DiffGen for automated regression unit-test generation and checking for Java programs. Given two versions of a Java class, our approach instruments the code by adding new branches such that if these branches can be covered by a test generation tool, behavioral differences between the two class versions are exposed. DiffGen then uses a coverage-based test generation tool to generate test inputs for covering the added branches to expose behavioral differences. We have evaluated DiffGen on finding behavioral differences between 21 classes and their versions. Experimental results show that our approach can effectively expose many behavioral differences that cannot be exposed by state-of-the-art techniques.},
	booktitle = {23rd {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}, 2008. {ASE} 2008},
	author = {Taneja, K. and Xie, T.},
	month = sep,
	year = {2008},
	note = {00057},
	keywords = {Computer science, Costs, DiffGen automated regression unit-test generation, Fault detection, Instruments, Java, Java program checking, Production facilities, Programming, Software testing, System testing, coverage-based test generation tool, program testing, program verification, software development, software maintenance, software program maintenance, software tools},
	pages = {407--410}
}

@inproceedings{hooimeijer_fast_2011,
	title = {Fast and {Precise} {Sanitizer} {Analysis} with {BEK}},
	url = {http://static.usenix.org/events/sec11/tech/full_papers/Hooimeijer.pdf},
	doi = {10.1.1.220.7309},
	abstract = {Web applications often use special string-manipulating sanitizers on untrusted user data, but it is difficult to reason manually about the behavior of these functions, leading to errors. For example, the Internet Explorer cross-site scripting filter turned out to transform some web pages without JavaScript into web pages with valid Java-Script, enabling attacks. In other cases, sanitizers may fail to commute, rendering one order of application safe and the other dangerous. BEK is a language and system for writing sanitizers that enables precise analysis of sanitizer behavior, including checking idempotence, commutativity, and equivalence. For example, BEK can determine if a target string, such as an entry on the XSS Cheat Sheet, is a valid output of a sanitizer. If so, our analysis synthesizes an input string that yields that target. Our language is expressive enough to capture real web sanitizers used in ASP.NET, the Internet Explorer XSS Filter, and the Google AutoEscape framework, which we demonstrate by porting these sanitizers to BEK. Our analyses use a novel symbolic finite automata representation to leverage fast satisfiability modulo theories (SMT) solvers and are quick in practice, taking fewer than two seconds to check the commutativity of the entire set of Internet Exporer XSS filters, between 36 and 39 seconds to check implementations of HTMLEncode against target strings from the XSS Cheat Sheet, and less than ten seconds to check equivalence between all pairs of a set of implementations of HTMLEncode. Programs written in BEK can be compiled to traditional languages such as JavaScript and C\#, making it possible for web developers to write sanitizers supported by deep analysis, yet deploy the analyzed code directly to real applications.},
	booktitle = {{SEC}'11: {Proceedings} of the 20th {USENIX} conference on {Security}},
	author = {Hooimeijer, Pieter and Livshits, Benjamin and Molnar, David and Saxena, Prateek and Veanes, Margus},
	year = {2011},
	note = {00124},
	keywords = {READ, ST},
	pages = {1--16}
}

@article{li_two_2014,
	title = {Two decades of {Web} application testing—{A} survey of recent advances},
	volume = {43},
	issn = {03064379},
	url = {http://www.sciencedirect.com/science/article/pii/S0306437914000271},
	doi = {10.1016/j.is.2014.02.001},
	abstract = {Since its inception of just over two decades ago, the World Wide Web has become a truly ubiquitous and transformative force in our life, with millions of Web applications serving billions of Web pages daily. Through a number of evolutions, Web applications have become interactive, dynamic and asynchronous. The Web׳s ubiquity and our reliance on it have made it imperative to ensure the quality, security and correctness of Web applications. Testing is a widely used technique for validating Web applications. It is also a long-standing, active and diverse research area. In this paper, we present a broad survey of recent Web testing advances and discuss their goals, targets, techniques employed, inputs/outputs and stopping criteria.},
	journal = {Information Systems},
	author = {Li, Yuan-Fang and Das, Paramjit K. and Dowe, David L.},
	year = {2014},
	note = {00029},
	keywords = {FV, Software testing, Survey, Web testing, World Wide Web, web applications},
	pages = {20--54}
}

@article{ayala-rivera_ontology-based_2015,
	title = {Ontology-{Based} {Quality} {Evaluation} of {Value} {Generalization} {Hierarchies} for {Data} {Anonymization}},
	url = {http://arxiv.org/abs/1503.01812},
	abstract = {In privacy-preserving data publishing, approaches using Value Generalization Hierarchies (VGHs) form an important class of anonymization algorithms. VGHs play a key role in the utility of published datasets as they dictate how the anonymization of the data occurs. For categorical attributes, it is imperative to preserve the semantics of the original data in order to achieve a higher utility. Despite this, semantics have not being formally considered in the specification of VGHs. Moreover, there are no methods that allow the users to assess the quality of their VGH. In this paper, we propose a measurement scheme, based on ontologies, to quantitatively evaluate the quality of VGHs, in terms of semantic consistency and taxonomic organization, with the aim of producing higher-quality anonymizations. We demonstrate, through a case study, how our evaluation scheme can be used to compare the quality of multiple VGHs and can help to identify faulty VGHs.},
	journal = {arXiv:1503.01812 [cs]},
	author = {Ayala-Rivera, Vanessa and McDonagh, Patrick and Cerqueus, Thomas and Murphy, Liam},
	month = mar,
	year = {2015},
	note = {00004 
arXiv: 1503.01812},
	keywords = {Computer Science - Databases, D.2.0, H.2.0}
}

@article{alshahwan_automated_2011,
	title = {Automated web application testing using search based software engineering},
	url = {http://dl.acm.org/citation.cfm?id=2190141},
	journal = {Ase},
	author = {Alshahwan, Nadia and Harman, Mark},
	year = {2011},
	note = {00079}
}

@incollection{thummalapenta_retrofitting_2011,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Retrofitting {Unit} {Tests} for {Parameterized} {Unit} {Testing}},
	copyright = {©2011 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-642-19810-6 978-3-642-19811-3},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-19811-3_21},
	abstract = {Recent advances in software testing introduced parameterized unit tests (PUT), which accept parameters, unlike conventional unit tests (CUT), which do not accept parameters. PUTs are more beneficial than CUTs with regards to fault-detection capability, since PUTs help describe the behaviors of methods under test for all test arguments. In general, existing applications often include manually written CUTs. With the existence of these CUTs, natural questions that arise are whether these CUTs can be retrofitted as PUTs to leverage the benefits of PUTs, and what are the cost and benefits involved in retrofitting CUTs as PUTs. To address these questions, in this paper, we conduct an empirical study to investigate whether existing CUTs can be retrofitted as PUTs with feasible effort and achieve the benefits of PUTs in terms of additional fault-detection capability and code coverage. We also propose a methodology, called test generalization, that helps in systematically retrofitting existing CUTs as PUTs. Our results on three real-world open-source applications (≈ 4.6 KLOC) show that the retrofitted PUTs detect 19 new defects that are not detected by existing CUTs, and also increase branch coverage by 4\% on average (with maximum increase of 52\% for one class under test and 10\% for one application under analysis) with feasible effort.},
	language = {en},
	number = {6603},
	urldate = {2016-09-19TZ},
	booktitle = {Fundamental {Approaches} to {Software} {Engineering}},
	publisher = {Springer Berlin Heidelberg},
	author = {Thummalapenta, Suresh and Marri, Madhuri R. and Xie, Tao and Tillmann, Nikolai and Halleux, Jonathan de},
	editor = {Giannakopoulou, Dimitra and Orejas, Fernando},
	month = mar,
	year = {2011},
	note = {00011 
DOI: 10.1007/978-3-642-19811-3\_21},
	keywords = {Computer Communication Networks, Logics and Meanings of Programs, Management of Computing and Information Systems, Programming Languages, Compilers, Interpreters, Programming Techniques, Software Engineering},
	pages = {294--309}
}

@inproceedings{parvez_analysis_2015,
	title = {Analysis of effectiveness of black-box web application scanners in detection of stored {SQL} injection and stored {XSS} vulnerabilities},
	doi = {10.1109/ICITST.2015.7412085},
	abstract = {Stored SQL injection (SQLI) and Stored Cross Site Scripting (XSS) are the top most critical web application vulnerabilities in present time. Previous researches have shown that black-box scanners have relatively poor performance in detecting these two vulnerabilities. In this paper, we analyze the performance and detection capabilities of latest black-box web application security scanners against stored SQLI and stored XSS. Our analysis shows that the recent scanners are showing improved performance in detection of stored SQLI and stored XSS. We developed our custom test-bed to challenge the scanners' capabilities to detect stored SQLI and stored XSS. Our analysis revealed that black box scanners still need improvements in detecting stored SQLI and stored XSS vulnerabilities. In addition to the results of performance tests, the paper provides a set of recommendations that could enhance performance of scanners in detecting stored SQLI and stored XSS vulnerabilities.},
	booktitle = {2015 10th {International} {Conference} for {Internet} {Technology} and {Secured} {Transactions} ({ICITST})},
	author = {Parvez, M. and Zavarsky, P. and Khoury, N.},
	month = dec,
	year = {2015},
	note = {00003},
	keywords = {Authentication, Databases, Internet, Java, SQL, Servers, Testing, XSS vulnerabilities, authoring languages, black-box Web application scanners, black-box scanners, security of data, stored Cross-Site Scripting, stored SQL injection, stored SQL injection vulnerability detection, stored SQLI, stored XSS vulnerability detection, stored cross site scripting},
	pages = {186--191}
}

@article{hydara_current_2013,
	title = {Current state of research on cross-site scripting ({XSS}) - {A} systematic literature review},
	volume = {58},
	issn = {09505849},
	url = {http://dx.doi.org/10.1016/j.infsof.2014.07.010},
	doi = {10.1016/j.infsof.2014.07.010},
	abstract = {Context: Cross-site scripting (XSS) is a security vulnerability that affects web applications. It occurs due to improper or lack of sanitization of user inputs. The security vulnerability caused many problems for users and server applications. Objective: To conduct a systematic literature review on the studies done on XSS vulnerabilities and attacks. Method: We followed the standard guidelines for systematic literature review as documented by Barbara Kitchenham and reviewed a total of 115 studies related to cross-site scripting from various journals and conference proceedings. Results: Research on XSS is still very active with publications across many conference proceedings and journals. Attack prevention and vulnerability detection are the areas focused on by most of the studies. Dynamic analysis techniques form the majority among the solutions proposed by the various studies. The type of XSS addressed the most is reflected XSS. Conclusion: XSS still remains a big problem for web applications, despite the bulk of solutions provided so far. There is no single solution that can effectively mitigate XSS attacks. More research is needed in the area of vulnerability removal from the source code of the applications before deployment. © 2014 Elsevier B.V.},
	journal = {Information and Software Technology},
	author = {Hydara, Isatou and Sultan, Abu Bakar Md and Zulzalil, Hazura and Admodisastro, Novia},
	year = {2013},
	note = {00038},
	keywords = {FV, Security, Systematic literature review, cross-site scripting, web applications},
	pages = {170--186}
}

@inproceedings{zhang_bridging_2013,
	title = {Bridging the gap between the total and additional test-case prioritization strategies},
	doi = {10.1109/ICSE.2013.6606565},
	abstract = {In recent years, researchers have intensively investigated various topics in test-case prioritization, which aims to re-order test cases to increase the rate of fault detection during regression testing. The total and additional prioritization strategies, which prioritize based on total numbers of elements covered per test, and numbers of additional (not-yet-covered) elements covered per test, are two widely-adopted generic strategies used for such prioritization. This paper proposes a basic model and an extended model that unify the total strategy and the additional strategy. Our models yield a spectrum of generic strategies ranging between the total and additional strategies, depending on a parameter referred to as the p value. We also propose four heuristics to obtain differentiated p values for different methods under test. We performed an empirical study on 19 versions of four Java programs to explore our results. Our results demonstrate that wide ranges of strategies in our basic and extended models with uniform p values can significantly outperform both the total and additional strategies. In addition, our results also demonstrate that using differentiated p values for both the basic and extended models with method coverage can even outperform the additional strategy using statement coverage.},
	booktitle = {2013 35th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	author = {Zhang, L. and Hao, D. and Zhang, L. and Rothermel, G. and Mei, H.},
	month = may,
	year = {2013},
	note = {00077},
	keywords = {Arrays, Educational institutions, Fault detection, Java, Java programs, Measurement, Software, Testing, heuristics, program testing, regression testing, software fault tolerance, test-case prioritization strategies},
	pages = {192--201}
}

@inproceedings{xiao_precise_2011,
	address = {New York, New York, USA},
	title = {Precise identification of problems for structural test generation},
	isbn = {978-1-4503-0445-0},
	url = {http://portal.acm.org/citation.cfm?doid=1985793.1985876},
	doi = {10.1145/1985793.1985876},
	abstract = {An important goal of software testing is to achieve at least high structural coverage. To reduce the manual efforts of producing such high-covering test inputs, testers or developers can employ tools built based on automated structural test-generation approaches. Although these tools can easily achieve high structural coverage for simple programs, when they are applied on complex programs in practice, these tools face various problems, such as (1) the external-method-call problem (EMCP), where tools cannot deal with method calls to external libraries; (2) the object-creation problem (OCP), where tools fails to generate method-call sequences to produce desirable object states. Since these tools currently could not be powerful enough to deal with these problems in testing complex programs in practice, we propose cooperative developer testing, where developers provide guidance to help tools achieve higher structural coverage. To reduce the efforts of developers in providing guidance to tools, in this paper, we propose a novel approach, called Covana, which precisely identifies and reports problems that prevent the tools from achieving high structural coverage primarily by determining whether branch statements containing notcovered branches have data dependencies on problem candidates. We provide two techniques to instantiate Covana to identify EMCPs and OCPs. Finally, we conduct evaluations on two open source projects to show the effectiveness of Covana in identifying EMCPs and OCPs.},
	booktitle = {Proceeding of the 33rd international conference on {Software} engineering - {ICSE} '11},
	publisher = {ACM Press},
	author = {Xiao, Xusheng and Xie, Tao and Tillmann, Nikolai and de Halleux, Jonathan},
	year = {2011},
	note = {00060},
	keywords = {data dependency, dynamic symbolic execution, problem identification, structural test generation},
	pages = {611}
}

@article{duchene_ligre:_2013,
	title = {{LigRE}: {Reverse}-engineering of control and data flow models for black-box {XSS} detection},
	issn = {10951350},
	doi = {10.1109/WCRE.2013.6671300},
	journal = {Proceedings - Working Conference on Reverse Engineering, WCRE},
	author = {Duchene, Fabien and Rawat, Sanjay and Richier, Jean Luc and Groz, Roland},
	year = {2013},
	note = {00008},
	keywords = {Control Flow Inference, Data-Flow Inference, Reverse-Engineering, penetration testing, web application, xss},
	pages = {252--261}
}

@inproceedings{khan_how_2012,
	title = {How secure is your smartphone: {An} analysis of smartphone security mechanisms},
	shorttitle = {How secure is your smartphone},
	doi = {10.1109/CyberSec.2012.6246082},
	abstract = {Smartphones are becoming more and more popular due to the increase in their processing power, mobility aspect and personal nature. Android is one of the most popular and fully customizable open source mobile platforms that come with a complete software stack. One of the main reasons behind the rapid growth in adoption of smartphones is their capability to facilitate users with third-party applications. Android offers hundreds of thousands of applications via application markets and users can readily install these applications. However, this rapid growth in smartphone usage and the ability to install third-party applications has given rise to several security concerns. In this paper, we present the current state of smartphone security mechanisms and their limitations in order to identify certain security requirements for proposing enhancements for the smartphone security model. We analyze the improvements proposed for the basic Android security model and discuss their advantages and limitations in detail. We also present certain security requirements that need to be fulfilled in order to design and implement security enhancements for Android that can be widely adopted by the broader community.},
	booktitle = {Proceedings {Title}: 2012 {International} {Conference} on {Cyber} {Security}, {Cyber} {Warfare} and {Digital} {Forensic} ({CyberSec})},
	author = {Khan, S. and Nauman, M. and Othman, A. T. and Musa, S.},
	month = jun,
	year = {2012},
	note = {00038},
	keywords = {Android security model, Androids, Humanoid robots, Internet, Runtime, Security, mobile computing, open source mobile platform, operating systems (computers), security of data, smart phone security mechanism, smart phones, software stack, telecommunication security, third-party applications},
	pages = {76--81}
}

@article{von_oheimb_aslan++_2011,
	title = {{ASLan}++ - {A} formal security specification language for distributed systems},
	volume = {6957 LNCS},
	issn = {03029743},
	doi = {10.1007/978-3-642-25271-6_1},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Von Oheimb, David and Mödersheim, Sebastian},
	year = {2011},
	note = {00027},
	keywords = {Security, formal analysis, services, specification language},
	pages = {1--22}
}

@incollection{johns_secure_2010,
	title = {Secure {Code} {Generation} for {Web} {Applications}},
	volume = {5965},
	url = {http://dx.doi.org/10.1007/978-3-642-11747-3_8},
	abstract = {A large percentage of recent security problems, such as Cross-site Scripting or SQL injection, is caused by string-based code injection vulnerabilities. These vulnerabilities exist because of implicit code creation through string serialization. Based on an analysis of the vulnerability class’ underlying mechanisms, we propose a general approach to outfit modern programming languages with mandatory means for explicit and secure code generation which provide strict separation between data and code. Using an exemplified implementation for the languages Java and HTML/JavaScript respectively, we show how our approach can be realized and enforced.},
	booktitle = {Engineering {Secure} {Software} and {Systems}},
	author = {Johns, Martin and Beyerlein, Christian and Giesecke, Rosemaria and Posegga, Joachim},
	year = {2010},
	note = {00033},
	keywords = {READ, VP},
	pages = {96--113}
}

@article{bisht_xss-guard:_2008,
	title = {{XSS}-{GUARD}: {Precise} dynamic prevention of cross-site scripting attacks},
	volume = {5137 LNCS},
	issn = {03029743},
	doi = {10.1007/978-3-540-70542-0_2},
	abstract = {This paper focuses on defense mechanisms for cross-site scripting attacks, the top threat on web applications today. It is believed that input validation (or filtering) can effectively prevent XSS attacks on the server side. In this paper, we discuss several recent real-world XSS attacks and analyze the reasons for the failure of filtering mechanisms in defending these attacks. We conclude that while filtering is useful as a first level of defense against XSS attacks, it is ineffective in preventing several instances of attack, especially when user input includes content-rich HTML. We then propose XSS-Guard, a new framework that is designed to be a prevention mechanism against XSS attacks on the server side. XSS-Guard works by dynamically learning the set of scripts that a web application intends to create for any HTML request. Our approach also includes a robust mechanism for identifying scripts at the server side and removes any script in the output that is not intended by the web application. We discuss extensive experimental results that demonstrate the resilience of XSS-Guard in preventing a number of real-world XSS exploits.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Bisht, Prithvi and Venkatakrishnan, V. N.},
	year = {2008},
	note = {00169},
	keywords = {AP, Attack Prevention, Cross-site scripting (XSS), Filtering, READ, Security},
	pages = {23--43}
}

@inproceedings{bojinov_xcs:_2009,
	title = {{XCS}: cross channel scripting and its impact on web applications},
	booktitle = {Proceedings of the 16th {ACM} conference on {Computer} and communications security},
	publisher = {ACM},
	author = {Bojinov, Hristo and Bursztein, Elie and Boneh, Dan},
	year = {2009},
	note = {00060 
bibtex: bojinov2009xcs},
	pages = {420--431}
}

@inproceedings{ricca_construction_2002,
	title = {Construction of the system dependence graph for {Web} application slicing},
	doi = {10.1109/SCAM.2002.1134112},
	abstract = {The computation of program slices on Web applications may be useful during debugging, when the amount of code to be inspected can be reduced, and during understanding, since the search for a given functionality can be better focused. The system dependence graph is an appropriate data structure for slice computation, in that it explicitly represents all dependences that have to be taken into account in slice determination. In this paper the main problems related to the construction of the system dependence graph are considered. With no loss of generality, solutions are presented with reference to the server side programming language PHP and to the client side language Javascript. Most of the difficulties concern event and hyperlink handling, dynamic generation of HTML code, and direct access to HTML elements by client code. An example of Web application is analyzed, supporting the feasibility of the approach.},
	booktitle = {Second {IEEE} {International} {Workshop} on {Source} {Code} {Analysis} and {Manipulation}, 2002. {Proceedings}},
	author = {Ricca, F. and Tonella, P.},
	year = {2002},
	note = {00045},
	keywords = {Application software, Computer languages, Data structures, HTML, Internet, Java, Javascript client side language, PHP server side programming language, Software safety, Software testing, Web application slicing, client-server systems, data structure, debugging, dynamic HTML code generation, event handling, hyperlink handling, program debugging, program slices, program slicing, program understanding, reverse engineering, software maintenance, system dependence graph},
	pages = {123--132}
}

@article{garn_applicability_2014,
	title = {On the {Applicability} of {Combinatorial} {Testing} to {Web} {Application} {Security} {Testing} : {A} {Case} {Study}},
	author = {Garn, Bernhard},
	year = {2014},
	note = {00013},
	keywords = {Combinatorial testing, penetration testing tools, security testing, web application security},
	pages = {16--21}
}

@article{wu_k-anonymity_2014,
	title = {K-{Anonymity} for {Crowdsourcing} {Database}},
	volume = {26},
	issn = {1041-4347},
	doi = {10.1109/TKDE.2013.93},
	abstract = {In crowdsourcing database, human operators are embedded into the database engine and collaborate with other conventional database operators to process the queries. Each human operator publishes small HITs (Human Intelligent Task) to the crowdsourcing platform, which consists of a set of database records and corresponding questions for human workers. The human workers complete the HITs and return the results to the crowdsourcing database for further processing. In practice, published records in HITs may contain sensitive attributes, probably causing privacy leakage so that malicious workers could link them with other public databases to reveal individual private information. Conventional privacy protection techniques, such as K-Anonymity, can be applied to partially solve the problem. However, after generalizing the data, the result of standard K-Anonymity algorithms may render uncontrollable information loss and affects the accuracy of crowdsourcing. In this paper, we first study the tradeoff between the privacy and accuracy for the human operator within data anonymization process. A probability model is proposed to estimate the lower bound and upper bound of the accuracy for general K-Anonymity approaches. We show that searching the optimal anonymity approach is NP-Hard and only heuristic approach is available. The second contribution of the paper is a general feedback-based K-Anonymity scheme. In our scheme, synthetic samples are published to the human workers, the results of which are used to guide the selection on anonymity strategies. We apply the scheme on Mondrian algorithm by adaptively cutting the dimensions based on our feedback results on the synthetic samples. We evaluate the performance of the feedback-based approach on U.S. census dataset, and show that given a predefined K, our proposal outperforms standard K-Anonymity approaches on retaining the effectiveness of crowdsourcing.},
	number = {9},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Wu, S. and Wang, X. and Wang, S. and Zhang, Z. and Tung, A. K. H.},
	month = sep,
	year = {2014},
	note = {00017},
	keywords = {Accuracy, Crowdsourcing, Database Management, Databases, Engines, General, HIT, Information Technology and Systems, Mondrian algorithm, NP-hard, Query design and implementation languages, Security, US census dataset, Upper bound, and protection, computational complexity, crowdsourcing database, crowdsourcing platform, data anonymization process, data partition, data privacy, database engine, database management systems, database operators, database privacy, general feedback-based k-anonymity scheme, human intelligent task, human operators, human workers, integrity, k-anonymity, malicious workers, privacy, privacy protection techniques, public databases, published records, query processing, uncontrollable information loss},
	pages = {2207--2221}
}

@article{xie_idea:_2011,
	title = {Idea: {Interactive} support for secure software development},
	volume = {6542 LNCS},
	issn = {03029743},
	doi = {10.1007/978-3-642-19125-1_19},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Xie, Jing and Chu, Bill and Richter Lipford, Heather},
	year = {2011},
	note = {00007},
	keywords = {code annotation, code refactoring, secure programming, security software development},
	pages = {248--255}
}

@inproceedings{weichselbaum_csp_2016,
	address = {Vienna, Austria},
	title = {{CSP} {Is} {Dead}, {Long} {Live} {CSP}! {On} the {Insecurity} of {Whitelists} and the {Future} of {Content} {Security} {Policy}},
	booktitle = {Proceedings of the 23rd {ACM} {Conference} on {Computer} and {Communications} {Security}},
	author = {Weichselbaum, Lukas and Spagnuolo, Michele and Lekies, Sebastian and Janc, Artur},
	year = {2016},
	note = {00005 
bibtex: 45542}
}

@inproceedings{doupe_fear_2011,
	address = {New York, NY, USA},
	series = {{CCS} '11},
	title = {Fear the {EAR}: {Discovering} and {Mitigating} {Execution} {After} {Redirect} {Vulnerabilities}},
	isbn = {978-1-4503-0948-6},
	shorttitle = {Fear the {EAR}},
	url = {http://doi.acm.org/10.1145/2046707.2046736},
	doi = {10.1145/2046707.2046736},
	abstract = {The complexity of modern web applications makes it difficult for developers to fully understand the security implications of their code. Attackers exploit the resulting security vulnerabilities to gain unauthorized access to the web application environment. Previous research into web application vulnerabilities has mostly focused on input validation flaws, such as cross site scripting and SQL injection, while logic flaws have received comparably less attention. In this paper, we present a comprehensive study of a relatively unknown logic flaw in web applications, which we call Execution After Redirect, or EAR. A web application developer can introduce an EAR by calling a redirect method under the assumption that execution will halt. A vulnerability occurs when server-side execution continues after the developer's intended halting point, which can lead to broken/insufficient access controls and information leakage. We start with an analysis of how susceptible applications written in nine web frameworks are to EAR vulnerabilities. We then discuss the results from the EAR challenge contained within the 2010 International Capture the Flag Competition. Finally, we present an open-source, white-box, static analysis tool to detect EARs in Ruby on Rails web applications. This tool found 3,944 EAR instances in 18,127 open-source applications. Finally, we describe an approach to prevent EARs in web frameworks.},
	urldate = {2016-03-13TZ},
	booktitle = {Proceedings of the 18th {ACM} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Doupé, Adam and Boe, Bryce and Kruegel, Christopher and Vigna, Giovanni},
	year = {2011},
	note = {00039},
	keywords = {Static analysis, execution after redirect, web applications},
	pages = {251--262}
}

@inproceedings{halfond_amnesia:_2005,
	address = {New York, NY, USA},
	series = {{ASE} '05},
	title = {{AMNESIA}: {Analysis} and {Monitoring} for {NEutralizing} {SQL}-injection {Attacks}},
	isbn = {978-1-58113-993-8},
	shorttitle = {{AMNESIA}},
	url = {http://doi.acm.org/10.1145/1101908.1101935},
	doi = {10.1145/1101908.1101935},
	abstract = {The use of web applications has become increasingly popular in our routine activities, such as reading the news, paying bills, and shopping on-line. As the availability of these services grows, we are witnessing an increase in the number and sophistication of attacks that target them. In particular, SQL injection, a class of code-injection attacks in which specially crafted input strings result in illegal queries to a database, has become one of the most serious threats to web applications. In this paper we present and evaluate a new technique for detecting and preventing SQL injection attacks. Our technique uses a model-based approach to detect illegal queries before they are executed on the database. In its static part, the technique uses program analysis to automatically build a model of the legitimate queries that could be generated by the application. In its dynamic part, the technique uses runtime monitoring to inspect the dynamically-generated queries and check them against the statically-built model. We developed a tool, AMNESIA, that implements our technique and used the tool to evaluate the technique on seven web applications. In the evaluation we targeted the subject applications with a large number of both legitimate and malicious inputs and measured how many attacks our technique detected and prevented. The results of the study show that our technique was able to stop all of the attempted attacks without generating any false positives.},
	booktitle = {Proceedings of the 20th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {ACM},
	author = {Halfond, William G. J. and Orso, Alessandro},
	year = {2005},
	note = {00596},
	keywords = {SQL injection, Static analysis, runtime monitoring},
	pages = {174--183}
}

@inproceedings{haldar_dynamic_2005,
	title = {Dynamic taint propagation for {Java}},
	doi = {10.1109/CSAC.2005.21},
	abstract = {Improperly validated user input is the underlying root cause for a wide variety of attacks on Web-based applications. Static approaches for detecting this problem help at the time of development, but require source code and report a number of false positives. Hence, they are of little use for securing fully deployed and rapidly evolving applications. We propose a dynamic solution that tags and tracks user input at runtime and prevents its improper use to maliciously affect the execution of the program. Our implementation can be transparently applied to Java classfiles, and does not require source code. Benchmarks show that the overhead of this runtime enforcement is negligible and can prevent a number of attacks},
	booktitle = {21st {Annual} {Computer} {Security} {Applications} {Conference} ({ACSAC}'05)},
	author = {Haldar, V. and Chandra, D. and Franz, M.},
	month = dec,
	year = {2005},
	note = {00238},
	keywords = {Computer science, HTML, Internet, Java, Java classfiles, Logic programming, Memory management, Protocols, Read-write memory, Runtime, Virtual machining, Web-based application, dynamic taint propagation, safety, source code},
	pages = {9 pp.--311}
}

@inproceedings{balzarotti_saner:_2008,
	title = {Saner: {Composing} {Static} and {Dynamic} {Analysis} to {Validate} {Sanitization} in {Web} {Applications}},
	isbn = {978-0-7695-3168-7},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4531166},
	doi = {10.1109/SP.2008.22},
	abstract = {Web applications are ubiquitous, perform mission- critical tasks, and handle sensitive user data. Unfortunately, web applications are often implemented by developers with limited security skills, and, as a result, they contain vulnerabilities. Most of these vulnerabilities stem from the lack of input validation. That is, web applications use malicious input as part of a sensitive operation, without having properly checked or sanitized the input values prior to their use. Past research on vulnerability analysis has mostly focused on identifying cases in which a web application directly uses external input in critical operations. However, little research has been performed to analyze the correctness of the sanitization process. Thus, whenever a web application applies some sanitization routine to potentially malicious input, the vulnerability analysis assumes that the result is innocuous. Unfortunately, this might not be the case, as the sanitization process itself could be incorrect or incomplete. In this paper, we present a novel approach to the analysis of the sanitization process. More precisely, we combine static and dynamic analysis techniques to identify faulty sanitization procedures that can be bypassed by an attacker. We implemented our approach in a tool, called Saner, and we applied it to a number of real-world applications. Our results demonstrate that we were able to identify several novel vulnerabilities that stem from erroneous sanitization procedures.},
	booktitle = {2008 {IEEE} {Symposium} on {Security} and {Privacy} (sp 2008)},
	publisher = {IEEE},
	author = {Balzarotti, Davide and Cova, Marco and Felmetsger, Vika and Jovanovic, Nenad and Kirda, Engin and Kruegel, Christopher and Vigna, Giovanni},
	month = may,
	year = {2008},
	note = {00348},
	keywords = {DN, FV, ST},
	pages = {387--401}
}
@article{meng_collaborative_2015,
	title = {Collaborative {Security}: {A} {Survey} and {Taxonomy}},
	volume = {48},
	issn = {0360-0300},
	shorttitle = {Collaborative {Security}},
	url = {http://doi.acm.org/10.1145/2785733},
	doi = {10.1145/2785733},
	abstract = {Security is oftentimes centrally managed. An alternative trend of using collaboration in order to improve security has gained momentum over the past few years. Collaborative security is an abstract concept that applies to a wide variety of systems and has been used to solve security issues inherent in distributed environments. Thus far, collaboration has been used in many domains such as intrusion detection, spam filtering, botnet resistance, and vulnerability detection. In this survey, we focus on different mechanisms of collaboration and defense in collaborative security. We systematically investigate numerous use cases of collaborative security by covering six types of security systems. Aspects of these systems are thoroughly studied, including their technologies, standards, frameworks, strengths and weaknesses. We then present a comprehensive study with respect to their analysis target, timeliness of analysis, architecture, network infrastructure, initiative, shared information and interoperability. We highlight five important topics in collaborative security, and identify challenges and possible directions for future research. Our work contributes the following to the existing research on collaborative security with the goal of helping to make collaborative security systems more resilient and efficient. This study (1) clarifies the scope of collaborative security, (2) identifies the essential components of collaborative security, (3) analyzes the multiple mechanisms of collaborative security, and (4) identifies challenges in the design of collaborative security.},
	number = {1},
	urldate = {2016-03-07TZ},
	journal = {ACM Comput. Surv.},
	author = {Meng, Guozhu and Liu, Yang and Zhang, Jie and Pokluda, Alexander and Boutaba, Raouf},
	month = jul,
	year = {2015},
	note = {00018},
	keywords = {Collaborative security, information sharing, intrusion detection, malware, privacy, spam, taxonomy, trust},
	pages = {1:1--1:42}
}

@inproceedings{nunan_automatic_2012,
	title = {Automatic classification of cross-site scripting in web pages using document-based and {URL}-based features},
	booktitle = {Computers and {Communications} ({ISCC}), 2012 {IEEE} {Symposium} on},
	publisher = {IEEE},
	author = {Nunan, Angelo Eduardo and Souto, Eduardo and dos Santos, Eulanda M and Feitosa, Eduardo},
	year = {2012},
	note = {00025},
	pages = {702--707}
}

@inproceedings{zheng_path_2013,
	title = {Path sensitive static analysis of web applications for remote code execution vulnerability detection},
	booktitle = {Proceedings of the 2013 {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Zheng, Yunhui and Zhang, Xiangyu},
	year = {2013},
	note = {00029 
bibtex: zheng2013path},
	pages = {652--661}
}

@inproceedings{chen_aspire:_2015,
	title = {{ASPIRE}: iterative specification synthesis for security},
	booktitle = {15th {Workshop} on {Hot} {Topics} in {Operating} {Systems} ({HotOS} {XV})},
	author = {Chen, Kevin and He, Warren and Akhawe, Devdatta and D'Silva, Vijay and Mittal, Prateek and Song, Dawn},
	year = {2015},
	note = {00002 
bibtex: chen2015aspire}
}

@article{joh_defining_2011,
	title = {Defining and assessing quantitative security risk measures using vulnerability lifecycle and cvss metrics},
	abstract = {... Cox [7] has pointed out that the discretization in a risk matrix can potentially result in incorrect ranking , but risk matrices are often used for convenient visualization. ... We will next examine the CVSS metrics that has emerged recently for software security vulnerabilities, and ...},
	journal = {The 2011 international conference on},
	author = {Joh, H C and Malaiya, Y K},
	year = {2011},
	note = {00025 
bibtex: Joh2011-xi}
}

@inproceedings{guo_characterizing_2013,
	title = {Characterizing and detecting resource leaks in {Android} applications},
	doi = {10.1109/ASE.2013.6693097},
	abstract = {Android phones come with a host of hardware components embedded in them, such as Camera, Media Player and Sensor. Most of these components are exclusive resources or resources consuming more memory/energy than general. And they should be explicitly released by developers. Missing release operations of these resources might cause serious problems such as performance degradation or system crash. These kinds of defects are called resource leaks. This paper focuses on resource leak problems in Android apps, and presents our lightweight static analysis tool called Relda, which can automatically analyze an application's resource operations and locate the resource leaks. We propose an automatic method for detecting resource leaks based on a modified Function Call Graph, which handles the features of event-driven mobile programming by analyzing the callbacks defined in Android framework. Our experimental data shows that Relda is effective in detecting resource leaks in real Android apps.},
	booktitle = {2013 28th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Guo, C. and Zhang, J. and Yan, J. and Zhang, Z. and Zhang, Y.},
	month = nov,
	year = {2013},
	note = {00048},
	keywords = {Android (operating system), Android applications, Android apps, Androids, Cameras, Humanoid robots, Java, Media, Mobile communication, Relda tool, Static analysis, application resource operations, callbacks, camera component, event-driven mobile programming, function call graph, lightweight static analysis tool, media player component, mobile computing, program diagnostics, resource allocation, resource leak, resource leak problems, resource leaks characterization, resource leaks detection, sensor component, smart phones},
	pages = {389--398}
}

@article{gedik_protecting_2008,
	title = {Protecting {Location} {Privacy} with {Personalized} k-{Anonymity}: {Architecture} and {Algorithms}},
	volume = {7},
	issn = {1536-1233},
	shorttitle = {Protecting {Location} {Privacy} with {Personalized} k-{Anonymity}},
	doi = {10.1109/TMC.2007.1062},
	abstract = {Continued advances in mobile networks and positioning technologies have created a strong market push for location-based applications. Examples include location-aware emergency response, location-based advertisement, and location-based entertainment. An important challenge in the wide deployment of location-based services (LBSs) is the privacy-aware management of location information, providing safeguards for location privacy of mobile clients against vulnerabilities for abuse. This paper describes a scalable architecture for protecting the location privacy from various privacy threats resulting from uncontrolled usage of LBSs. This architecture includes the development of a personalized location anonymization model and a suite of location perturbation algorithms. A unique characteristic of our location privacy architecture is the use of a flexible privacy personalization framework to support location k-anonymity for a wide range of mobile clients with context-sensitive privacy requirements. This framework enables each mobile client to specify the minimum level of anonymity that it desires and the maximum temporal and spatial tolerances that it is willing to accept when requesting k-anonymity-preserving LBSs. We devise an efficient message perturbation engine to implement the proposed location privacy framework. The prototype that we develop is designed to be run by the anonymity server on a trusted platform and performs location anonymization on LBS request messages of mobile clients such as identity removal and spatio-temporal cloaking of the location information. We study the effectiveness of our location cloaking algorithms under various conditions by using realistic location data that is synthetically generated from real road maps and traffic volume data. Our experiments show that the personalized location k-anonymity model, together with our location perturbation engine, can achieve high resilience to location privacy threats without introducing any significan- performance penalty.},
	number = {1},
	journal = {IEEE Transactions on Mobile Computing},
	author = {Gedik, B. and Liu, L.},
	month = jan,
	year = {2008},
	note = {00675},
	keywords = {Business, Disaster management, Engines, Information management, Location Privacy, Location-based Applications, Mobile Computing Systems, Protection, Prototypes, Resilience, Roads, Traffic control, context-sensitive privacy requirements, data privacy, flexible privacy personalization framework, k-anonymity, location cloaking algorithms, location perturbation algorithms, location perturbation engine, location privacy protection, location-based services, message perturbation engine, mobile client, mobile computing, mobility management (mobile radio), personalized k-anonymity, personalized location anonymization model, road maps, scalable architecture, security of data, telecommunication security, traffic volume data},
	pages = {1--18}
}

@inproceedings{lefevre_mondrian_2006,
	address = {Washington, DC, USA},
	series = {{ICDE} '06},
	title = {Mondrian {Multidimensional} {K}-{Anonymity}},
	isbn = {978-0-7695-2570-9},
	url = {https://doi.org/10.1109/ICDE.2006.101},
	doi = {10.1109/ICDE.2006.101},
	abstract = {K-Anonymity has been proposed as a mechanism for protecting privacy in microdata publishing, and numerous recoding "models" have been considered for achieving anonymity. This paper proposes a new multidimensional model, which provides an additional degree of flexibility not seen in previous (single-dimensional) approaches. Often this flexibility leads to higher-quality anonymizations, as measured both by general-purpose metrics and more specific notions of query answerability. Optimal multidimensional anonymization is NP-hard (like previous optimal anonymity problems). However, we introduce a simple greedy approximation algorithm, and experimental results show that this greedy algorithm frequently leads to more desirable anonymizations than exhaustive optimal algorithms for two single-dimensional models.},
	booktitle = {Proceedings of the 22Nd {International} {Conference} on {Data} {Engineering}},
	publisher = {IEEE Computer Society},
	author = {LeFevre, Kristen and DeWitt, David J. and Ramakrishnan, Raghu},
	year = {2006},
	note = {01053},
	pages = {25--}
}

@inproceedings{lefevre_incognito:_2005,
	address = {New York, NY, USA},
	series = {{SIGMOD} '05},
	title = {Incognito: {Efficient} {Full}-domain {K}-anonymity},
	isbn = {978-1-59593-060-6},
	shorttitle = {Incognito},
	url = {http://doi.acm.org/10.1145/1066157.1066164},
	doi = {10.1145/1066157.1066164},
	abstract = {A number of organizations publish microdata for purposes such as public health and demographic research. Although attributes that clearly identify individuals, such as Name and Social Security Number, are generally removed, these databases can sometimes be joined with other public databases on attributes such as Zipcode, Sex, and Birthdate to re-identify individuals who were supposed to remain anonymous. "Joining" attacks are made easier by the availability of other, complementary, databases over the Internet.K-anonymization is a technique that prevents joining attacks by generalizing and/or suppressing portions of the released microdata so that no individual can be uniquely distinguished from a group of size k. In this paper, we provide a practical framework for implementing one model of k-anonymization, called full-domain generalization. We introduce a set of algorithms for producing minimal full-domain generalizations, and show that these algorithms perform up to an order of magnitude faster than previous algorithms on two real-life databases.Besides full-domain generalization, numerous other models have also been proposed for k-anonymization. The second contribution in this paper is a single taxonomy that categorizes previous models and introduces some promising new alternatives.},
	booktitle = {Proceedings of the 2005 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {LeFevre, Kristen and DeWitt, David J. and Ramakrishnan, Raghu},
	year = {2005},
	note = {01215},
	pages = {49--60}
}

@article{tassa_k-concealment:_2012,
	title = {k-{Concealment}: {An} {Alternative} {Model} of k-{Type} {Anonymity}},
	volume = {5},
	issn = {1888-5063},
	shorttitle = {k-{Concealment}},
	url = {http://dl.acm.org/citation.cfm?id=2207141.2207142},
	abstract = {We introduce a new model of k-type anonymity, called k-concealment, as an alternative to the well-known model of k-anonymity. This new model achieves similar privacy goals as k-anonymity: While in k-anonymity one generalizes the table records so that each one of them becomes equal to at least k-1 other records, when projected on the subset of quasi-identifiers, k-concealment proposes to generalize the table records so that each one of them becomes computationally-indistinguishable from at least k-1 others. As the new model extends that of k-anonymity, it offers higher utility. To motivate the new model and to lay the ground for its introduction, we first present three other models, called (1, k)-, (k, 1)-and (k, k)-anonymity which also extend k-anonymity. We characterize the interrelation between the four models and propose algorithms for anonymizing data according to them. Since k-anonymity, on its own, is insecure, as it may allow adversaries to learn the sensitive information of some individuals, it must be enhanced by a security measure such as p-sensitivity or l-diversity. We show how also k-concealment can be enhanced by such measures. We demonstrate the usefulness of our models and algorithms through extensive experiments.},
	number = {1},
	journal = {Trans. Data Privacy},
	author = {Tassa, Tamir and Mazza, Arnon and Gionis, Aristides},
	month = apr,
	year = {2012},
	note = {00042},
	pages = {189--222}
}

@article{kenig_practical_2012,
	title = {A practical approximation algorithm for optimal k-anonymity},
	volume = {25},
	issn = {1384-5810, 1573-756X},
	url = {https://link.springer.com/article/10.1007/s10618-011-0235-9},
	doi = {10.1007/s10618-011-0235-9},
	abstract = {k-Anonymity is a privacy preserving method for limiting disclosure of private information in data mining. The process of anonymizing a database table typically involves generalizing table entries and, consequently, it incurs loss of relevant information. This motivates the search for anonymization algorithms that achieve the required level of anonymization while incurring a minimal loss of information. The problem of k-anonymization with minimal loss of information is NP-hard. We present a practical approximation algorithm that enables solving the k-anonymization problem with an approximation guarantee of O(ln k). That algorithm improves an algorithm due to Aggarwal et al. (Proceedings of the international conference on database theory (ICDT), 2005) that offers an approximation guarantee of O(k), and generalizes that of Park and Shim (SIGMOD ’07: proceedings of the 2007 ACM SIGMOD international conference on management of data, 2007) that was limited to the case of generalization by suppression. Our algorithm uses techniques that we introduce herein for mining closed frequent generalized records. Our experiments show that the significance of our algorithm is not limited only to the theory of k-anonymization. The proposed algorithm achieves lower information losses than the leading approximation algorithm, as well as the leading heuristic algorithms. A modified version of our algorithm that issues ℓ-diverse k-anonymizations also achieves lower information losses than the corresponding modified versions of the leading algorithms.},
	language = {en},
	number = {1},
	urldate = {2017-09-05TZ},
	journal = {Data Mining and Knowledge Discovery},
	author = {Kenig, Batya and Tassa, Tamir},
	month = jul,
	year = {2012},
	note = {00031},
	pages = {134--168}
}

@article{verykios_state---art_2004,
	title = {State-of-the-art in {Privacy} {Preserving} {Data} {Mining}},
	volume = {33},
	issn = {0163-5808},
	url = {http://doi.acm.org/10.1145/974121.974131},
	doi = {10.1145/974121.974131},
	abstract = {We provide here an overview of the new and rapidly emerging research area of privacy preserving data mining. We also propose a classification hierarchy that sets the basis for analyzing the work which has been performed in this context. A detailed review of the work accomplished in this area is also given, along with the coordinates of each work to the classification hierarchy. A brief evaluation is performed, and some initial conclusions are made.},
	number = {1},
	journal = {SIGMOD Rec.},
	author = {Verykios, Vassilios S. and Bertino, Elisa and Fovino, Igor Nai and Provenza, Loredana Parasiliti and Saygin, Yucel and Theodoridis, Yannis},
	month = mar,
	year = {2004},
	note = {00879},
	pages = {50--57}
}

@inproceedings{agrawal_design_2001,
	address = {New York, NY, USA},
	series = {{PODS} '01},
	title = {On the {Design} and {Quantification} of {Privacy} {Preserving} {Data} {Mining} {Algorithms}},
	isbn = {978-1-58113-361-5},
	url = {http://doi.acm.org/10.1145/375551.375602},
	doi = {10.1145/375551.375602},
	abstract = {The increasing ability to track and collect large amounts of data with the use of current hardware technology has lead to an interest in the development of data mining algorithms which preserve user privacy. A recently proposed technique addresses the issue of privacy preservation by perturbing the data and reconstructing distributions at an aggregate level in order to perform the mining. This method is able to retain privacy while accessing the information implicit in the original attributes. The distribution reconstruction process naturally leads to some loss of information which is acceptable in many practical situations. This paper discusses an Expectation Maximization (EM) algorithm for distribution reconstruction which is more effective than the currently available method in terms of the level of information loss. Specifically, we prove that the EM algorithm converges to the maximum likelihood estimate of the original distribution based on the perturbed data. We show that when a large amount of data is available, the EM algorithm provides robust estimates of the original distribution. We propose metrics for quantification and measurement of privacy-preserving data mining algorithms. Thus, this paper provides the foundations for measurement of the effectiveness of privacy preserving data mining algorithms. Our privacy metrics illustrate some interesting results on the relative effectiveness of different perturbing distributions.},
	booktitle = {Proceedings of the {Twentieth} {ACM} {SIGMOD}-{SIGACT}-{SIGART} {Symposium} on {Principles} of {Database} {Systems}},
	publisher = {ACM},
	author = {Agrawal, Dakshi and Aggarwal, Charu C.},
	year = {2001},
	note = {01145},
	pages = {247--255}
}

@inproceedings{bayardo_data_2005,
	title = {Data privacy through optimal k-anonymization},
	doi = {10.1109/ICDE.2005.42},
	abstract = {Data de-identification reconciles the demand for release of data for research purposes and the demand for privacy from individuals. This paper proposes and evaluates an optimization algorithm for the powerful de-identification procedure known as k-anonymization. A k-anonymized dataset has the property that each record is indistinguishable from at least k - 1 others. Even simple restrictions of optimized k-anonymity are NP-hard, leading to significant computational challenges. We present a new approach to exploring the space of possible anonymizations that tames the combinatorics of the problem, and develop data-management strategies to reduce reliance on expensive operations such as sorting. Through experiments on real census data, we show the resulting algorithm can find optimal k-anonymizations under two representative cost measures and a wide range of k. We also show that the algorithm can produce good anonymizations in circumstances where the input data or input parameters preclude finding an optimal solution in reasonable time. Finally, we use the algorithm to explore the effects of different coding approaches and problem variations on anonymization quality and performance. To our knowledge, this is the first result demonstrating optimal k-anonymization of a non-trivial dataset under a general model of the problem.},
	booktitle = {21st {International} {Conference} on {Data} {Engineering} ({ICDE}'05)},
	author = {Bayardo, R. J. and Agrawal, Rakesh},
	month = apr,
	year = {2005},
	note = {01163},
	keywords = {Costs, Data engineering, Frequency, Iterative algorithms, Machine learning algorithms, Proposals, Stochastic processes, data deidentification, data integrity, data privacy, data-management strategies, database management systems, machine learning, optimal k-anonymized dataset, optimisation, optimization algorithm, sorting, sorting operation, tree searching},
	pages = {217--228}
}

@inproceedings{domingo-ferrer_comparing_2001,
	title = {Comparing {SDC} {Methods} for {Microdata} on the {Basis} of {Information} {Loss} and {Disclosure}},
	abstract = {We present in this paper the first empirical comparison of SDC methods  for microdata which encompasses both continuous and categorical microdata. Based  on re-identification experiments, we try to optimize the tradeoff between information  loss and disclosure risk. First, relevant SDC methods for continuous and categorical  microdata are identified. Then generic information loss measures (not targeted to  specific data uses) are defined, both in the continuous and the categorical case.},
	booktitle = {Proceedings of {ETK}-{NTTS} 2001, {Luxemburg}: {Eurostat}},
	publisher = {Eurostat},
	author = {Domingo-ferrer, Josep and Mateo-sanz, Josep M. and Torra, Vicenç},
	year = {2001},
	note = {00136},
	pages = {807--826}
}

@article{chen_privacy-preserving_2009,
	title = {Privacy-{Preserving} {Data} {Publishing}},
	volume = {2},
	issn = {1931-7883},
	url = {http://dx.doi.org/10.1561/1900000008},
	doi = {10.1561/1900000008},
	abstract = {Privacy is an important issue when one wants to make use of data that involves individuals' sensitive information. Research on protecting the privacy of individuals and the confidentiality of data has received contributions from many fields, including computer science, statistics, economics, and social science. In this paper, we survey research work in privacy-preserving data publishing. This is an area that attempts to answer the problem of how an organization, such as a hospital, government agency, or insurance company, can release data to the public without violating the confidentiality of personal information. We focus on privacy criteria that provide formal safety guarantees, present algorithms that sanitize data to make it safe for release while preserving useful information, and discuss ways of analyzing the sanitized data. Many challenges still remain. This survey provides a summary of the current state-of-the-art, based on which we expect to see advances in years to come.},
	number = {1–2},
	journal = {Found. Trends databases},
	author = {Chen, Bee-Chung and Kifer, Daniel and LeFevre, Kristen and Machanavajjhala, Ashwin},
	month = jan,
	year = {2009},
	note = {00825},
	pages = {1--167}
}

@article{fung_privacy-preserving_2010,
	title = {Privacy-preserving {Data} {Publishing}: {A} {Survey} of {Recent} {Developments}},
	volume = {42},
	issn = {0360-0300},
	shorttitle = {Privacy-preserving {Data} {Publishing}},
	url = {http://doi.acm.org/10.1145/1749603.1749605},
	doi = {10.1145/1749603.1749605},
	abstract = {The collection of digital information by governments, corporations, and individuals has created tremendous opportunities for knowledge- and information-based decision making. Driven by mutual benefits, or by regulations that require certain data to be published, there is a demand for the exchange and publication of data among various parties. Data in its original form, however, typically contains sensitive information about individuals, and publishing such data will violate individual privacy. The current practice in data publishing relies mainly on policies and guidelines as to what types of data can be published and on agreements on the use of published data. This approach alone may lead to excessive data distortion or insufficient protection. Privacy-preserving data publishing (PPDP) provides methods and tools for publishing useful information while preserving data privacy. Recently, PPDP has received considerable attention in research communities, and many approaches have been proposed for different data publishing scenarios. In this survey, we will systematically summarize and evaluate different approaches to PPDP, study the challenges in practical data publishing, clarify the differences and requirements that distinguish PPDP from other related problems, and propose future research directions.},
	number = {4},
	journal = {ACM Comput. Surv.},
	author = {Fung, Benjamin C. M. and Wang, Ke and Chen, Rui and Yu, Philip S.},
	month = jun,
	year = {2010},
	note = {00825},
	keywords = {anonymity, data mining, information sharing, privacy protection, sensitive information},
	pages = {14:1--14:53}
}

@inproceedings{truta_privacy_2006,
	title = {Privacy {Protection}: p-{Sensitive} k-{Anonymity} {Property}},
	shorttitle = {Privacy {Protection}},
	doi = {10.1109/ICDEW.2006.116},
	abstract = {In this paper, we introduce a new privacy protection property called p-sensitive k-anonymity. The existing kanonymity property protects against identity disclosure, but it fails to protect against attribute disclosure. The new introduced privacy model avoids this shortcoming. Two necessary conditions to achieve p-sensitive kanonymity property are presented, and used in developing algorithms to create masked microdata with p-sensitive k-anonymity property using generalization and suppression.},
	booktitle = {22nd {International} {Conference} on {Data} {Engineering} {Workshops} ({ICDEW}'06)},
	author = {Truta, T. M. and Vinay, B.},
	year = {2006},
	note = {00309},
	keywords = {Biomedical imaging, Computer science, Databases, History, Medical diagnostic imaging, Medical services, Protection, Statistical analysis, data mining, privacy},
	pages = {94--94}
}

@article{sweeney_k-anonymity:_2002,
	title = {k-{ANONYMITY}: {A} {MODEL} {FOR} {PROTECTING} {PRIVACY}},
	volume = {10},
	issn = {0218-4885},
	shorttitle = {k-{ANONYMITY}},
	url = {http://www.worldscientific.com/doi/abs/10.1142/S0218488502001648},
	doi = {10.1142/S0218488502001648},
	abstract = {Consider a data holder, such as a hospital or a bank, that has a privately held  collection of person-specific, field structured data. Suppose the data holder  wants to share a version of the data with researchers. How can a data holder release  a version of its private data with scientific guarantees that the individuals who are  the subjects of the data cannot be re-identified while the data remain  practically useful? The solution provided in this paper includes a formal  protection model named k-anonymity and a set of accompanying policies  for deployment. A release provides k-anonymity protection if the  information for each person contained in the release cannot be distinguished from  at least k-1 individuals whose information also appears in the release.  This paper also examines re-identification attacks that can be realized on  releases that adhere to k-anonymity unless accompanying policies  are respected. The k-anonymity protection model is important because  it forms the basis on which the real-world systems known as Datafly,  μ-Argus and k-Similar provide guarantees of privacy  protection.},
	number = {05},
	journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
	author = {Sweeney, Latanya},
	month = oct,
	year = {2002},
	note = {04447},
	pages = {557--570}
}

@inproceedings{sengupta_moving_2017,
	address = {Richland, SC},
	series = {{AAMAS} '17},
	title = {Moving {Target} {Defense}: {A} {Symbiotic} {Framework} for {AI} \& {Security}},
	shorttitle = {Moving {Target} {Defense}},
	url = {http://dl.acm.org/citation.cfm?id=3091125.3091473},
	abstract = {Modern day technology has found its way into every aspect of our lives-- be it the server storing our social information, the hand-held smartphones, the home security systems or a remotely monitored pacemaker. Unfortunately, this also increases the opportunity for agents with malicious intent to violate the privacy, availability or integrity of these applications. In fact, with the advancement of Artificial Intelligence (AI) and faster hardware, the process of finding and exploiting vulnerabilities is no longer as time-consuming as before. Moving Target Defense (MTD) is emerging as an effective technique in addressing these security concerns. This technique, as used by the cyber security community, however, does not incorporate the dynamics of a multi-agent system between an attacker and defender, resulting in sub-optimal behavior. My study of such systems in a multi-agent context helps to enhance the security of MTD systems and proposes a list of challenges for the AI community. Furthermore, borrowing the example of MTD systems from the cyber security community, we can address some security concerns of the present day AI algorithms. In this abstract, I describe my research work that uses AI for enhancing security of a multi-agent MTD system and highlight research avenues in using MTD for enhancing security of present AI algorithms.},
	booktitle = {Proceedings of the 16th {Conference} on {Autonomous} {Agents} and {MultiAgent} {Systems}},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	author = {Sengupta, Sailik},
	year = {2017},
	note = {00000},
	keywords = {Bayesian games, adversarial deep learning, cybersecurity, moving target defense, multi-agent adversarial systems, stackelberg equilibrium},
	pages = {1861--1862}
}

@article{shar_web_2015,
	title = {Web {Application} {Vulnerability} {Prediction} {Using} {Hybrid} {Program} {Analysis} and {Machine} {Learning}},
	volume = {12},
	issn = {1545-5971},
	doi = {10.1109/TDSC.2014.2373377},
	abstract = {Due to limited time and resources, web software engineers need support in identifying vulnerable code. A practical approach to predicting vulnerable code would enable them to prioritize security auditing efforts. In this paper, we propose using a set of hybrid (static+dynamic) code attributes that characterize input validation and input sanitization code patterns and are expected to be significant indicators of web application vulnerabilities. Because static and dynamic program analyses complement each other, both techniques are used to extract the proposed attributes in an accurate and scalable way. Current vulnerability prediction techniques rely on the availability of data labeled with vulnerability information for training. For many real world applications, past vulnerability data is often not available or at least not complete. Hence, to address both situations where labeled past data is fully available or not, we apply both supervised and semi-supervised learning when building vulnerability predictors based on hybrid code attributes. Given that semi-supervised learning is entirely unexplored in this domain, we describe how to use this learning scheme effectively for vulnerability prediction. We performed empirical case studies on seven open source projects where we built and evaluated supervised and semi-supervised models. When cross validated with fully available labeled data, the supervised models achieve an average of 77 percent recall and 5 percent probability of false alarm for predicting SQL injection, cross site scripting, remote code execution and file inclusion vulnerabilities. With a low amount of labeled data, when compared to the supervised model, the semi-supervised model showed an average improvement of 24 percent higher recall and 3 percent lower probability of false alarm, thus suggesting semi-supervised learning may be a preferable solution for many real world applications where vulnerability data is missing.},
	number = {6},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Shar, L. K. and Briand, L. C. and Tan, H. B. K.},
	month = nov,
	year = {2015},
	note = {00011},
	keywords = {Computer security, Data models, HTML, Internet, Predictive models, SQL injection, Servers, Software protection, Vulnerability prediction, Web application vulnerability prediction, cross site scripting, dynamic program analyses, empirical study, false alarm probability, file inclusion vulnerabilities, hybrid program analysis, hybrid static+dynamic code attributes, input sanitization code patterns, input validation and sanitization, input validation code patterns, learning (artificial intelligence), machine learning, open source projects, program analysis, program diagnostics, remote code execution, security auditing, security measures, security of data, semisupervised learning, static program analyses, vulnerability prediction techniques, vulnerability predictors, vulnerable code identification, vulnerable code prediction},
	pages = {688--707}
}

@article{qin_when_2016,
	title = {When things matter: {A} survey on data-centric internet of things},
	volume = {64},
	issn = {1084-8045},
	shorttitle = {When things matter},
	url = {http://www.sciencedirect.com/science/article/pii/S1084804516000606},
	doi = {10.1016/j.jnca.2015.12.016},
	abstract = {With the recent advances in radio-frequency identification (RFID), low-cost wireless sensor devices, and Web technologies, the Internet of Things (IoT) approach has gained momentum in connecting everyday objects to the Internet and facilitating machine-to-human and machine-to-machine communication with the physical world. IoT offers the capability to connect and integrate both digital and physical entities, enabling a whole new class of applications and services, but several significant challenges need to be addressed before these applications and services can be fully realized. A fundamental challenge centers around managing IoT data, typically produced in dynamic and volatile environments, which is not only extremely large in scale and volume, but also noisy and continuous. This paper reviews the main techniques and state-of-the-art research efforts in IoT from data-centric perspectives, including data stream processing, data storage models, complex event processing, and searching in IoT. Open research issues for IoT data management are also discussed.},
	journal = {Journal of Network and Computer Applications},
	author = {Qin, Yongrui and Sheng, Quan Z. and Falkner, Nickolas J. G. and Dustdar, Schahram and Wang, Hua and Vasilakos, Athanasios V.},
	month = apr,
	year = {2016},
	note = {00049},
	keywords = {Data management, Internet of Things, RFID systems, wireless sensor networks},
	pages = {137--153}
}

@article{chen_efficient_2017,
	title = {An efficient power saving polling scheme in the internet of energy},
	volume = {89},
	issn = {1084-8045},
	doi = {10.1016/j.jnca.2017.01.002},
	abstract = {A global WLAN standard which utilizes country specific carrier frequencies below 1 GHz has been drafted by the IEEE 802.11ah (.ah) Task Group. This draft is highly useful in rural areas due to an improved wireless propagation characteristic and a large coverage. However, the .ah draft still adopts the CSMA/CA (Carrier Sense Multiple Access with Collision Avoidance) as its media access and collision resolution protocol, which is energy consuming and not suitable for networks where STAs (STAtions) are generally battery supplied such as meters in a SG (Smart-Grid) network, one of the typical usecases of Internet of Energy in the .ah draft. In addition, since .ah could support up to at most 6000 STAs to be scheduled within one BSS, the introduced overheads and corresponding processing delay are non-trivial and need more considerations. In this paper, a power saving scheduling scheme is proposed which could greatly reduce the introduced overhead while successfully scheduling the uplink/downlink traffics of meters in the Internet of Energy. Considering the service model and special characters of .ah networks, our model could also extend the STAs battery life with best efforts thus making our protocol specifically suitable for SG networks where battery changing for a device is usually very difficult. Further, to make our model work well, a dynamic AID allocation scheme is also presented by which the AIDs of STAs are assigned according to the average service duration thus making those scheduled STAs with similar service characteristics occupy successive AIDs. In this way, the compressed Bitmap scheme proposed in our Power Saving polling scheme for SG in the Internet of Energy (PSSG), could greatly reduce the communication overheads. Numerical results show that our scheme is better than the Power Saving Mechanism (PSM) and Power Save Multi-Poll (PSMP) protocols in terms of overheads, throughput, average awaken time and energy consumptions.},
	language = {English},
	journal = {Journal of Network and Computer Applications},
	author = {Chen, Chen and Zhao, Honghui and Qiu, Tie and Hu, Mingcheng and Han, Hui and Ren, Zhiyuan},
	month = jul,
	year = {2017},
	note = {00001 
WOS:000403517500006},
	keywords = {Big Data, Internet of energy, Next generation WLAN, Polling scheme, Power saving, Security, Smart grid, cities, cloud, computing adoption framework, network, smart grid communications, storage, systems, things},
	pages = {48--61}
}

@article{tamandani_two-step_2017,
	title = {Two-step fuzzy logic system to achieve energy efficiency and prolonging the lifetime of {WSNs}},
	volume = {23},
	issn = {1022-0038},
	doi = {10.1007/s11276-016-1266-3},
	abstract = {Mainly because of resource restrictions in wireless sensor networks (WSNs), extending the lifetime of the network, has gained significant attention in the last several years. As energy becomes a quite challenging issue in these networks, clustering protocols are employed to deal with this problem. One of the main research areas in cluster-based routing protocols is fair distribution and balancing the overall energy consumption in the WSN, by selecting the most suitable cluster heads (CHs). In order to reduce the energy consumption and enhancing the CHs selection process a new routing protocol based on fuzzy logic has been proposed. There exist several algorithms based of fuzzy logic to select the most proper CHs for the network. But these algorithms do not consider all the important parameters and information of the sensor nodes in order to guarantee the optimal selection of the CHs. In The proposed algorithm, a two-step fuzzy logic system is used to select the appropriate CHs. The selection of CHs is based on six descriptors; residual energy, density, distance to base station, vulnerability index, centrality and distance between CHs. The result of the simulation indicates that, the proposed algorithm performs better comparing with some other similar approaches in case of fair distribution and balancing of the overall energy consumption.},
	language = {English},
	number = {6},
	journal = {Wireless Networks},
	author = {Tamandani, Yahya Kord and Bokhari, Mohammad Ubaidullah and Shallal, Qahtan Makki},
	month = aug,
	year = {2017},
	note = {00000 
WOS:000405314900018},
	keywords = {Cluster heads selection, Clustering protocols, Fuzzy logic, Internet, WSNs, algorithm, challenges, energy, protocol, things, topology control, wireless sensor networks},
	pages = {1889--1899}
}

@article{arkian_mist:_2017,
	title = {{MIST}: {Fog}-based data analytics scheme with cost-efficient resource provisioning for {IoT} crowdsensing applications},
	volume = {82},
	issn = {1084-8045},
	shorttitle = {{MIST}},
	doi = {10.1016/j.jnca.2017.01.012},
	abstract = {Development of Internet of things (IoT) has revitalized the feature scale of wearables and smart home/city applications. The landscape of these applications encountering big data needs to be replotted on cloud instead of solely relying on limited storage and computational resources of small devices. However, with the rapid increase in the number of Internet-connected devices, the increased demand of real-time, low-latency services is proving to be challenging for the traditional cloud computing framework. Although, fog computing, by providing elastic resources and services to end users at the edge of network, emerges as a promising solution, but the upsurge of novel social applications such as crowd sensing has fostered the need for scalable cost-efficient platforms that can enable distributed data analytics, while optimizing the allocation of resources and minimizing the response time. Following the existing trends, we are motivated to propose a fog computing based scheme, called MIST (i.e. a cloud near the earth's surface with lesser density than fog), to support crowd sensing applications in the context of IoT. For cost-efficient provisioning limited resources, we also jointly investigate data consumer association, task distribution, and virtual machine placement towards MIST. We first formulate the problem into a mixed-integer non-linear program (MINLP) and then linearise it into a mixed integer linear program (MILP). A comprehensive evaluation of MIST is performed by consideration of real world parameters of the Tehran province, the capital of Iran. Results show that as the number of applications demanding real-time service increases, the MIST fog-based scheme outperforms traditional cloud computing.},
	language = {English},
	journal = {Journal of Network and Computer Applications},
	author = {Arkian, Hamid Reza and Diyanat, Abolfazl and Pourkhalili, Atefe},
	month = mar,
	year = {2017},
	note = {00002 
WOS:000397074800012},
	keywords = {Crowdsensing, Internet, Internet of Things (IoT), Optimization, Resource   allocation, allocation, challenges, fog computing, integration, state, things},
	pages = {152--165}
}

@article{nguyen_optimization_2017,
	title = {Optimization of non-functional properties in {Internet} of {Things} applications},
	volume = {89},
	issn = {1084-8045},
	doi = {10.1016/j.jnca.2017.03.019},
	abstract = {A major challenge in designing Internet of Things (IoT) systems is to meet various non-functional requirements such as lifetime, reliability, throughput, delay, and so forth. Furthermore, IoT systems tend to have competing requirements, which exacerbate these design challenges. We analyze this problem in detail and propose a model-driven approach to optimize an IoT application regarding to its non-functional requirements. Our approach defines optimizing as finding the best set of adjustable application parameters, which satisfies a given objective function. The relevant parameters are extracted during a simulation process. We apply a source code transformation that updates the source code with the generated adjustable parameter values and executes the compiler to create a new binary image of the application. Our experiment results demonstrate that nonfunctional requirements such as power consumption and reliability can'be improved substantially during the optimization process.},
	language = {English},
	journal = {Journal of Network and Computer Applications},
	author = {Nguyen, Xuan Thang and Tran, Huu Tam and Baraki, Harun and Geihs, Kurt},
	month = jul,
	year = {2017},
	note = {00001 
WOS:000403517500012},
	keywords = {Algorithms, Internet of Things, Non-functional   requirements, Optimization, Security, Sensor networks, Simulation, Wireless sensor network, computing adoption framework, energy, multiobjective   optimization},
	pages = {120--129}
}

@article{ghaleb_mobility_2016,
	title = {Mobility management for {IoT}: a survey},
	issn = {1687-1499},
	shorttitle = {Mobility management for {IoT}},
	doi = {10.1186/s13638-016-0659-4},
	abstract = {Internet of Thing (IoT) or also referred to as IP-enabled wireless sensor network (IP-WSN) has become a rich area of research. This is due to the rapid growth in a wide spectrum of critical application domains. However, the properties within these systems such as memory size, processing capacity, and power supply have led to imposing constraints on IP-WSN applications and its deployment in the real world. Consequently, IP-WSN is constantly faced with issues as the complexity further rises due to IP mobility. IP mobility management is utilized as a mechanism to resolve these issues. The management protocols introduced to support mobility has evolved from host-based to network-based mobility management protocols. The presence of both types of solutions is dominant but depended on the nature of systems being deployed. The mobile node (MN) is involved with the mobility-related signaling in host-based protocols, while network-based protocols shield the host by transferring the mobility-related signaling to the network entities. The features of the IoT are inclined towards the network-based solutions. The wide spectrum of strategies derived to achieve enhanced performance evidently displays superiority in performance and simultaneous issues such as long handover latency, intense signaling, and packet loss which affects the QoS for the real-time applications. This paper extensively reviews and discusses the algorithms developed to address the challenges and the techniques of integrating IP over WSNs, the attributes of mobility management within the IPv4 and IPv6, respectively, and special focus is given on a comprehensive review encompassing mechanisms, advantages, and disadvantages on related work within the IPv6 mobility management. The paper is concluded with the proposition of several pertinent open issues which are of high research value.},
	language = {English},
	journal = {Eurasip Journal on Wireless Communications and Networking},
	author = {Ghaleb, Safwan M. and Subramaniam, Shamala and Zukarnain, Zuriati Ahmed and Muhammed, Abdullah},
	month = jul,
	year = {2016},
	note = {00005 
WOS:000379516100001},
	keywords = {6lowpan, IP-enabled wireless sensor network, IPv6   protocol, Internet, Mobility management, Mobility wireless sensor network, Ubiquitous computing, Wireless sensor network, challenges, energy-efficient, open   issues, protocol, scheme, things, trust management, wireless sensor networks},
	pages = {165}
}

@article{gonzalez_novel_2016,
	title = {Novel {Networked} {Remote} {Laboratory} {Architecture} for {Open} {Connectivity} {Based} on {PLC}-{OPC}-{LabVIEW}-{EJS} {Integration}. {Application} in {Remote} {Fuzzy} {Control} and {Sensors} {Data} {Acquisition}},
	volume = {16},
	issn = {1424-8220},
	doi = {10.3390/s16111822},
	abstract = {In this paper the design and implementation of a network for integrating Programmable Logic Controllers (PLC), the Object-Linking and Embedding for Process Control protocol (OPC) and the open-source Easy Java Simulations (EJS) package is presented. A LabVIEW interface and the Java-Internet-Lab VIEW (JIL) server complete the scheme for data exchange. This configuration allows the user to remotely interact with the PLC. Such integration can be considered a novelty in scientific literature for remote control and sensor data acquisition of industrial plants. An experimental application devoted to remote laboratories is developed to demonstrate the feasibility and benefits of the proposed approach. The experiment to be conducted is the parameterization and supervision of a fuzzy controller of a DC servomotor. The graphical user interface has been developed with EJS and the fuzzy control is carried out by our own PLC. In fact, the distinctive features of the proposed novel network application are the integration of the OPC protocol to share information with the PLC and the application under control. The user can perform the tuning of the controller parameters online and observe in real time the effect on the servomotor behavior. The target group is engineering remote users, specifically in control- and automation-related tasks. The proposed architecture system is described and experimental results are presented.},
	language = {English},
	number = {11},
	journal = {Sensors},
	author = {Gonzalez, Isaias and Calderon, Antonio Jose and Mejias, Andres and Andujar, Jose Manuel},
	month = nov,
	year = {2016},
	note = {00003 
WOS:000389641700055},
	keywords = {Hardware, automatic-control systems, challenges, data acquisition, easy java, easy java simulations, engineering-education, fuzzy control, low-cost, networked remote laboratory, of-the-art, opc, programmable logic   controller, programmable logic controllers, simulations, sw spain},
	pages = {1822}
}

@incollection{qin_approximate_2016,
	address = {Cham},
	title = {Approximate {Semantic} {Matching} over {Linked} {Data} {Streams}},
	volume = {9828},
	isbn = {978-3-319-44406-2 978-3-319-44405-5},
	abstract = {In the Internet of Things (IoT), data can be generated by all kinds of smart things. In such context, enabling machines to process and understand such data is critical. Semantic Web technologies, such as Linked Data, provide an effective and machine-understandable way to represent IoT data for further processing. It is a challenging issue to match Linked Data streams semantically based on text similarity as text similarity computation is time consuming. In this paper, we present a hashing-based approximate approach to efficiently match Linked Data streams with users' needs. We use the Resource Description Framework (RDF) to represent IoT data and adopt triple patterns as user queries to describe users' data needs. We then apply locality-sensitive hashing techniques to transform semantic data into numerical values to support efficient matching between data and user queries. We design a modified k nearest neighbors (kNN) algorithm to speedup the matching process. The experimentalresults show that our approach is up to five times faster than the traditional methods and can achieve high precisions and recalls.},
	language = {English},
	booktitle = {Database and {Expert} {Systems} {Applications}, {Dexa} 2016, {Pt} {Ii}},
	publisher = {Springer Int Publishing Ag},
	author = {Qin, Yongrui and Yao, Lina and Sheng, Quan Z.},
	editor = {Hartmann, S. and Ma, H.},
	year = {2016},
	note = {00000 
WOS:000389020200005},
	keywords = {Internet, Internet of Things, Linked Data, Semantic matching, kNN classification, things},
	pages = {37--51}
}

@article{said_performance_2016,
	title = {Performance {Evaluation} of a {Dual} {Coverage} {System} for {Internet} of {Things} {Environments}},
	issn = {1574-017X},
	doi = {10.1155/2016/3464392},
	abstract = {A dual coverage system for Internet of Things (IoT) environments is introduced. This system is used to connect IoT nodes regardless of their locations. The proposed system has three different architectures, which are based on satellites and High Altitude Platforms (HAPs). In case of Internet coverage problems, the Internet coverage will be replaced with the Satellite/HAP network coverage under specific restrictions such as loss and delay. According to IoT requirements, the proposed architectures should include multiple levels of satellites or HAPs, or a combination of both, to cover the global Internet things. It was shown that the Satellite/HAP/HAP/Things architecture provides the largest coverage area. A network simulation package, NS2, was used to test the performance of the proposed multilevel architectures. The results indicated that the HAP/HAP/Things architecture has the best end-to-end delay, packet loss, throughput, energy consumption, and handover.},
	language = {English},
	journal = {Mobile Information Systems},
	author = {Said, Omar and Tolba, Amr},
	year = {2016},
	note = {00000 
WOS:000391617000001},
	keywords = {platforms, wireless sensor networks},
	pages = {3464392}
}

@article{ngu_iot_2017,
	title = {{IoT} {Middleware}: {A} {Survey} on {Issues} and {Enabling} {Technologies}},
	volume = {4},
	issn = {2327-4662},
	shorttitle = {{IoT} {Middleware}},
	doi = {10.1109/JIOT.2016.2615180},
	abstract = {The Internet of Things (IoT) provides the ability for humans and computers to learn and interact from billions of things that include sensors, actuators, services, and other Internet-connected objects. The realization of IoT systems will enable seamless integration of the cyber world with our physical world and will fundamentally change and empower human interaction with the world. A key technology in the realization of IoT systems is middleware, which is usually described as a software system designed to be the intermediary between IoT devices and applications. In this paper, we first motivate the need for an IoT middleware via an IoT application designed for real-time prediction of blood alcohol content using smartwatch sensor data. This is then followed by a survey on the capabilities of the existing IoT middleware. We further conduct a thorough analysis of the challenges and the enabling technologies in developing an IoT middleware that embraces the heterogeneity of IoT devices and also supports the essential ingredients of composition, adaptability, and security aspects of an IoT system.},
	language = {English},
	number = {1},
	journal = {Ieee Internet of Things Journal},
	author = {Ngu, Anne H. and Gutierrez, Mario and Metsis, Vangelis and Nepal, Surya and Sheng, Quan Z.},
	month = feb,
	year = {2017},
	note = {00011 
WOS:000395769400001},
	keywords = {Internet, Internet of Things (IoT), IoT middleware, IoT service discovery, Security, challenges, privacy, security and privacy, things, threats, web},
	pages = {1--20}
}

@article{ibarra-esquer_tracking_2017,
	title = {Tracking the {Evolution} of the {Internet} of {Things} {Concept} {Across} {Different} {Application} {Domains}},
	volume = {17},
	issn = {1424-8220},
	doi = {10.3390/s17061379},
	abstract = {Both the idea and technology for connecting sensors and actuators to a network to remotely monitor and control physical systems have been known for many years and developed accordingly. However, a littlemore than a decade ago the concept of the Internet of Things (IoT) was coined and used to integrate such approaches into a common framework. Technology has been constantly evolving and so has the concept of the Internet of Things, incorporating new terminology appropriate to technological advances and different application domains. This paper presents the changes that the IoT has undertaken since its conception and research on how technological advances have shaped it and fostered the arising of derived names suitable to specific domains. A two-step literature review through major publishers and indexing databases was conducted; first by searching for proposals on the Internet of Things concept and analyzing them to find similarities, differences, and technological features that allow us to create a timeline showing its development; in the second step the most mentioned names given to the IoT for specific domains, as well as closely related concepts were identified and briefly analyzed. The study confirms the claimthat a consensus on the IoT definition has not yet been reached, as enabling technology keeps evolving and new application domains are being proposed. However, recent changes have been relatively moderated, and its variations on application domains are clearly differentiated, with data and data technologies playing an important role in the IoT landscape.},
	language = {English},
	number = {6},
	journal = {Sensors},
	author = {Ibarra-Esquer, Jorge E. and Gonzalez-Navarro, Felix E. and Flores-Rios, Brenda L. and Burtseva, Larysa and Astorga-Vargas, Maria A.},
	month = jun,
	year = {2017},
	note = {00000 
WOS:000404553900195},
	keywords = {Internet of Things, IoT, application domains, challenges, china, definition, directions, opportunities, things capabilities, vision},
	pages = {1379}
}

@article{sezer_extended_2016,
	title = {An {Extended} {IoT} {Framework} with {Semantics}, {Big} {Data}, and {Analytics}},
	abstract = {Many experts claim that data will be the most valuable commodity in the 21st century. At the same time, two of the most influential components of this era, Big Data and IoT are moving very fast, on a collision course with the methodologies that are associated with conventional data processing and database systems. As a result, new approaches like NoSQL databases, distributed architectures, etc. started appearing on the stage. Meanwhile, another technology, ontology and semantic data processing can be a very convenient catalyzer that might assist in smoothly providing this transformation process. In this paper, we propose a combined framework that brings Big Data, IoT, and semantic web together to build an augmented framework for this new era. We not only list the components of such a system and define the necessary bindings that needs to be integrated together, but also provide a realistic use case that demonstrates how the model can implement the desired functionality and achieve the goals of such a model.},
	language = {English},
	journal = {2016 Ieee International Conference on Big Data (big Data)},
	author = {Sezer, Omer Berat and Dogdu, Erdogan and Ozbayoglu, Murat and Onal, Aras},
	editor = {Joshi, J. and Karypis, G. and Liu, L. and Hu, X. and Ak, R. and Xia, Y. and Xu, W. and Sato, A. H. and Rachuri, S. and Ungar, L. and Yu, P. S. and Govindaraju, R. and Suzumura, T.},
	year = {2016},
	note = {00001 
WOS:000399115001110},
	keywords = {Internet, Internet of Things, Semantics, activity recognition, big data analytics, framework, open   system, things},
	pages = {1849--1856}
}

@incollection{igor_proposal_2016,
	address = {New York},
	title = {Proposal of communication standardization of industrial networks in {Industry} 4.0},
	isbn = {978-1-5090-1216-9},
	abstract = {This paper provides an overview of the current state of the use of Industry 4.0 standards in real practice and deals with application problems of Smart Factory technology in various industries. Essential conditions of a successful standards implementation, especially according to industrial networks communication are presented. In the second part of this paper consumption method for maintaining compatibility and safety communications in the control of industrial process is established. Part of the article is a communication model proposal suitable for the implementation.},
	language = {English},
	booktitle = {{INES} 2016 20th {Jubilee} {IEEE} {International} {Conference} on {Intelligent}   {Engineering} {Systems}},
	publisher = {Ieee},
	author = {Igor, Halenar and Bohuslava, Juhasova and Martin, Juhas},
	editor = {Szakal, A.},
	year = {2016},
	note = {00002 
WOS:000389448700021},
	keywords = {Industry 4.0, Security, algorithm, cyber-physical system, industrial network, intelligent products, systems},
	pages = {119--123}
}

@article{kiryakova_can_2017,
	title = {Can we make {Schools} and {Universities} smarter with the {Internet} of {Things}?},
	volume = {6},
	issn = {2217-8309},
	doi = {10.18421/TEM61-11},
	abstract = {Schools and universities stand up to the challenge the Internet of Things, which has the potential to significantly change teaching and learning. The learning and administrative processes and the relationships between all participants in education may benefit from the Internet of Things since the linked physical devices ensure connectivity of people and ensure their activity. The implementation of the Internet of Things in education, unlike other spheres, has a very important and difficult task. The Internet of Thing has to guarantee the creation of an environment that supports the acquisition of knowledge in a new, natural and effective way, consistent with the new realities and learners' expectations. The questions of how and in what direction the Internet of Things will lead to changes in educational activities and processes have many answers and need discussions and debates. The objective of the current work is to answer these questions by presenting the concept the Internet of Things and consider its possible applications in education.},
	language = {English},
	number = {1},
	journal = {Tem Journal-Technology Education Management Informatics},
	author = {Kiryakova, Gabriela and Yordanova, Lina and Angelova, Nadezhda},
	month = feb,
	year = {2017},
	note = {00001 
WOS:000397267800011},
	keywords = {Education, Internet of Things, smart devices},
	pages = {80--84}
}

@article{sheng_guest_2016,
	title = {Guest editorial: web of things},
	volume = {18},
	issn = {1387-3326},
	shorttitle = {Guest editorial},
	doi = {10.1007/s10796-016-9677-3},
	language = {English},
	number = {4},
	journal = {Information Systems Frontiers},
	author = {Sheng, Quan Z. and Li, Xue and Ngu, Anne H. H. and Qin, Yongrui and Xie, Dong},
	month = aug,
	year = {2016},
	note = {00000 
WOS:000380713800001},
	keywords = {Internet},
	pages = {639--643}
}

@article{fernandez-gago_modelling_2017,
	title = {Modelling trust dynamics in the {Internet} of {Things}},
	volume = {396},
	issn = {0020-0255},
	doi = {10.1016/j.ins.2017.02.039},
	abstract = {The Internet of Things (loT) is a paradigm based on the interconnection of everyday objects. It is expected that the 'things' involved in the loT paradigm will have to interact with each other, often in uncertain conditions. It is therefore of paramount importance for the success of IoT that there are mechanisms in place that help overcome the lack of certainty. Trust can help achieve this goal. In this paper, we introduce a framework that assists developers in including trust in loT scenarios. This framework takes into account trust, privacy and identity requirements as well as other functional requirements derived from IoT scenarios to provide the different services that allow the inclusion of trust in the' loT. (C) 2017 Elsevier Inc. All rights reserved.},
	language = {English},
	journal = {Information Sciences},
	author = {Fernandez-Gago, Carmen and Moyano, Francisco and Lopez, Javier},
	month = aug,
	year = {2017},
	note = {00000 
WOS:000397379000006},
	keywords = {Dynamic framework, Internet of Things, management, reputation, trust},
	pages = {72--82}
}

@article{zhou_spatial_2017,
	title = {Spatial {Indexing} for {Data} {Searching} in {Mobile} {Sensing} {Environments}},
	volume = {17},
	issn = {1424-8220},
	doi = {10.3390/s17061427},
	abstract = {Data searching and retrieval is one of the fundamental functionalities in many Web of Things applications, which need to collect, process and analyze huge amounts of sensor stream data. The problem in fact has been well studied for data generated by sensors that are installed at fixed locations; however, challenges emerge along with the popularity of opportunistic sensing applications in which mobile sensors keep reporting observation and measurement data at variable intervals and changing geographical locations. To address these challenges, we develop the Geohash-Grid Tree, a spatial indexing technique specially designed for searching data integrated from heterogeneous sources in a mobile sensing environment. Results of the experiments on a real-world dataset collected from the SmartSantander smart city testbed show that the index structure allows efficient search based on spatial distance, range and time windows in a large time series database.},
	language = {English},
	number = {6},
	journal = {Sensors},
	author = {Zhou, Yuchao and De, Suparna and Wang, Wei and Moessner, Klaus and Palaniswami, Marimuthu S.},
	month = jun,
	year = {2017},
	note = {00000 
WOS:000404553900243},
	keywords = {Internet, Web of Things (WoT), discovery, mobile sensing, mobile sensor data search, opportunistic sensing, sensor web, spatial indexing, things},
	pages = {1427}
}

@article{siddiqa_analysis_2017,
	title = {On the analysis of big data indexing execution strategies},
	volume = {32},
	issn = {1064-1246},
	doi = {10.3233/JIFS-169269},
	abstract = {Efficient response to search queries is very crucial for data analysts to obtain timely results from big data spanned over heterogeneous machines. Currently, a number of big-data processing frameworks are available in which search operations are performed in distributed and parallel manner. However, implementation of indexing mechanism results in noticeable reduction of overall query processing time. There is an urge to assess the feasibility and impact of indexing towards query execution performance. This paper investigates the performance of state-of-the-art clustered indexing approaches over Hadoop framework which is de facto standard for big data processing. Moreover, this study leverages a comparative analysis of nonclustered indexing overhead in terms of time and space taken by indexing process for varying volume data sets with increasing Index Hit Ratio. Furthermore, the experiments evaluate performance of search operations in terms of data access and retrieval time for queries that use indexes. We then validated the obtained results using Petri net mathematical modeling. We used multiple data sets in our experiments to manifest the impact of growing volume of data on indexing and data search and retrieval performance. The results and highlighted challenges favorably lead researchers towards improved implication of indexing mechanism in perspective of data retrieval from big data. Additionally, this study advocates selection of a non-clustered indexing solution so that optimized search performance over big data is obtained.},
	language = {English},
	number = {5},
	journal = {Journal of Intelligent \& Fuzzy Systems},
	author = {Siddiqa, Aisha and Karim, Ahmad and Saba, Tanzila and Chang, Victor},
	year = {2017},
	note = {00000 
WOS:000400023600004},
	keywords = {Big Data, big data processing, data retrieval, indexing},
	pages = {3259--3271}
}

@article{chiang_fog_2016,
	title = {Fog and {IoT}: {An} {Overview} of {Research} {Opportunities}},
	volume = {3},
	issn = {2327-4662},
	shorttitle = {Fog and {IoT}},
	doi = {10.1109/JIOT.2016.2584538},
	abstract = {Fog is an emergent architecture for computing, storage, control, and networking that distributes these services closer to end users along the cloud-to-things continuum. It covers both mobile and wireline scenarios, traverses across hardware and software, resides on network edge but also over access networks and among end users, and includes both data plane and control plane. As an architecture, it supports a growing variety of applications, including those in the Internet of Things (IoT), fifth-generation (5G) wireless systems, and embedded artificial intelligence (AI). This survey paper summarizes the opportunities and challenges of fog, focusing primarily in the networking context of IoT.},
	number = {6},
	journal = {IEEE Internet of Things Journal},
	author = {Chiang, M. and Zhang, T.},
	month = dec,
	year = {2016},
	note = {00057},
	keywords = {5G mobile communication, 5G wireless systems, Computer architecture, Edge computing, Hardware, Internet of Things, Internet of Things (IoT), IoT, Security, cloud computing, edge networking, edge storage, embedded artificial intelligence, fifth-generation wireless systems, fog, fog computing, fog control, fog networking, fog storage},
	pages = {854--864}
}

@inproceedings{niu_achieving_2014,
	title = {Achieving k-anonymity in privacy-aware location-based services},
	doi = {10.1109/INFOCOM.2014.6848002},
	abstract = {Location-Based Service (LBS) has become a vital part of our daily life. While enjoying the convenience provided by LBS, users may lose privacy since the untrusted LBS server has all the information about users in LBS and it may track them in various ways or release their personal data to third parties. To address the privacy issue, we propose a Dummy-Location Selection (DLS) algorithm to achieve k-anonymity for users in LBS. Different from existing approaches, the DLS algorithm carefully selects dummy locations considering that side information may be exploited by adversaries. We first choose these dummy locations based on the entropy metric, and then propose an enhanced-DLS algorithm, to make sure that the selected dummy locations are spread as far as possible. Evaluation results show that the proposed DLS algorithm can significantly improve the privacy level in terms of entropy. The enhanced-DLS algorithm can enlarge the cloaking region while keeping similar privacy level as the DLS algorithm.},
	booktitle = {{IEEE} {INFOCOM} 2014 - {IEEE} {Conference} on {Computer} {Communications}},
	author = {Niu, B. and Li, Q. and Zhu, X. and Cao, G. and Li, H.},
	month = apr,
	year = {2014},
	note = {00110},
	keywords = {Algorithm design and analysis, Computers, Conferences, DLS algorithm, Entropy, Measurement, Servers, cloaking region, data privacy, dummy-location selection algorithm, entropy metric, k-anonymity, mobile computing, privacy, privacy-aware location-based services, untrusted LBS server, user information},
	pages = {754--762}
}

@article{caballero-gil_providing_2016,
	series = {Vehicular {Networking} for {Mobile} {Crowd} {Sensing}},
	title = {Providing k-anonymity and revocation in ubiquitous {VANETs}},
	volume = {36},
	issn = {1570-8705},
	url = {http://www.sciencedirect.com/science/article/pii/S1570870515001213},
	doi = {10.1016/j.adhoc.2015.05.016},
	abstract = {Vehicular ad hoc networks (VANETs) is considered a milestone in improving the safety and efficiency in transportation. Nevertheless, when information from the vehicular communications is combined with data from the cloud, it also introduces some privacy risks by making it easier to track the physical location of vehicles. For this reason, to guarantee the proper performance of a VANET it is essential to protect the service against malicious users aiming at disrupting the proper operation of the network. Current researches usually define a traditional identity-based authentication for nodes, which are loaded with individual credentials. However, the use of these credentials in VANETs without any security mechanism enables vehicle tracking and therefore, violate users’ privacy, a risk that may be overcome by means of appropriate anonymity schemes. This comes at the cost, however, of on the one hand preventing VANET centralized authorities from identifying malicious users and revoking them from the network, or on the other hand to avoid complete anonymity of nodes in front of the CA thus to allow their revocation. In this paper, a novel revocation scheme that is able to track and revoke specific malicious users only after a number of complaints have been received while otherwise guaranteeing node’s k-anonymity is described. The proper performance of these mechanisms has been widely evaluated with NS-2 simulator and an analytical model validated with scripts. The results show that presented work is a promising approach in order to increase privacy protection while allowing revocation with little extra costs.},
	journal = {Ad Hoc Networks},
	author = {Caballero-Gil, Cándido and Molina-Gil, Jezabel and Hernández-Serrano, Juan and León, Olga and Soriano-Ibañez, Miguel},
	month = jan,
	year = {2016},
	note = {00009},
	keywords = {-anonymity, Revocation, Security, Ubiquitous, VANET, privacy},
	pages = {482--494}
}

@article{zhou_brief_2008,
	title = {A {Brief} {Survey} on {Anonymization} {Techniques} for {Privacy} {Preserving} {Publishing} of {Social} {Network} {Data}},
	volume = {10},
	issn = {1931-0145},
	url = {http://doi.acm.org/10.1145/1540276.1540279},
	doi = {10.1145/1540276.1540279},
	abstract = {Nowadays, partly driven by many Web 2.0 applications, more and more social network data has been made publicly available and analyzed in one way or another. Privacy preserving publishing of social network data becomes a more and more important concern. In this paper, we present a brief yet systematic review of the existing anonymization techniques for privacy preserving publishing of social network data. We identify the new challenges in privacy preserving publishing of social network data comparing to the extensively studied relational case, and examine the possible problem formulation in three important dimensions: privacy, background knowledge, and data utility. We survey the existing anonymization methods for privacy preservation in two categories: clustering-based approaches and graph modification approaches.},
	number = {2},
	journal = {SIGKDD Explor. Newsl.},
	author = {Zhou, Bin and Pei, Jian and Luk, WoShun},
	month = dec,
	year = {2008},
	note = {00278},
	pages = {12--22}
}

@misc{noauthor_privacy_nodate,
	title = {A {Privacy} {Mechanism} for {Access} {Controlled} {Graph} {Data}},
	url = {https://www.researchgate.net/publication/317556743_A_Privacy_Mechanism_for_Access_Controlled_Graph_Data},
	abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
	urldate = {2017-07-19TZ},
	journal = {ResearchGate},
	note = {00000}
}
@misc{noauthor_cocoa:_nodate,
	title = {{COCOA}: {A} {Synthetic} {Data} {Generator} for {Testing} {Anonymization} {Techniques}},
	shorttitle = {{COCOA}},
	url = {https://www.researchgate.net/publication/307507464_COCOA_A_Synthetic_Data_Generator_for_Testing_Anonymization_Techniques},
	abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
	urldate = {2017-07-19TZ},
	journal = {ResearchGate},
	note = {00002}
}

@article{ghinita_framework_2009,
	title = {A {Framework} for {Efficient} {Data} {Anonymization} {Under} {Privacy} and {Accuracy} {Constraints}},
	volume = {34},
	issn = {0362-5915},
	url = {http://doi.acm.org/10.1145/1538909.1538911},
	doi = {10.1145/1538909.1538911},
	abstract = {Recent research studied the problem of publishing microdata without revealing sensitive information, leading to the privacy-preserving paradigms of k-anonymity and l-diversity. k-anonymity protects against the identification of an individual's record. l-diversity, in addition, safeguards against the association of an individual with specific sensitive information. However, existing approaches suffer from at least one of the following drawbacks: (i) l-diversification is solved by techniques developed for the simpler k-anonymization problem, causing unnecessary information loss. (ii) The anonymization process is inefficient in terms of computational and I/O cost. (iii) Previous research focused exclusively on the privacy-constrained problem and ignored the equally important accuracy-constrained (or dual) anonymization problem. In this article, we propose a framework for efficient anonymization of microdata that addresses these deficiencies. First, we focus on one-dimensional (i.e., single-attribute) quasi-identifiers, and study the properties of optimal solutions under the k-anonymity and l-diversity models for the privacy-constrained (i.e., direct) and the accuracy-constrained (i.e., dual) anonymization problems. Guided by these properties, we develop efficient heuristics to solve the one-dimensional problems in linear time. Finally, we generalize our solutions to multidimensional quasi-identifiers using space-mapping techniques. Extensive experimental evaluation shows that our techniques clearly outperform the existing approaches in terms of execution time and information loss.},
	number = {2},
	journal = {ACM Trans. Database Syst.},
	author = {Ghinita, Gabriel and Karras, Panagiotis and Kalnis, Panos and Mamoulis, Nikos},
	month = jul,
	year = {2009},
	note = {00074},
	keywords = {anonymity, privacy},
	pages = {9:1--9:47}
}

@inproceedings{loh_ontology-enhanced_2010,
	title = {Ontology-{Enhanced} {Interactive} {Anonymization} in {Domain}-{Driven} {Data} {Mining} {Outsourcing}},
	doi = {10.1109/ISDPE.2010.7},
	abstract = {This paper focuses on a domain-driven data mining outsourcing scenario whereby a data owner publishes data to an application service provider who returns mining results. To ensure data privacy against an un-trusted party, anonymization, a widely used technique capable of preserving true attribute values and supporting various data mining algorithms is required. Several issues emerge when anonymization is applied in a real world outsourcing scenario. The majority of methods have focused on the traditional data mining paradigm, therefore they do not implement domain knowledge nor optimize data for domain-driven usage. Furthermore, existing techniques are mostly non-interactive in nature, providing little control to users while assuming their natural capability of producing Domain Generalization Hierarchies (DGH). Moreover, previous utility metrics have not considered attribute correlations during generalization. To successfully obtain optimal data privacy and actionable patterns in a real world setting, these concerns need to be addressed. This paper proposes an anonymization framework for aiding users in a domain-driven data mining outsourcing scenario. The framework involves several components designed to anonymize data while preserving meaningful or actionable patterns that can be discovered after mining. In contrast with existing works for traditional data-mining, this framework integrates domain ontology knowledge during DGH creation to retain value meanings after anonymization. In addition, users can implement constraints based on their mining tasks thereby controlling how data generalization is performed. Finally, attribute correlations are calculated to ensure preservation of important features. Preliminary experiments show that an ontology-based DGH manages to preserve semantic meaning after attribute generalization. Also, using Chi-Square as a correlation measure can possibly improve attribute selection before generalization.},
	booktitle = {2010 {Second} {International} {Symposium} on {Data}, {Privacy}, and {E}-{Commerce}},
	author = {Loh, B. C. S. and Then, P. H. H.},
	month = sep,
	year = {2010},
	note = {00005},
	keywords = {Anonymization, Chi-Square, Correlation, Diseases, Heart, Measurement, application service provider, business data processing, data mining, data privacy, data publishing, domain generalization hierarchies, domain-driven data mining, domain-driven data mining outsourcing, ontologies (artificial intelligence), ontology-enhanced interactive anonymization, outsourcing, privacy},
	pages = {9--14}
}

@inproceedings{pinto_comparison_2012,
	title = {A {Comparison} of anonymization protection principles},
	doi = {10.1109/IRI.2012.6303012},
	abstract = {We do a survey of some of the most important principles of anonymization present in the literature. We identify different kinds of attacks that can be thrown against an anonymized dataset and give formulas for the maximum probability of success for each. For each principle, we identify whether it is monotonous, what attacks it is suited to counter, if any, and what principles imply other principles. We end by giving a classification of anonymization principles and giving guidelines to choosing the right principle for an application. Although we could not cover all principles in the literature, this is a first step to a systematization and simplification of proposals for anonymization principles.},
	booktitle = {2012 {IEEE} 13th {International} {Conference} on {Information} {Reuse} {Integration} ({IRI})},
	author = {Pinto, A. M.},
	month = aug,
	year = {2012},
	note = {00003},
	keywords = {Databases, Entropy, Guidelines, Organizations, Radiation detectors, anonymization protection principles, anonymized dataset, data mining, maximum probability, privacy, probability, security of data, simplification, systematization},
	pages = {207--214}
}

@article{gionis_k-anonymization_2009,
	title = {k-{Anonymization} with {Minimal} {Loss} of {Information}},
	volume = {21},
	abstract = {The technique of k-anonymization allows the releasing of databases that contain personal information while ensuring some degree of individual privacy. Anonymization is usually performed by generalizing database entries. We formally study the concept of generalization, and propose three information-theoretic measures for capturing the amount of information that is lost during the anonymization process. The proposed measures are more general and more accurate than those that were proposed by Meyerson and Williams and Aggarwal et al. We study the problem of achieving k-anonymity with minimal loss of information. We prove that it is NP-hard and study polynomial approximations for the optimal solution. Our first algorithm gives an approximation guarantee of O(ln k) for two of our measures as well as for the previously studied measures. This improves the best-known O(k)-approximation in. While the previous approximation algorithms relied on the graph representation framework, our algorithm relies on a novel hypergraph representation that enables the improvement in the approximation ratio from O(k) to O(ln k). As the running time of the algorithm is O(n2k},
	number = {2},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Gionis, A. and Tassa, T.},
	year = {2009},
	note = {00065},
	pages = {206--219}
}

@article{woo_global_2009,
	title = {Global {Measures} of {Data} {Utility} for {Microdata} {Masked} for {Disclosure} {Limitation}},
	volume = {1},
	url = {http://repository.cmu.edu/jpc/vol1/iss1/7},
	number = {1},
	journal = {Journal of Privacy and Confidentiality},
	author = {Woo, Mi-Ja and Reiter, Jerome and Oganian, Anna and Karr, Alan},
	month = apr,
	year = {2009},
	note = {00070}
}

@inproceedings{mohammadi_detecting_2017,
	address = {Prague, Czech Republic},
	title = {Detecting {Cross}-{Site} {Scripting} {Vulnerabilities} through {Automated} {Unit} {Testing}},
	abstract = {The best practice to prevent Cross Site Scripting (XSS) attacks is to apply encoders to sanitize untrusted data. To balance security and functionality, encoders should be applied to match the web page context, such as Html body, JavaScript, and style sheets.  A common programming error is the use of a wrong encoder to sanitize untrusted data, leaving the application vulnerable. We present a security unit testing approach to detect XSS vulnerabilities caused by improper encoding of untrusted data. Unit tests for the XSS vulnerability are automatically constructed out of each web page and then evaluated by a unit test execution framework. A grammar-based attack generator is used to automatically generate test inputs. We evaluate our approach on a large open source medical records application, demonstrating that we can detect many 0-day XSS vulnerabilities with very low false positives, and that the grammar-based attack generator has better test coverage than industry best practices},
	booktitle = {2017 {IEEE} {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} ({QRS})},
	publisher = {IEEE},
	author = {Mohammadi, Mahmoud and Chu, Bill and Richter Lipford, Heather},
	year = {2017},
	note = {00000}
}

@inproceedings{landman_challenges_2017,
	address = {Piscataway, NJ, USA},
	series = {{ICSE} '17},
	title = {Challenges for {Static} {Analysis} of {Java} {Reflection}: {Literature} {Review} and {Empirical} {Study}},
	isbn = {978-1-5386-3868-2},
	shorttitle = {Challenges for {Static} {Analysis} of {Java} {Reflection}},
	url = {https://doi.org/10.1109/ICSE.2017.53},
	doi = {10.1109/ICSE.2017.53},
	abstract = {The behavior of software that uses the Java Reflection API is fundamentally hard to predict by analyzing code. Only recent static analysis approaches can resolve reflection under unsound yet pragmatic assumptions. We survey what approaches exist and what their limitations are. We then analyze how real-world Java code uses the Reflection API, and how many Java projects contain code challenging state-of-the-art static analysis. Using a systematic literature review we collected and categorized all known methods of statically approximating reflective Java code. Next to this we constructed a representative corpus of Java systems and collected descriptive statistics of the usage of the Reflection API. We then applied an analysis on the abstract syntax trees of all source code to count code idioms which go beyond the limitation boundaries of static analysis approaches. The resulting data answers the research questions. The corpus, the tool and the results are openly available. We conclude that the need for unsound assumptions to resolve reflection is widely supported. In our corpus, reflection can not be ignored for 78\% of the projects. Common challenges for analysis tools such as non-exceptional exceptions, programmatic filtering meta objects, semantics of collections, and dynamic proxies, widely occur in the corpus. For Java software engineers prioritizing on robustness, we list tactics to obtain more easy to analyze reflection code, and for static analysis tool builders we provide a list of opportunities to have significant impact on real Java code.},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Landman, Davy and Serebrenik, Alexander and Vinju, Jurgen J.},
	year = {2017},
	note = {00002},
	keywords = {Java, Static analysis, empirical study, reflection, systematic literature review},
	pages = {507--518}
}

@article{misek_control_2017,
	series = {8th {International} {Conference} on {Ambient} {Systems}, {Networks} and {Technologies}, {ANT}-2017 and the 7th {International} {Conference} on {Sustainable} {Energy} {Information} {Technology}, {SEIT} 2017, 16-19 {May} 2017, {Madeira}, {Portugal}},
	title = {Control {Flow} {Ambiguous}-{Type} {Inter}-{Procedural} {Semantic} {Analysis} for {Dynamic} {Language} {Compilation}},
	volume = {109},
	issn = {1877-0509},
	url = {http://www.sciencedirect.com/science/article/pii/S1877050917311341},
	doi = {10.1016/j.procs.2017.05.452},
	abstract = {The recent expansion of cloud-based solutions highlights that legacy programming languages and technologies, such as the PHP language, are still in heavy use. Furthermore, it turns out that for their effective integration into modern platforms, it is important to understand the legacy code base to provide modern analysis, testing and eventually compiler tooling. The dynamic language PHP would be the perfect candidate for such a synergy, due to its extensive usage, frequent issues in source codes and the need for a better performance and scalability.
In this paper, we describe a solution that combines known static language code analysis techniques and that is enhanced by numerous modifications. As a result, we have a design and implementation of a PHP dynamic language code analysis, compiler and runtime, which improves performance and allows for more advanced high level code analysis and tools, all incorporated into the modern platform of .NET Core while taking advantage of the Microsoft Roslyn Compiler Platform.},
	journal = {Procedia Computer Science},
	author = {Misek, Jakub and Zavoral, Filip},
	year = {2017},
	note = {00000},
	keywords = {PHP, compilers, dynamic languages, fast static analysis, type analysis},
	pages = {955--962}
}

@inproceedings{simos_tls_2016,
	title = {{TLS} {Cipher} {Suites} {Recommendations}: {A} {Combinatorial} {Coverage} {Measurement} {Approach}},
	shorttitle = {{TLS} {Cipher} {Suites} {Recommendations}},
	doi = {10.1109/QRS.2016.18},
	abstract = {We present a coverage measurement for TLS cipher suites recommendations provided by various regulatory and intelligence organizations such as the IETF, Mozilla, ENISA, German BSI, and USA NSA. These cipher suites are measured and analyzed using a combinatorial approach, which was made feasible via developing the necessary input models. Besides shedding light on the coverage achieved by the proposed recommendations, we discuss implications towards aspects of test quality. One of them relates to the testing of a TLS implementation, where a system designer or tester should expand the TLS cipher suite registry and integrate the information back to the TLS implementation itself such that the (overall) testing effort is reduced.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} ({QRS})},
	author = {Simos, D. E. and Kleine, K. and Voyiatzis, A. G. and Kuhn, R. and Kacker, R.},
	month = aug,
	year = {2016},
	note = {00001},
	keywords = {Ciphers, Combinatorial testing, Cryptography, Encryption, Measurement, Protocols, Software, TLS, TLS cipher suite recommendations, TLS cipher suite registry, TLS implementation, Testing, cipher suites, combinatorial coverage measurement, combinatorial mathematics, coverage, intelligence organizations, program testing, regulatory organizations, subsets, test quality},
	pages = {69--73}
}

@inproceedings{hills_static_2014,
	address = {New York, NY, USA},
	series = {{ASE} '14},
	title = {Static, {Lightweight} {Includes} {Resolution} for {PHP}},
	isbn = {978-1-4503-3013-8},
	url = {http://doi.acm.org/10.1145/2642937.2643017},
	doi = {10.1145/2642937.2643017},
	abstract = {Dynamic languages include a number of features that are challenging to model properly in static analysis tools. In PHP, one of these features is the include expression, where an arbitrary expression provides the path of the file to include at runtime. In this paper we present two complementary analyses for statically resolving PHP includes, one that works at the level of individual PHP files, and one targeting PHP programs possibly consisting of multiple scripts. To evaluate the effectiveness of these analyses we have applied the first to a corpus of 20 open-source systems, totaling more than 4.5 million lines of PHP, and the second to a number of programs from a subset of these systems. Our results show that, in many cases, includes can be resolved to a specific file or a small subset of possible files, enabling better IDE features and more advanced program analysis tools for PHP.},
	booktitle = {Proceedings of the 29th {ACM}/{IEEE} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {ACM},
	author = {Hills, Mark and Klint, Paul and Vinju, Jurgen J.},
	year = {2014},
	note = {00011},
	keywords = {PHP, Static analysis, dynamic language features},
	pages = {503--514}
}

@article{prokhorenko_web_2016,
	title = {Web application protection techniques: {A} taxonomy},
	volume = {60},
	issn = {1084-8045},
	shorttitle = {Web application protection techniques},
	url = {http://www.sciencedirect.com/science/article/pii/S1084804515002908},
	doi = {10.1016/j.jnca.2015.11.017},
	abstract = {The growing popularity of web applications makes them an attractive target for malicious users. Large amounts of private data commonly processed and stored by web applications are a valuable asset for attackers, resulting in more sophisticated web-oriented attacks. Therefore, multiple web application protections have been proposed. Such protections range from narrow, vector-specific solutions used to prevent some attacks only, to generic development practices aiming to build secure software from the ground up. However, due to the diversity of the proposed protection methods, choosing one to protect an existing or a planned application becomes an issue of its own. This paper surveys the web application protection techniques, aiming to systematise the existing approaches into a holistic big picture. First, a general background is presented to highlight the issues specific to web applications. Then, a novel classification of the protections is provided. A variety of existing protections is overviewed and systematised next, followed by a discussion of current issues and limitation inherent to the existing protection methods. Finally, the overall picture is summarised and future potentially beneficial research lines are discussed.},
	journal = {Journal of Network and Computer Applications},
	author = {Prokhorenko, Victor and Choo, Kim-Kwang Raymond and Ashman, Helen},
	month = jan,
	year = {2016},
	note = {00021},
	keywords = {Web IDS, Web protection, web application, web security},
	pages = {95--112}
}

@article{el-hajj_security-by-construction_2016,
	title = {Security-by-construction in web applications development via database annotations},
	volume = {59},
	issn = {0167-4048},
	url = {http://www.sciencedirect.com/science/article/pii/S0167404815001972},
	doi = {10.1016/j.cose.2015.12.004},
	abstract = {Huge amounts of data and personal information are being sent to and retrieved from web applications on daily basis. Every application has its own confidentiality and integrity policies. Violating these policies can have broad negative impact on the involved company's financial status, while enforcing them is very hard even for the developers with good security background. In this paper, we propose a framework that enforces security-by-construction in web applications. Minimal developer effort is required, in a sense that the developer only needs to annotate database attributes by a security class. The web application code is then converted into an intermediary representation, called Extended Program Dependence Graph (EPDG). Using the EPDG, the provided annotations are propagated to the application code and run against generic security enforcement rules that were carefully designed to detect insecure information flows as early as they occur. As a result, any violation in the data's confidentiality or integrity policies is reported. As a proof of concept, two PHP web applications, Hotel Reservation and Auction, were used for testing and validation. The proposed system was able to catch all the existing insecure information flows at their source. Apart from the proof of concept and to comprehensively test the performance of our system, we compared it to JLift, a state-of-the-art type-based system approach to detect information leaks. Both approaches were run against custom made PHP web applications and publicly available applications downloaded from SourceForge and GitHub. The results show that our approach outperforms JLift in terms of accuracy and the number of false alarms, and is able to catch the insecure flows at their source when they first occurred.},
	journal = {Computers \& Security},
	author = {El-Hajj, Wassim and Ben Brahim, Ghassen and Hajj, Hazem and Safa, Haidar and Adaimy, Ralph},
	month = jun,
	year = {2016},
	note = {00000},
	keywords = {Database annotation, Program dependence graph, Secure information flow, Security by construction, Web applications security},
	pages = {151--165}
}

@misc{noauthor_owasp_nodate,
	title = {{OWASP} {Zed} {Attack} {Proxy} {Project} - {OWASP}},
	url = {https://www.owasp.org/index.php/OWASP_Zed_Attack_Proxy_Project},
	urldate = {2017-05-30TZ},
	note = {00006}
}

@inproceedings{barros_static_2015,
	title = {Static {Analysis} of {Implicit} {Control} {Flow}: {Resolving} {Java} {Reflection} and {Android} {Intents} ({T})},
	booktitle = {Automated {Software} {Engineering} ({ASE}), 2015 30th {IEEE}/{ACM} {International} {Conference} on},
	publisher = {IEEE},
	author = {Barros, Paulo and Just, René and Millstein, Suzanne and Vines, Paul and Dietl, Werner and Ernst, Michael D and {others}},
	year = {2015},
	note = {00019},
	pages = {669--679}
}

@misc{noauthor_acunetix_nodate,
	title = {Acunetix {Web} {Application} {Vulnerability} {Report} 2016},
	url = {https://www.acunetix.com/acunetix-web-application-vulnerability-report-2016/},
	abstract = {Out now Acunetix Web Application Vulnerability Report 2016. Results show that 55\% of websites have one or more high-severity vulnerabilities up 9\% from 2015},
	journal = {Acunetix},
	note = {00000}
}

@inproceedings{xie_why_2011,
	title = {Why do programmers make security errors?},
	doi = {10.1109/VLHCC.2011.6070393},
	abstract = {A large number of software security vulnerabilities are caused by software errors that are committed by software developers. We believe that interactive tool support will play an important role in aiding software developers to develop more secure software. However, an in-depth understanding of how and why software developers produce security bugs is needed to design such tools. We conducted a semi-structured interview study on 15 professional software developers to understand their perceptions and behaviors related to software security. Our results reveal a disconnect between developers' conceptual understanding of security and their attitudes regarding their personal responsibility and practices for software security.},
	booktitle = {2011 {IEEE} {Symposium} on {Visual} {Languages} and {Human}-{Centric} {Computing} ({VL}/{HCC})},
	author = {Xie, J. and Lipford, H. R. and Chu, B.},
	month = sep,
	year = {2011},
	note = {00019},
	keywords = {Computer bugs, Context, Interviews, Programming profession, Security, Software, interactive tool support, programmers, security bugs, security errors, security of data, software developers, software errors, software security vulnerabilities, software tools},
	pages = {161--164}
}

@misc{center_for_history_and_new_media_zotero_nodate,
	title = {Zotero {Quick} {Start} {Guide}},
	url = {http://zotero.org/support/quick_start_guide},
	author = {{Center for History and New Media}},
	note = {00000}
}

@article{austin_comparison_2013,
	title = {A comparison of the efficiency and effectiveness of vulnerability discovery techniques},
	volume = {55},
	issn = {0950-5849},
	url = {http://www.sciencedirect.com/science/article/pii/S0950584912002339},
	doi = {10.1016/j.infsof.2012.11.007},
	abstract = {Context
Security vulnerabilities discovered later in the development cycle are more expensive to fix than those discovered early. Therefore, software developers should strive to discover vulnerabilities as early as possible. Unfortunately, the large size of code bases and lack of developer expertise can make discovering software vulnerabilities difficult. A number of vulnerability discovery techniques are available, each with their own strengths.
Objective
The objective of this research is to aid in the selection of vulnerability discovery techniques by comparing the vulnerabilities detected by each and comparing their efficiencies.
Method
We conducted three case studies using three electronic health record systems to compare four vulnerability discovery techniques: exploratory manual penetration testing, systematic manual penetration testing, automated penetration testing, and automated static analysis.
Results
In our case study, we found empirical evidence that no single technique discovered every type of vulnerability. We discovered that the specific set of vulnerabilities identified by one tool was largely orthogonal to that of other tools. Systematic manual penetration testing found the most design flaws, while automated static analysis found the most implementation bugs. The most efficient discovery technique in terms of vulnerabilities discovered per hour was automated penetration testing.
Conclusion
The results show that employing a single technique for vulnerability discovery is insufficient for finding all types of vulnerabilities. Each technique identified only a subset of the vulnerabilities, which, for the most part were independent of each other. Our results suggest that in order to discover the greatest variety of vulnerability types, at least systematic manual penetration testing and automated static analysis should be performed.},
	number = {7},
	urldate = {2017-05-30TZ},
	journal = {Information and Software Technology},
	author = {Austin, Andrew and Holmgreen, Casper and Williams, Laurie},
	month = jul,
	year = {2013},
	note = {00021},
	keywords = {Black box testing, Penetration testing, Security, Static analysis, Vulnerability, White box testing},
	pages = {1279--1288}
}

@article{halfond_wasp:_2008,
	title = {{WASP}: {Protecting} {Web} {Applications} {Using} {Positive} {Tainting} and {Syntax}-{Aware} {Evaluation}},
	volume = {34},
	issn = {0098-5589},
	shorttitle = {{WASP}},
	doi = {10.1109/TSE.2007.70748},
	abstract = {Many software systems have evolved to include a Web-based component that makes them available to the public via the Internet and can expose them to a variety of Web-based attacks. One of these attacks is SQL injection, which can give attackers unrestricted access to the databases that underlie Web applications and has become increasingly frequent and serious. This paper presents a new highly automated approach for protecting Web applications against SQL injection that has both conceptual and practical advantages over most existing techniques. From a conceptual standpoint, the approach is based on the novel idea of positive tainting and on the concept of syntax-aware evaluation. From a practical standpoint, our technique is precise and efficient, has minimal deployment requirements, and incurs a negligible performance overhead in most cases. We have implemented our techniques in the Web application SQL-injection preventer (WASP) tool, which we used to perform an empirical evaluation on a wide range of Web applications that we subjected to a large and varied set of attacks and legitimate accesses. WASP was able to stop all of the otherwise successful attacks and did not generate any false positives.},
	number = {1},
	journal = {IEEE Transactions on Software Engineering},
	author = {Halfond, W. and Orso, A. and Manolios, P.},
	month = jan,
	year = {2008},
	note = {00187},
	keywords = {Internet, SQL, Security and Protection, Software systems, WASP, Web application SQL-injection preventer, Web-based attacks, positive tainting, protection mechanisms, security of data, syntax-aware evaluation},
	pages = {65--81}
}

@misc{noauthor_itrust:_nodate,
	title = {{iTrust}: {Role}-{Based} {Healthcare}},
	url = {http://agile.csc.ncsu.edu/iTrust/wiki/doku.php},
	urldate = {2017-05-30TZ},
	note = {00000}
}

@misc{noauthor_server-side_nodate,
	title = {Server-{Side} {XSS} {Attack} {Detection} with {ModSecurity} and {PhantomJS}},
	url = {https://www.trustwave.com/Resources/SpiderLabs-Blog/Server-Side-XSS-Attack-Detection-with-ModSecurity-and-PhantomJS/},
	abstract = {Client-Side JS Overriding Limitations In a previous blog post, I outlined how you could use ModSecurity to inject defensive JS into the HTML response page sent to the client web browser. The goal of this technique was to override many...},
	urldate = {2017-05-03TZ},
	journal = {Trustwave},
	note = {00000}
}

@inproceedings{mirhoseini_cryptoml:_2016,
	title = {{CryptoML}: {Secure} outsourcing of big data machine learning applications},
	shorttitle = {{CryptoML}},
	doi = {10.1109/HST.2016.7495574},
	abstract = {We present CryptoML, the first practical framework for provably secure and efficient delegation of a wide range of contemporary matrix-based machine learning (ML) applications on massive datasets. In CryptoML a delegating client with memory and computational resource constraints wishes to assign the storage and ML-related computations to the cloud servers, while preserving the privacy of its data. We first suggest the dominant components of delegation performance cost, and create a matrix sketching technique that aims at minimizing the cost by data pre-processing. We then propose a novel interactive delegation protocol based on the provably secure Shamir's secret sharing. The protocol is customized for our new sketching technique to maximize the client's resource efficiency. CryptoML shows a new trade-off between the efficiency of secure delegation and the accuracy of the ML task. Proof of concept evaluations corroborate applicability of CryptoML to datasets with billions of non-zero records.},
	booktitle = {2016 {IEEE} {International} {Symposium} on {Hardware} {Oriented} {Security} and {Trust} ({HOST})},
	author = {Mirhoseini, A. and Sadeghi, A. R. and Koushanfar, F.},
	month = may,
	year = {2016},
	note = {00002},
	keywords = {Algorithm design and analysis, Approximation algorithms, Big Data, Big Data machine learning, CryptoML, Cryptography, Decision support systems, ML-related computations, Protocols, Shamir secret sharing, Sparse matrices, cloud servers, computational resource constraints, contemporary matrix-based ML, contemporary matrix-based machine learning, cryptographic protocols, data pre-processing, data privacy, interactive delegation protocol, learning (artificial intelligence), matrix algebra, matrix sketching technique, minimisation, public key cryptography, secure outsourcing},
	pages = {149--154}
}

@misc{noauthor_xssed_nodate,
	title = {{XSSed} {\textbar} {Cross} {Site} {Scripting} ({XSS}) attacks information and archive},
	url = {http://xssed.com/},
	urldate = {2017-04-14TZ},
	note = {00000}
}

@misc{noauthor_cyclos_nodate,
	title = {Cyclos - mobile and online payment software},
	url = {http://www.cyclos.org/},
	abstract = {Cyclos online and mobile payment software developed by STRO, a leading organisation in monetary innovations. STRO's mission is to enable local/regional economies to flourish in a sustainable way.},
	urldate = {2017-03-31TZ},
	journal = {Cyclos software},
	note = {00000}
}

@article{noauthor_cvss_nodate,
	title = {{CVSS} v3.0 {Specification} {Document}},
	url = {https://www.first.org/cvss/specification-document},
	abstract = {The Common Vulnerability Scoring System (CVSS) provides an open framework for communicating the characteristics and impacts of IT vulnerabilities.},
	note = {00000 
Accessed: 2017-3-24 
bibtex: noauthor\_undated-yh}
}

@misc{noauthor_bapp_nodate,
	title = {{BApp} {Details} - {XSS} {Validator}},
	url = {https://portswigger.net/bappstore/ShowBappDetails.aspx?uuid=98275a25394a417c9480f58740c1d981},
	urldate = {2017-03-30TZ},
	note = {00000}
}
@inproceedings{ghosh_privacy_2012,
	title = {Privacy {Control} in {Smart} {Phones} {Using} {Semantically} {Rich} {Reasoning} and {Context} {Modeling}},
	doi = {10.1109/SPW.2012.27},
	abstract = {We present our ongoing work on user data and contextual privacy preservation in mobile devices through semantic reasoning. Recent advances in context modeling, tracking and collaborative localization have led to the emergence of a new class of smart phone applications that can access and share embedded sensor data. Unfortunately, this also means significant amount of user context information is now accessible to applications and potentially others, creating serious privacy and security concerns. Mobile OS frameworks like Android lack mechanisms for dynamic privacy control. We show how data flow among applications can be successfully filtered at a much more granular level using semantic web driven technologies that model device location, surroundings, application roles as well as context-dependent information sharing policies.},
	booktitle = {2012 {IEEE} {Symposium} on {Security} and {Privacy} {Workshops}},
	author = {Ghosh, D. and Joshi, A. and Finin, T. and Jagtap, P.},
	month = may,
	year = {2012},
	note = {00027},
	keywords = {Android, Cognition, Context, Jena, Mobile communication, Security, application roles, collaborative localization, context awareness, context modeling, context tracking, context-dependent information sharing policies, contextual privacy preservation, data privacy, embedded sensor data access, embedded sensor data sharing, inference mechanisms, mobile, mobile OS frameworks, mobile computing, mobile devices, model device location, operating systems (computers), privacy, privacy control, rich reasoning, security concerns, security of data, semantic Web, semantic Web driven technologies, semantic reasoning, smart phones, user context information},
	pages = {82--85}
}

@inproceedings{plachkinova_taxonomy_2015,
	title = {A {Taxonomy} of {mHealth} {Apps} – {Security} and {Privacy} {Concerns}},
	doi = {10.1109/HICSS.2015.385},
	abstract = {With the increasing use of smartphones for healthcare purposes, more and more people now share their personal healthcare information using a variety of applications. The vast number of existing mobile health (mHealth) applications creates a serious problem for users, as often times they are unaware of how their data are managed and used. We propose a taxonomy incorporating the most significant security and privacy aspects of mHealth applications. This artifact can help outline some of the problems related to creating and downloading mHealth applications. The taxonomy was tested with 38 top-rated Android and iOS healthcare applications. Results of the evaluation suggest that having a unified mechanism to categorize mHealth applications with respect to security and privacy is important and can be beneficial. This study contributes to literature, as it builds upon prior work and adds knowledge to a still new and relatively unexplored domain such as mobile healthcare.},
	booktitle = {2015 48th {Hawaii} {International} {Conference} on {System} {Sciences}},
	author = {Plachkinova, M. and Andrés, S. and Chatterjee, S.},
	month = jan,
	year = {2015},
	note = {00012},
	keywords = {Android (operating system), Android healthcare application, Education, Medical services, Mobile communication, Security, data privacy, health care, healthcare, iOS healthcare application, information security, mHealth, mHealth application, mHealth apps, mobile, mobile computing, mobile health application, mobile healthcare, personal healthcare information, privacy, privacy aspect, security aspect, security of data, smart phones, smartphone, taxonomy},
	pages = {3187--3196}
}

@inproceedings{bhattacharya_empirical_2013,
	title = {An {Empirical} {Analysis} of {Bug} {Reports} and {Bug} {Fixing} in {Open} {Source} {Android} {Apps}},
	doi = {10.1109/CSMR.2013.23},
	abstract = {Smartphone platforms and applications (apps) have gained tremendous popularity recently. Due to the novelty of the smartphone platform and tools, and the low barrier to entry for app distribution, apps are prone to errors, which affects user experience and requires frequent bug fixes. An essential step towards correcting this situation is understanding the nature of the bugs and bug-fixing processes associated with smartphone platforms and apps. However, prior empirical bug studies have focused mostly on desktop and server applications. Therefore, in this paper, we perform an in-depth empirical study on bugs in the Google Android smartphone platform and 24 widely-used open-source Android apps from diverse categories such as communication, tools, and media. Our analysis has three main thrusts. First, we define several metrics to understand the quality of bug reports and analyze the bug-fix process, including developer involvement. Second, we show how differences in bug life-cycles can affect the bug-fix process. Third, as Android devices carry significant amounts of security-sensitive information, we perform a study of Android security bugs. We found that, although contributor activity in these projects is generally high, developer involvement decreases in some projects, similarly, while bug-report quality is high, bug triaging is still a problem. Finally, we observe that in Android apps, security bug reports are of higher quality but get fixed slower than non-security bugs. We believe that the findings of our study could potentially benefit both developers and users of Android apps.},
	booktitle = {2013 17th {European} {Conference} on {Software} {Maintenance} and {Reengineering}},
	author = {Bhattacharya, P. and Ulanova, L. and Neamtiu, I. and Koduru, S. C.},
	month = mar,
	year = {2013},
	note = {00032},
	keywords = {Android security bugs, Androids, Computer bugs, Google, Google Android, Google Android smartphone, Humanoid robots, Mobile communication, Security, app distribution, bug fixing, bug fixing process, bug life-cycle, bug report, bug reports, developer involvement, empirical studies, mobile computing, open source Android apps, operating systems (computers), program debugging, security bugs, security of data, smart phone application, smart phone platform, smart phones, smartphone apps},
	pages = {133--143}
}

@inproceedings{sadeghi_analysis_2015,
	title = {Analysis of {Android} {Inter}-{App} {Security} {Vulnerabilities} {Using} {COVERT}},
	volume = {2},
	doi = {10.1109/ICSE.2015.233},
	abstract = {The state-of-the-art in securing mobile software systems are substantially intended to detect and mitigate vulnerabilities in a single app, but fail to identify vulnerabilities that arise due to the interaction of multiple apps, such as collusion attacks and privilege escalation chaining, shown to be quite common in the apps on the market. This paper demonstrates COVERT, a novel approach and accompanying tool-suite that relies on a hybrid static analysis and lightweight formal analysis technique to enable compositional security assessment of complex software. Through static analysis of Android application packages, it extracts relevant security specifications in an analyzable formal specification language, and checks them as a whole for inter-app vulnerabilities. To our knowledge, COVERT is the first formally-precise analysis tool for automated compositional analysis of Android apps. Our study of hundreds of Android apps revealed dozens of inter-app vulnerabilities, many of which were previously unknown.},
	booktitle = {2015 {IEEE}/{ACM} 37th {IEEE} {International} {Conference} on {Software} {Engineering}},
	author = {Sadeghi, A. and Bagheri, H. and Malek, S.},
	month = may,
	year = {2015},
	note = {00012},
	keywords = {Analytical models, Android (operating system), Android application package static analysis, Android inter-app security vulnerability analysis, Androids, COVERT approach, Humanoid robots, Metals, Mobile communication, Security, collusion attacks, complex software compositional security assessment, formal specification, formal specification language, formally-precise analysis tool, hybrid static analysis, lightweight formal analysis technique, mobile computing, mobile software systems, privilege escalation chaining, program diagnostics, security of data, smart phones, specification languages},
	pages = {725--728}
}

@misc{noauthor_full_nodate,
	title = {Full property table},
	url = {https://www.w3.org/TR/CSS21/propidx.html},
	urldate = {2017-01-24TZ},
	note = {00000}
}

@inproceedings{monshizadeh_mace:_2014,
	address = {New York, NY, USA},
	series = {{CCS} '14},
	title = {{MACE}: {Detecting} {Privilege} {Escalation} {Vulnerabilities} in {Web} {Applications}},
	isbn = {978-1-4503-2957-6},
	shorttitle = {{MACE}},
	url = {http://doi.acm.org/10.1145/2660267.2660337},
	doi = {10.1145/2660267.2660337},
	abstract = {We explore the problem of identifying unauthorized privilege escalation instances in a web application. These vulnerabilities are typically caused by missing or incorrect authorizations in the server side code of a web application. The problem of identifying these vulnerabilities is compounded by the lack of an access control policy specification in a typical web application, where the only supplied documentation is in fact its source code. This makes it challenging to infer missing checks that protect a web application's sensitive resources. To address this challenge, we develop a notion of authorization context consistency, which is satisfied when a web application consistently enforces its authorization checks across the code. We then present an approach based on program analysis to check for authorization state consistency in a web application. Our approach is implemented in a tool called MACE that uncovers vulnerabilities that could be exploited in the form of privilege escalation attacks. In particular, MACE is the first tool reported in the literature to identify a new class of web application vulnerabilities called Horizontal Privilege Escalation (HPE) vulnerabilities. MACE works on large codebases, and discovers serious, previously unknown, vulnerabilities in 5 out of 7 web applications tested. Without MACE, a comparable human-driven security audit would require weeks of effort in code inspection and testing.},
	urldate = {2017-01-01TZ},
	booktitle = {Proceedings of the 2014 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Monshizadeh, Maliheh and Naldurg, Prasad and Venkatakrishnan, V. N.},
	year = {2014},
	note = {00011},
	keywords = {access control, authorization, web security},
	pages = {690--701}
}

@misc{jr_css_2014,
	type = {text},
	title = {{CSS} {Syntax} {Module} {Level} 3},
	url = {https://www.w3.org/TR/css-syntax/},
	urldate = {2016-11-23TZ},
	author = {Jr, Tab Atkins and Sapin, Simon},
	month = feb,
	year = {2014},
	note = {00000}
}

@inproceedings{heiderich_scriptless_2012,
	title = {Scriptless attacks: stealing the pie without touching the sill},
	booktitle = {Proceedings of the 2012 {ACM} conference on {Computer} and communications security},
	publisher = {ACM},
	author = {Heiderich, Mario and Niemietz, Marcus and Schuster, Felix and Holz, Thorsten and Schwenk, Jörg},
	year = {2012},
	note = {00060 
bibtex: heiderich2012scriptless},
	pages = {760--771}
}

@inproceedings{heiderich_mxss_2013,
	title = {mxss attacks: {Attacking} well-secured web-applications by using innerhtml mutations},
	booktitle = {Proceedings of the 2013 {ACM} {SIGSAC} conference on {Computer} \& communications security},
	publisher = {ACM},
	author = {Heiderich, Mario and Schwenk, Jörg and Frosch, Tilman and Magazinius, Jonas and Yang, Edward Z},
	year = {2013},
	note = {00035 
bibtex: heiderich2013mxss},
	pages = {777--788}
}

@misc{noauthor_[dom_nodate,
	title = {[{DOM} {Based} {Cross} {Site} {Scripting} or {XSS} of the {Third} {Kind}] {Web} {Security} {Articles} - {Web} {Application} {Security} {Consortium}},
	url = {http://www.webappsec.org/projects/articles/071105.shtml},
	urldate = {2016-11-22TZ},
	note = {00126}
}

@misc{noauthor_nvd_nodate,
	title = {{NVD} - {CVSS} v2 {Calculator}},
	url = {https://nvd.nist.gov/CVSS/v2-calculator},
	urldate = {2016-10-07TZ},
	note = {00000}
}

@misc{noauthor_first.org_nodate,
	title = {{FIRST}.org / {Global} {Initiatives} / {Special} {Interest} {Groups} ({SIGs}) / {Common} {Vulnerability} {Scoring} {System} ({CVSS}-{SIG}) / {CVSS} v3.0 {Calculator}},
	url = {https://www.first.org/cvss/calculator/3.0},
	urldate = {2016-10-07TZ},
	note = {00000}
}

@misc{noauthor_nvd_nodate-1,
	title = {{NVD} - {CVSS}},
	url = {https://nvd.nist.gov/cvss.cfm},
	urldate = {2016-10-07TZ},
	note = {00000}
}

@inproceedings{thevenod-fosse_experimental_1991,
	title = {An experimental study on software structural testing: deterministic versus random input generation},
	shorttitle = {An experimental study on software structural testing},
	doi = {10.1109/FTCS.1991.146694},
	abstract = {The fault revealing power of different test patterns derived from ten structural test criteria currently referred to in unit testing is investigated. Experiments performed on four programs that are pieces of a real-life software system from the nuclear field are reported. Three test input generation techniques are studied: (1) deterministic choice, (2) random selection based on an input probability distribution determined according to the adopted structural test criterion, and (3) random selection from a uniform distribution on the input domain. Mutation analysis is used to assess the test set efficiency with respect to error detection. The experimental results involve a total of 2914 mutants. They show that structural statistical testing, which exhibits the highest mutation scores, leaving alive only six from 2816 nonequivalent mutants within short testing times, is the most efficient. A regards unit testing of programs whose structure remains tractable, the experiments show the adequacy of a fault removal strategy combining statistical and deterministic test patterns.{\textless}{\textgreater}},
	booktitle = {Fault-{Tolerant} {Computing}, 1991. {FTCS}-21. {Digest} of {Papers}., {Twenty}-{First} {International} {Symposium}},
	author = {Thevenod-Fosse, P. and Waeselynck, H. and Crouzet, Y.},
	month = jun,
	year = {1991},
	note = {00079},
	keywords = {Application software, Flow graphs, Genetic mutations, Nuclear power generation, Software design, Software systems, Software testing, Statistical analysis, deterministic choice, deterministic generation, error detection, fault diagnosis, fault removal strategy, fault revealing power, fault tolerant computing, mutation analysis, probability distribution, program testing, random input generation, random selection, software structural testing, test input generation techniques, unit testing},
	pages = {410--417}
}

@inproceedings{martin_automatic_2008,
	address = {Berkeley, CA, USA},
	series = {{SS}'08},
	title = {Automatic {Generation} of {XSS} and {SQL} {Injection} {Attacks} with {Goal}-directed {Model} {Checking}},
	url = {http://dl.acm.org/citation.cfm?id=1496711.1496714},
	abstract = {Cross-site scripting (XSS) and SQL injection errors are two prominent examples of taint-based vulnerabilities that have been responsible for a large number of security breaches in recent years. This paper presents QED, a goal-directed model-checking system that automatically generates attacks exploiting taint-based vulnerabilities in large Java web applications. This is the first time where model checking has been used successfully on real-life Java programs to create attack sequences that consist of multiple HTTP requests. QED accepts any Java web application that is written to the standard servlet specification. The analyst specifies the vulnerability of interest in a specification that looks like a Java code fragment, along with a range of values for form parameters. QED then generates a goal-directed analysis from the specification to perform session-aware tests, optimizes to eliminate inputs that are not of interest, and feeds the remainder to a model checker. The checker will systematically explore the remaining state space and report example attacks if the vulnerability specification is matched. QED provides better results than traditional analyses because it does not generate any false positive warnings. It proves the existence of errors by providing an example attack and a program trace showing how the code is compromised. Past experience suggests this is important because it makes it easy for the application maintainer to recognize the errors and to make the necessary fixes. In addition, for a class of applications, QED can guarantee that it has found all the potential bugs in the program. We have run QED over 3 Java web applications totaling 130,000 lines of code. We found 10 SQL injections and 13 cross-site scripting errors.},
	urldate = {2016-10-03TZ},
	booktitle = {Proceedings of the 17th {Conference} on {Security} {Symposium}},
	publisher = {USENIX Association},
	author = {Martin, Michael and Lam, Monica S.},
	year = {2008},
	note = {00120},
	pages = {31--43}
}

@article{garousi_systematic_2013,
	title = {A systematic mapping study of web application testing},
	volume = {55},
	issn = {0950-5849},
	url = {http://www.sciencedirect.com/science/article/pii/S0950584913000396},
	doi = {10.1016/j.infsof.2013.02.006},
	abstract = {Context
The Web has had a significant impact on all aspects of our society. As our society relies more and more on the Web, the dependability of web applications has become increasingly important. To make these applications more dependable, for the past decade researchers have proposed various techniques for testing web-based software applications. Our literature search for related studies retrieved 147 papers in the area of web application testing, which have appeared between 2000 and 2011.
Objective
As this research area matures and the number of related papers increases, it is important to systematically identify, analyze, and classify the publications and provide an overview of the trends in this specialized field.
Method
We review and structure the body of knowledge related to web application testing through a systematic mapping (SM) study. As part of this study, we pose two sets of research questions, define selection and exclusion criteria, and systematically develop and refine a classification schema. In addition, we conduct a bibliometrics analysis of the papers included in our study.
Results
Our study includes a set of 79 papers (from the 147 retrieved papers) published in the area of web application testing between 2000 and 2011. We present the results of our systematic mapping study. Our mapping data is available through a publicly-accessible repository. We derive the observed trends, for instance, in terms of types of papers, sources of information to derive test cases, and types of evaluations used in papers. We also report the demographics and bibliometrics trends in this domain, including top-cited papers, active countries and researchers, and top venues in this research area.
Conclusion
We discuss the emerging trends in web application testing, and discuss the implications for researchers and practitioners in this area. The results of our systematic mapping can help researchers to obtain an overview of existing web application testing approaches and indentify areas in the field that require more attention from the research community.},
	number = {8},
	urldate = {2016-10-03TZ},
	journal = {Information and Software Technology},
	author = {Garousi, Vahid and Mesbah, Ali and Betin-Can, Aysu and Mirshokraie, Shabnam},
	month = aug,
	year = {2013},
	note = {00036},
	keywords = {Bibliometrics, Paper repository, Systematic mapping, Testing, web application},
	pages = {1374--1396}
}

@inproceedings{sparks_automated_2007,
	title = {Automated {Vulnerability} {Analysis}: {Leveraging} {Control} {Flow} for {Evolutionary} {Input} {Crafting}},
	shorttitle = {Automated {Vulnerability} {Analysis}},
	doi = {10.1109/ACSAC.2007.27},
	abstract = {We present an extension of traditional "black box" fuzz testing using a genetic algorithm based upon a dynamic Markov model fitness heuristic. This heuristic allows us to "intelligently" guide input selection based upon feedback concerning the "success" of past inputs that have been tried. Unlike many software testing tools, our implementation is strictly based upon binary code and does not require that source code be available. Our evaluation on a Windows server program shows that this approach is superior to random black box fuzzing for increasing code coverage and depth of penetration into program control flow logic. As a result, the technique may be beneficial to the development of future automated vulnerability analysis tools.},
	booktitle = {Computer {Security} {Applications} {Conference}, 2007. {ACSAC} 2007. {Twenty}-{Third} {Annual}},
	author = {Sparks, S. and Embleton, S. and Cunningham, R. and Zou, C.},
	month = dec,
	year = {2007},
	note = {00059},
	keywords = {Application software, Automatic control, Computer security, Data security, Feedback, Flow graphs, Logic testing, Markov processes, National security, System testing, Windows server program, automated vulnerability analysis tool, binary code, black box fuzz testing, dynamic Markov model, fitness heuristic, genetic algorithm, genetic algorithms, program control flow logic, program control structures, program testing, security of data, software testing tool},
	pages = {477--486}
}

@inproceedings{ricca_web_2001,
	address = {Washington, DC, USA},
	series = {{ICSM} '01},
	title = {Web {Application} {Slicing}},
	isbn = {978-0-7695-1189-4},
	url = {http://dx.doi.org/10.1109/ICSM.2001.972725},
	doi = {10.1109/ICSM.2001.972725},
	abstract = {Program slicing revealed a useful way to limit the search of software defects during debugging and to better understand the decomposition of the application into computations. We propose to extend the extraction of slices to Web applications, in order to produce a reduced Web application which behaves as the original one with respect to some criterion, i.e., some displayed information of interest.After presenting the theoretical implications of applying slicing to Web applications, we will demonstrate its usefulness with reference to an example, derived from a survey of a set of travel agency sites. Web application slicing helps disclosing relevant information and understanding the internal system structure.},
	urldate = {2016-09-29TZ},
	booktitle = {Proceedings of the {IEEE} {International} {Conference} on {Software} {Maintenance} ({ICSM}'01)},
	publisher = {IEEE Computer Society},
	author = {Ricca, Filippo and Tonella, Paolo},
	year = {2001},
	note = {00060},
	keywords = {code analysis, program slicing, reverse engineering, web applications},
	pages = {148--}
}

@inproceedings{de_lucia_program_2001,
	title = {Program {Slicing}: {Methods} and {Applications}.},
	booktitle = {scam},
	author = {De Lucia, Andrea and {others}},
	year = {2001},
	note = {00168 
bibtex: de2001program},
	pages = {144--151}
}

@inproceedings{ricca_analysis_2001,
	address = {Washington, DC, USA},
	series = {{ICSE} '01},
	title = {Analysis and {Testing} of {Web} {Applications}},
	isbn = {978-0-7695-1050-7},
	url = {http://dl.acm.org/citation.cfm?id=381473.381476},
	abstract = {The economic relevance of Web applications increases the importance of controlling and improving their quality. Moreover, the new available technologies for their development allow the insertion of sophisticated functions, but often leave the developers responsible for their organization and evolution. As a consequence, a high demand is emerging for methodologies and tools for quality assurance of Web based systems.
In this paper, a UML model of Web applications is proposed for their high level representation. Such a model is the starting point for several analyses, which can help in the assessment of the static site structure. Moreover, it drives Web application testing, in that it can be exploited to define white box testing criteria and to semi-automatically generate the associated test cases.
The proposed techniques were applied to several real world Web applications. Results suggest that an automatic support to the verification and validation activities can be extremely beneficial. In fact, it guarantees that all paths in the site which satisfy a selected criterion are properly exercised before delivery. The high level of automation that is achieved in test case generation and execution increases the number of tests that are conducted and simplifies the regression checks.},
	urldate = {2016-09-29TZ},
	booktitle = {Proceedings of the 23rd {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Computer Society},
	author = {Ricca, Filippo and Tonella, Paolo},
	year = {2001},
	note = {00568},
	keywords = {Testing, UML modeling, code analysis, reverse engineering, web applications},
	pages = {25--34}
}
@misc{noauthor_dom_nodate,
	title = {{DOM} based {XSS} {Prevention} {Cheat} {Sheet} - {OWASP}},
	url = {https://www.owasp.org/index.php/DOM_based_XSS_Prevention_Cheat_Sheet},
	urldate = {2016-09-07TZ},
	note = {00000}
}

@misc{noauthor_xss_nodate,
	title = {{XSS} ({Cross} {Site} {Scripting}) {Prevention} {Cheat} {Sheet} - {OWASP}},
	url = {https://www.owasp.org/index.php/XSS_(Cross_Site_Scripting)_Prevention_Cheat_Sheet},
	urldate = {2016-09-07TZ},
	note = {00004}
}

@inproceedings{hoschele_mining_2016,
	address = {New York, NY, USA},
	series = {{ASE} 2016},
	title = {Mining {Input} {Grammars} from {Dynamic} {Taints}},
	isbn = {978-1-4503-3845-5},
	url = {http://doi.acm.org/10.1145/2970276.2970321},
	doi = {10.1145/2970276.2970321},
	abstract = {Knowing which part of a program processes which parts of an input can reveal the structure of the input as well as the structure of the program. In a URL {\textless}pre{\textgreater}http://www.example.com/path/{\textless}/pre{\textgreater}, for instance, the protocol {\textless}pre{\textgreater}http{\textless}/pre{\textgreater}, the host {\textless}pre{\textgreater}www.example.com{\textless}/pre{\textgreater}, and the path {\textless}pre{\textgreater}path{\textless}/pre{\textgreater} would be handled by different functions and stored in different variables. Given a set of sample inputs, we use dynamic tainting to trace the data flow of each input character, and aggregate those input fragments that would be handled by the same function into lexical and syntactical entities. The result is a context-free grammar that reflects valid input structure. In its evaluation, our AUTOGRAM prototype automatically produced readable and structurally accurate grammars for inputs like URLs, spreadsheets or configuration files. The resulting grammars not only allow simple reverse engineering of input formats, but can also directly serve as input for test generators.},
	urldate = {2016-09-07TZ},
	booktitle = {Proceedings of the 31st {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {ACM},
	author = {Höschele, Matthias and Zeller, Andreas},
	year = {2016},
	note = {00000},
	keywords = {Input formats, context-free grammars, dynamic tainting, fuzzing},
	pages = {720--725}
}

@inproceedings{agosta_automated_2012,
	title = {Automated security analysis of dynamic web applications through symbolic code execution},
	booktitle = {Information {Technology}: {New} {Generations} ({ITNG}), 2012 {Ninth} {International} {Conference} on},
	publisher = {IEEE},
	author = {Agosta, Giovanni and Barenghi, Alessandro and Parata, Antonio and Pelosi, Gerardo},
	year = {2012},
	note = {00018 
bibtex: agosta2012automated},
	pages = {189--194}
}

@misc{noauthor_yahoo_nodate,
	title = {Yahoo {Secure} {Handelbars}},
	url = {https://yahoo.github.io/secure-handlebars/},
	abstract = {Create powerful and efficient JavaScript.},
	urldate = {2016-03-02TZ},
	journal = {Yahoo Secure Handelbars},
	note = {00000},
	keywords = {READ, VP}
}

@inproceedings{weinberger_systematic_2011,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Systematic} {Analysis} of {XSS} {Sanitization} in {Web} {Application} {Frameworks}},
	copyright = {©2011 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-642-23821-5 978-3-642-23822-2},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-23822-2_9},
	abstract = {While most research on XSS defense has focused on techniques for securing existing applications and re-architecting browser mechanisms, sanitization remains the industry-standard defense mechanism. By streamlining and automating XSS sanitization, web application frameworks stand in a good position to stop XSS but have received little research attention. In order to drive research on web frameworks, we systematically study the security of the XSS sanitization abstractions frameworks provide. We develop a novel model of the web browser and characterize the challenges of XSS sanitization. Based on the model, we systematically evaluate the XSS abstractions in 14 major commercially-used web frameworks. We find that frameworks often do not address critical parts of the XSS conundrum. We perform an empirical analysis of 8 large web applications to extract the requirements of sanitization primitives from the perspective of real-world applications. Our study shows that there is a wide gap between the abstractions provided by frameworks and the requirements of applications.},
	language = {en},
	urldate = {2016-03-17TZ},
	booktitle = {Computer {Security} – {ESORICS} 2011},
	publisher = {Springer Berlin Heidelberg},
	author = {Weinberger, Joel and Saxena, Prateek and Akhawe, Devdatta and Finifter, Matthew and Shin, Richard and Song, Dawn},
	editor = {Atluri, Vijay and Diaz, Claudia},
	month = sep,
	year = {2011},
	note = {00077 
DOI: 10.1007/978-3-642-23822-2\_9},
	keywords = {Algorithm Analysis and Problem Complexity, Computer Communication Networks, Computers and Society, Data Encryption, Information Systems Applications (incl. Internet), Management of Computing and Information Systems, READ},
	pages = {150--171}
}

@inproceedings{kiezun_automatic_2009,
	title = {Automatic creation of {SQL} injection and cross-site scripting attacks},
	doi = {10.1109/ICSE.2009.5070521},
	abstract = {We present a technique for finding security vulnerabilities in Web applications. SQL injection (SQLI) and cross-site scripting (XSS) attacks are widespread forms of attack in which the attacker crafts the input to the application to access or modify user data and execute malicious code. In the most serious attacks (called second-order, or persistent, XSS), an attacker can corrupt a database so as to cause subsequent users to execute malicious code. This paper presents an automatic technique for creating inputs that expose SQLI and XSS vulnerabilities. The technique generates sample inputs, symbolically tracks taints through execution (including through database accesses), and mutates the inputs to produce concrete exploits. Ours is the first analysis of which we are aware that precisely addresses second-order XSS attacks. Our technique creates real attack vectors, has few false positives, incurs no runtime overhead for the deployed application, works without requiring modification of application code, and handles dynamic programming-language constructs. We implemented the technique for PHP, in a tool ARDILLA. We evaluated ARDILLA on five PHP applications and found 68 previously unknown vulnerabilities (23 SQLI, 33 first-order XSS, and 12 second-order XSS).},
	booktitle = {Proceedings - {International} {Conference} on {Software} {Engineering}},
	author = {Kiezun, Adam and Guo, Philip J. and Jayaraman, Karthick and Ernst, Michael D.},
	year = {2009},
	note = {00000},
	keywords = {Application software, Concrete, Data security, Databases, FV, HTML, Internet, Monitoring, Runtime, SQL, SQL injection, Testing, cross-site scripting attacks, malicious code, privacy, security of data, security vulnerabilities, web applications},
	pages = {199--209}
}

@article{fonseca_evaluation_2014,
	title = {Evaluation of {Web} {Security} {Mechanisms} {Using} {Vulnerability} amp; {Attack} {Injection}},
	volume = {11},
	issn = {1545-5971},
	doi = {10.1109/TDSC.2013.45},
	abstract = {In this paper we propose a methodology and a prototype tool to evaluate web application security mechanisms. The methodology is based on the idea that injecting realistic vulnerabilities in a web application and attacking them automatically can be used to support the assessment of existing security mechanisms and tools in custom setup scenarios. To provide true to life results, the proposed vulnerability and attack injection methodology relies on the study of a large number of vulnerabilities in real web applications. In addition to the generic methodology, the paper describes the implementation of the Vulnerability \& Attack Injector Tool (VAIT) that allows the automation of the entire process. We used this tool to run a set of experiments that demonstrate the feasibility and the effectiveness of the proposed methodology. The experiments include the evaluation of coverage and false positives of an intrusion detection system for SQL Injection attacks and the assessment of the effectiveness of two top commercial web application vulnerability scanners. Results show that the injection of vulnerabilities and attacks is indeed an effective way to evaluate security mechanisms and to point out not only their weaknesses but also ways for their improvement.},
	number = {5},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	author = {Fonseca, J. and Vieira, M. and Madeira, H.},
	month = sep,
	year = {2014},
	note = {00000},
	keywords = {Databases, Educational institutions, Input variables, Internet, Probes, SQL, SQL Injection attacks, Security, Software, TV, VAIT, Web application security mechanism evaluation, attack injection methodology, fault diagnosis, fault injection, internet applications, intrusion detection system, review and evaluation, security of data, software fault tolerance, vulnerability injection methodology, vulnerability-\&-attack injector tool},
	pages = {440--453}
}

@inproceedings{saxena_symbolic_2010,
	title = {A {Symbolic} {Execution} {Framework} for {JavaScript}},
	doi = {10.1109/SP.2010.38},
	abstract = {As AJAX applications gain popularity, client-side JavaScript code is becoming increasingly complex. However, few automated vulnerability analysis tools for JavaScript exist. In this paper, we describe the first system for exploring the execution space of JavaScript code using symbolic execution. To handle JavaScript code’s complex use of string operations, we design a new language of string constraints and implement a solver for it. We build an automatic end-to-end tool, Kudzu, and apply it to the problem of finding client-side code injection vulnerabilities. In experiments on 18 live web applications, Kudzu automatically discovers 2 previously unknown vulnerabilities and 9 more that were previously found only with a manually-constructed test suite.},
	booktitle = {2010 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Saxena, P. and Akhawe, D. and Hanna, S. and Mao, F. and McCamant, S. and Song, D.},
	month = may,
	year = {2010},
	note = {00251},
	keywords = {Assembly, Computational modeling, Computer architecture, Digital signal processing, Digital signal processing chips, FV, Java, Large scale integration, Logic, Registers, Telecommunication control},
	pages = {513--528}
}

@incollection{mcallister_leveraging_2008,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Leveraging {User} {Interactions} for {In}-{Depth} {Testing} of {Web} {Applications}},
	copyright = {©2008 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-87402-7 978-3-540-87403-4},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-87403-4_11},
	abstract = {Over the last years, the complexity of web applications has grown significantly, challenging desktop programs in terms of functionality and design. Along with the rising popularity of web applications, the number of exploitable bugs has also increased significantly. Web application flaws, such as cross-site scripting or SQL injection bugs, now account for more than two thirds of the reported security vulnerabilities. Black-box testing techniques are a common approach to improve software quality and detect bugs before deployment. There exist a number of vulnerability scanners, or fuzzers, that expose web applications to a barrage of malformed inputs in the hope to identify input validation errors. Unfortunately, these scanners often fail to test a substantial fraction of a web application’s logic, especially when this logic is invoked from pages that can only be reached after filling out complex forms that aggressively check the correctness of the provided values. In this paper, we present an automated testing tool that can find reflected and stored cross-site scripting (XSS) vulnerabilities in web applications. The core of our system is a black-box vulnerability scanner. This scanner is enhanced by techniques that allow one to generate more comprehensive test cases and explore a larger fraction of the application. Our experiments demonstrate that our approach is able to test more thoroughly these programs and identify more bugs than a number of open-source and commercial web vulnerability scanners.},
	language = {en},
	number = {5230},
	urldate = {2016-03-13TZ},
	booktitle = {Recent {Advances} in {Intrusion} {Detection}},
	publisher = {Springer Berlin Heidelberg},
	author = {McAllister, Sean and Kirda, Engin and Kruegel, Christopher},
	editor = {Lippmann, Richard and Kirda, Engin and Trachtenberg, Ari},
	month = sep,
	year = {2008},
	note = {00056 
DOI: 10.1007/978-3-540-87403-4\_11},
	keywords = {Computer Communication Networks, Computers and Society, Data Encryption, FV, Management of Computing and Information Systems, Systems and Data Security},
	pages = {191--210}
}

@inproceedings{minamide_static_2005,
	address = {New York, NY, USA},
	series = {{WWW} '05},
	title = {Static {Approximation} of {Dynamically} {Generated} {Web} {Pages}},
	isbn = {978-1-59593-046-0},
	url = {http://doi.acm.org/10.1145/1060745.1060809},
	doi = {10.1145/1060745.1060809},
	abstract = {Server-side programming is one of the key technologies that support today's WWW environment. It makes it possible to generate Web pages dynamically according to a user's request and to customize pages for each user. However, the flexibility obtained by server-side programming makes it much harder to guarantee validity and security of dynamically generated pages.To check statically the properties of Web pages generated dynamically by a server-side program, we develop a static program analysis that approximates the string output of a program with a context-free grammar. The approximation obtained by the analyzer can be used to check various properties of a server-side program and the pages it generates.To demonstrate the effectiveness of the analysis, we have implemented a string analyzer for the server-side scripting language PHP. The analyzer is successfully applied to publicly available PHP programs to detect cross-site scripting vulnerabilities and to validate pages they generate dynamically.},
	urldate = {2016-03-13TZ},
	booktitle = {Proceedings of the 14th {International} {Conference} on {World} {Wide} {Web}},
	publisher = {ACM},
	author = {Minamide, Yasuhiko},
	year = {2005},
	note = {00252},
	keywords = {FV, HTML validation, Static analysis, context-free grammars, cross-site scripting, server-side scripting},
	pages = {432--441}
}

@inproceedings{bau_state_2010,
	title = {State of the {Art}: {Automated} {Black}-{Box} {Web} {Application} {Vulnerability} {Testing}},
	shorttitle = {State of the {Art}},
	doi = {10.1109/SP.2010.27},
	abstract = {Black-box web application vulnerability scanners are automated tools that probe web applications for security vulnerabilities. In order to assess the current state of the art, we obtained access to eight leading tools and carried out a study of: (i) the class of vulnerabilities tested by these scanners, (ii) their effectiveness against target vulnerabilities, and (iii) the relevance of the target vulnerabilities to vulnerabilities found in the wild. To conduct our study we used a custom web application vulnerable to known and projected vulnerabilities, and previous versions of widely used web applications containing known vulnerabilities. Our results show the promise and effectiveness of automated tools, as a group, and also some limitations. In particular, "stored" forms of Cross Site Scripting (XSS) and SQL Injection (SQLI) vulnerabilities are not currently found by many tools. Because our goal is to assess the potential of future research, not to evaluate specific vendors, we do not report comparative data or make any recommendations about purchase of specific tools.},
	booktitle = {2010 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Bau, J. and Bursztein, E. and Gupta, D. and Mitchell, J.},
	month = may,
	year = {2010},
	note = {00173},
	keywords = {Automatic testing, Code standards, Computer hacking, Credit cards, Data security, Decision support systems, Forgery, Personnel, Probes, privacy},
	pages = {332--345}
}

@misc{liu_efficient_2015,
	title = {Efficient {Context}-aware {Output} {Escaping} for {JavaScript} {Template} {Engines}},
	url = {http://sched.co/3U34},
	author = {Liu, Nera and Fung, Adonis and Yu, Albert},
	year = {2015},
	note = {00000}
}

@inproceedings{kiezun_hampi:_2009,
	title = {{HAMPI}: a solver for string constraints},
	url = {http://portal.acm.org/citation.cfm?id=1572272.1572286},
	doi = {10.1145/1572272.1572286},
	abstract = {Many automatic testing, analysis, and verification techniques for programs can be effectively reduced to a constraint generation phase followed by a constraint-solving phase. This separation of concerns often leads to more effective and maintainable tools. The increasing efficiency of off-the-shelf constraint solvers makes this approach even more compelling. However, there are few effective and sufficiently expressive off-the-shelf solvers for string constraints generated by analysis techniques for string-manipulating programs.},
	booktitle = {Proceedings of the eighteenth international symposium on {Software} testing and analysis},
	author = {Kiezun, Adam and Ganesh, Vijay and Guo, Philip J and Hooimeijer, Pieter and Ernst, Michael D},
	year = {2009},
	note = {00196},
	keywords = {FV, context-free lan-, regular languages, string constraints},
	pages = {105--116}
}

@article{abdelnur_kif_2007,
	title = {{KiF} : {A} stateful {SIP} {Fuzzer} {To} cite this version : {KiF} : {A} stateful {SIP} {Fuzzer}},
	author = {Abdelnur, Humberto and Festor, Olivier and State, Radu and Abdelnur, Humberto and Festor, Olivier and State, Radu and Kif, A and Fuzzer, S I P and Abdelnur, Humberto J and Lorraine, Loria Inria and Lorraine, Loria Inria and Festor, Olivier and Lorraine, Loria Inria},
	year = {2007},
	note = {00000},
	keywords = {Protocol Fuzzer, Software Testing Techniques, VoIP, rity, sip vulnerabilities, voip secu-}
}

@inproceedings{mohammadi_poster_2015,
	address = {Denver,Colorado},
	title = {{POSTER} : {Using} {Unit} {Testing} to {Detect} {Sanitization} {Flaws}},
	isbn = {978-1-4503-3832-5},
	doi = {http://dx.doi.org/10.1145/2810103.2810130},
	booktitle = {{CCS}'15: {The} 22nd {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	author = {Mohammadi, Mahmoud and Chu, Bill and Lipford, Heather Richter},
	year = {2015},
	note = {00000},
	keywords = {cross-site scripting, grammar-based attack generation, program analysis, sanitization correctness, unit testing, xss}
}

@article{razzaq_sciencedirect_2014,
	title = {{ScienceDirect} {Ontology} for attack detection : {An} intelligent approach to web application security},
	volume = {45},
	issn = {0167-4048},
	url = {http://dx.doi.org/10.1016/j.cose.2014.05.005},
	doi = {10.1016/j.cose.2014.05.005},
	journal = {Computers \& Security},
	author = {Razzaq, Abdul and Anwar, Zahid and Ahmad, H Farooq and Latif, Khalid},
	year = {2014},
	note = {00000},
	keywords = {ontology based intelligent system, web application security},
	pages = {124--146}
}

@inproceedings{gupta_static_2014,
	title = {Static analysis approaches to detect {SQL} injection and cross site scripting vulnerabilities in web applications: {A} survey},
	isbn = {978-1-4799-4040-0},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909173},
	doi = {10.1109/ICRAIE.2014.6909173},
	booktitle = {International {Conference} on {Recent} {Advances} and {Innovations} in {Engineering} ({ICRAIE}-2014)},
	author = {Gupta, Mukesh Kumar and Govil, M.C. and Singh, Girdhari},
	year = {2014},
	note = {00001},
	keywords = {READ},
	pages = {1--5}
}

@article{dukes_case_2013,
	title = {A case study on web application security testing with tools and manual testing},
	issn = {07347502},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6567420},
	doi = {10.1109/SECON.2013.6567420},
	journal = {2013 Proceedings of IEEE Southeastcon},
	author = {Dukes, LaShanda and Yuan, Xiaohong and Akowuah, Francis},
	year = {2013},
	note = {00010},
	pages = {1--6}
}

@article{wilander_modeling_2005,
	title = {Modeling and visualizing security properties of code using dependence graphs},
	author = {Wilander, J.},
	year = {2005},
	note = {00010},
	pages = {65--74}
}

@inproceedings{buchler_spacite_2012,
	title = {{SPaCiTE} - {Web} application testing engine},
	isbn = {978-0-7695-4670-4},
	doi = {10.1109/ICST.2012.187},
	abstract = {Web applications and web services enjoy an ever-increasing popularity. Such applications have to face a variety of sophisticated and subtle attacks. The difficulty of identifying respective vulnerabilities steadily increases with the complexity of applications. Moreover, the art of penetration testing predominantly depends on the skills of highly trained test experts. The difficulty to test web applications hence represents a daunting challenge to their developers. As a step towards improving security analyses, model checking has, at the model level, been found capable of identifying complex attacks and thus moving security analyses towards a push-button technology. In order to bridge the gap with actual systems, we present Spa Cite. This tool relies on a dedicated model-checker for security analyses that generates potential attacks with regard to common vulnerabilities in web applications. Then, it semi-automatically runs those attacks on the System Under Validation (SUV) and reports which vulnerabilities were successfully exploited. We applied Spa Cite to Role-Based-Access-Control (RBAC) and Cross-Site Scripting (XSS) lessons of Web Goat, an insecure web application maintained by OWASP. The tool successfully reproduced RBAC and XSS attacks.},
	booktitle = {Proceedings - {IEEE} 5th {International} {Conference} on {Software} {Testing}, {Verification} and {Validation}, {ICST} 2012},
	author = {Büchler, Matthias and Oudinet, Johan and Pretschner, Alexander},
	year = {2012},
	note = {00011},
	keywords = {WebGoat, bridging abstraction gaps, fault-injection, model-checking, mutation testing, security testing, web application},
	pages = {858--859}
}

@inproceedings{avancini_security_2011,
	title = {Security testing of web applications: {A} search-based approach for cross-site scripting vulnerabilities},
	isbn = {978-0-7695-4347-5},
	doi = {10.1109/SCAM.2011.7},
	abstract = {More and more web applications suffer the presence of cross-site scripting vulnerabilities that could be exploited by attackers to access sensitive information (such as credentials or credit card numbers). Hence proper tests are required to assess the security of web applications. In this paper, we resort to a search based approach for security testing web applications. We take advantage of static analysis to detect candidate cross-site scripting vulnerabilities. Input values that expose these vulnerabilities are searched by a genetic algorithm and, to help the genetic algorithm escape local optima, symbolic constraints are collected at run-time and passed to a solver. Search results represent test cases to be used by software developers to understand and fix security problems. We implemented this approach in a prototype and evaluated it on real world PHP code.},
	booktitle = {Proceedings - 11th {IEEE} {International} {Working} {Conference} on {Source} {Code} {Analysis} and {Manipulation}, {SCAM} 2011},
	author = {Avancini, Andrea and Ceccato, Mariano},
	year = {2011},
	note = {00014},
	pages = {85--94}
}

@inproceedings{bozic_xss_2013,
	title = {{XSS} pattern for attack modeling in testing},
	isbn = {978-1-4673-6161-3},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6595794},
	doi = {10.1109/IWAST.2013.6595794},
	booktitle = {2013 8th {International} {Workshop} on {Automation} of {Software} {Test} ({AST})},
	publisher = {IEEE},
	author = {Bozic, Josip and Wotawa, Franz},
	month = may,
	year = {2013},
	note = {00014},
	keywords = {Attack pattern model, FV, cross-site scripting, model-based testing, security testing},
	pages = {71--74}
}

@article{armando_model-checking_2010,
	title = {Model-checking driven security testing of web-based applications},
	doi = {10.1109/ICSTW.2010.54},
	abstract = {Model checking and security testing are two verification techniques available to help finding flaws in security-sensitive, distributed applications. In this paper, we present an approach to security testing of web-based applications in which test cases are automatically derived from counterexamples found through model checking. We illustrate our approach by discussing its application against of the SAML-based Single Sign-On for Google Apps.},
	journal = {ICSTW 2010 - 3rd International Conference on Software Testing, Verification, and Validation Workshops},
	author = {Armando, Alessandro and Carbone, Roberto and Compagna, Luca and Li, Keqin and Pellegrino, Giancarlo},
	year = {2010},
	note = {00017},
	keywords = {DN, Model checking, READ, Web-based applications, security testing},
	pages = {361--370}
}

@article{razzaq_semantic_2014,
	title = {Semantic security against web application attacks},
	volume = {254},
	issn = {00200255},
	url = {http://dx.doi.org/10.1016/j.ins.2013.08.007},
	doi = {10.1016/j.ins.2013.08.007},
	abstract = {In this paper, we propose a method of detecting and classifying web application attacks. In contrast to current signature-based security methods, our solution is an ontology based technique. It specifies web application attacks by using semantic rules, the context of consequence and the specifications of application protocols. The system is capable of detecting sophisticated attacks effectively and efficiently by analyzing the specified portion of a user request where attacks are possible. Semantic rules help to capture the context of the application, possible attacks and the protocol that was used. These rules also allow inference to run over the ontological models in order to detect, the often complex polymorphic variations of web application attacks. The ontological model was developed using Description Logic that was based on the Web Ontology Language (OWL). The inference rules are Horn Logic statements and are implemented using the Apache JENA framework. The system is therefore platform and technology independent. Prior to the evaluation of the system the knowledge model was validated by using OntoClean to remove inconsistency, incompleteness and redundancy in the specification of ontological concepts. The experimental results show that the detection capability and performance of our system is significantly better than existing state of the art solutions. The system successfully detects web application attacks whilst generating few false positives. The examples that are presented demonstrate that a semantic approach can be used to effectively detect zero day and more sophisticated attacks in a real-world environment. ?? 2013 Elsevier Inc. All rights reserved.},
	journal = {Information Sciences},
	author = {Razzaq, Abdul and Latif, Khalid and Farooq Ahmad, H. and Hur, Ali and Anwar, Zahid and Bloodsworth, Peter Charles},
	year = {2014},
	note = {00019},
	keywords = {FV, Semantic rule engine, Semantic security, application security},
	pages = {19--38}
}

@article{dai_configuration_2010,
	title = {Configuration fuzzing for software vulnerability detection},
	issn = {1947-3036},
	doi = {10.1109/ARES.2010.22},
	abstract = {Many software security vulnerabilities only reveal themselves under certain conditions, i.e., particular configurations of the software together with its particular run-time environment. One approach to detecting these vulnerabilities is fuzz testing, which feeds a range of randomly modified inputs to a software application while monitoring it for failures. However, typical fuzz testing makes no guarantees regarding the syntactic and semantic validity of the input, or of how much of the input space will be explored. To address these problems, in this paper we present a new testing methodology called configuration fuzzing. Configuration fuzzing is a technique whereby the configuration of the running application is randomly modified at certain execution points, in order to check for vulnerabilities that only arise in certain conditions. As the application runs in the deployment environment, this testing technique continuously fuzzes the configuration and checks "security invariants" that, if violated, indicate a vulnerability; however, the fuzzing is performed in a duplicated copy of the original process, so that it does not affect the state of the running application. In addition to discussing the approach and describing a prototype framework for implementation, we also present the results of a case study to demonstrate the approach's efficiency.},
	journal = {ARES 2010 - 5th International Conference on Availability, Reliability, and Security},
	author = {Dai, Huning and Murphy, Christian and Kaiser, Gail},
	year = {2010},
	pmid = {21037923},
	note = {00019 },
	keywords = {Configuration fuzzing, Fuzz testing, In vivo testing, Security invariants, Vulnerability},
	pages = {525--530}
}

@inproceedings{athanasopoulos_xjs:_2010,
	address = {Berkeley, CA, USA},
	series = {{WebApps}'10},
	title = {{xJS}: {Practical} {XSS} {Prevention} for {Web} {Application} {Development}},
	url = {http://dl.acm.org/citation.cfm?id=1863166.1863179},
	booktitle = {Proceedings of the 2010 {USENIX} {Conference} on {Web} {Application} {Development}},
	publisher = {USENIX Association},
	author = {Athanasopoulos, Elias and Pappas, Vasilis and Krithinakis, Antonis and Ligouras, Spyros and Markatos, Evangelos P and Karagiannis, Thomas},
	year = {2010},
	note = {00020},
	keywords = {FV},
	pages = {13}
}

@inproceedings{wurzinger_swap:_2009,
	title = {{SWAP}: {Mitigating} {XSS} attacks using a reverse proxy},
	isbn = {978-1-4244-3725-2},
	doi = {10.1109/IWSESS.2009.5068456},
	abstract = {Due to the increasing amount of Web sites offering features to contribute rich content, and the frequent failure of Web developers to properly sanitize user input, cross-site scripting prevails as the most significant security threat to Web applications. Using cross-site scripting techniques, miscreants can hijack Web sessions, and craft credible phishing sites. Previous work towards protecting against cross-site scripting attacks suffers from various drawbacks, such as practical infeasibility of deployment due to the need for client-side modifications, inability to reliably detect all injected scripts, and complex, error-prone parameterization. In this paper, we introduce SWAP (secure Web application proxy), a server-side solution for detecting and preventing cross-site scripting attacks. SWAP comprises a reverse proxy that intercepts all HTML responses, as well as a modified Web browser which is utilized to detect script content. SWAP can be deployed transparently for the client, and requires only a simple automated transformation of the original Web application. Using SWAP, we were able to correctly detect exploits on several authentic vulnerabilities in popular Web applications.},
	booktitle = {Proceedings of the 2009 {ICSE} {Workshop} on {Software} {Engineering} for {Secure} {Systems}, {SESS} 2009},
	author = {Wurzinger, Peter and Platzer, Christian and Ludl, Christian and Kirda, Engin and Kruegel, Christopher},
	year = {2009},
	note = {00067},
	keywords = {AP, FV, [Electronic Manuscript]},
	pages = {33--39}
}

@inproceedings{saxena_flax_2010,
	title = {{FLAX} : {Systematic} {Discovery} of {Client}-side {Validation} {Vulnerabilities} in {Rich} {Web} {Applications}},
	abstract = {The complexity of the client-side components of web applications has exploded with the increase in popularity of web 2.0 applications. Today, traditional desktop ap- plications, such as document viewers, presentation tools and chat applications are commonly available as online JavaScript applications. Previous research on web vulnerabilities has primarily concentrated on flaws in the server-side components of web applications. This paper highlights a new class of vulnera- bilities, which we term client-side validation (or CSV) vul- nerabilities. CSV vulnerabilities arise from unsafe usage of untrusted data in the client-side code of the web applica- tion that is typically written in JavaScript. In this paper, we demonstrate that they can result in a broad spectrum of attacks. Our work provides empirical evidence that CSV vulnerabilities are not merely conceptual but are prevalent in todays web applications. We propose dynamic analysis techniques to systemati- cally discover vulnerabilities of this class. The techniques are light-weight, efficient, and have no false positives. We implement our techniques in a prototype tool called FLAX, which scales to real-world applications and has discovered 11 vulnerabilities in the wild so far.},
	booktitle = {17th {Annual} {Network} {Distributed} {System} {Security} {SymposiumNDSS}},
	author = {Saxena, Prateek and Hanna, Steve},
	year = {2010},
	note = {00078},
	keywords = {READ, ST},
	pages = {17}
}

@article{mesbah_invariant-based_2012,
	title = {Invariant-based automatic testing of modern web applications},
	volume = {38},
	issn = {00985589},
	doi = {10.1109/TSE.2011.28},
	abstract = {Ajax-based Web 2.0 applications rely on stateful asynchronous client/server communication, and client-side runtime manipulation of the DOM tree. This not only makes them fundamentally different from traditional web applications, but also more error-prone and harder to test. We propose a method for testing Ajax applications automatically, based on a crawler to infer a state-flow graph for all (client-side) user interface states. We identify Ajax-specific faults that can occur in such states (related to, e.g., DOM validity, error messages, discoverability, back-button compatibility) as well as DOM-tree invariants that can serve as oracles to detect such faults. Our approach, called Atusa, is implemented in a tool offering generic invariant checking components, a plugin-mechanism to add application-specific state validators, and generation of a test suite covering the paths obtained during crawling. We describe three case studies, consisting of six subjects, evaluating the type of invariants that can be obtained for Ajax applications as well as the fault revealing capabilities, scalability, required manual effort, and level of automation of our testing approach.},
	number = {1},
	journal = {IEEE Transactions on Software Engineering},
	author = {Mesbah, Ali and Van Deursen, Arie and Roest, Danny},
	year = {2012},
	note = {00092},
	keywords = {Automated testing, ajax, web applications},
	pages = {35--53}
}

@article{kuhn_combinatorial_2000,
	title = {Combinatorial testing},
	url = {http://zernike.eldoc.ub.rug.nl/FILES/root/2000/AngewChemStork/2000AngewChemIntEdStork.pdf},
	doi = {10.4018/978-1-60566-731-7.ch014},
	number = {January},
	journal = {Angewandte},
	author = {Kuhn, Rick},
	year = {2000},
	note = {00110},
	pages = {1--13}
}

@article{duchene_kameleonfuzz_nodate,
	title = {{KameleonFuzz} : {Evolutionary} {Fuzzing} for {Black}-{Box} {XSS} {Detection}},
	volume = {1},
	doi = {http://dx.doi.org/10.1145/2557547.2557550},
	journal = {Codaspy 2014},
	author = {Duchene, Fabien and Richier, Jean-luc and Groz, Roland},
	note = {00007},
	keywords = {FV, Fuzzing, black-, box security testing, cross-site scripting, evolutionary algorithm, model inference, taint inference}
}
@article{li_application_2013,
	title = {The application of fuzzing in web software security vulnerabilities test},
	doi = {10.1109/ITA.2013.36},
	abstract = {Web applications need for extensive testing before deployment and use, for early detecting security vulnerabilities to improve the quality of the safety of the software, the purpose of this paper is to research the fuzzing applications in security vulnerabilities. This article first introduces the common Web software security vulnerabilities, and then provide a comprehensive overview of the fuzzing technology, and using fuzzing tools Web fuzz to execute a software vulnerability testing, test whether there is a software security hole. Test results prove that fuzzing is suitable for software security vulnerabilities testing, but this methodology applies only to security research field, and in the aspect of software security vulnerabilities detection is still insufficient.},
	journal = {Proceedings - 2013 International Conference on Information Technology and Applications, ITA 2013},
	author = {Li, Li and Dong, Qiu and Liu, Dan and Zhu, Leilei},
	year = {2013},
	note = {00001},
	keywords = {Fuzzing, Web software security vulnerabilities, Webfuzz},
	pages = {130--133}
}

@article{zhang_simfuzz:_2012,
	title = {{SimFuzz}: {Test} case similarity directed deep fuzzing},
	volume = {85},
	issn = {01641212},
	url = {http://dx.doi.org/10.1016/j.jss.2011.07.028},
	doi = {10.1016/j.jss.2011.07.028},
	abstract = {Fuzzing is widely used to detect software vulnerabilities. Blackbox fuzzing does not require program source code. It mutates well-formed inputs to produce new ones. However, these new inputs usually do not exercise deep program semantics since the possibility that they can satisfy the conditions of a deep program state is low. As a result, blackbox fuzzing is often limited to identify vulnerabilities in input validation components of a program. Domain knowledge such as input specifications can be used to mitigate these limitations. However, it is often expensive to obtain such knowledge in practice. Whitebox fuzzing employs heavy analysis techniques, i.e.; dynamic symbolic execution, to systematically generate test inputs and explore as many paths as possible. It is powerful to explore new program branches so as to identify more vulnerabilities. However, it has fundamental challenges such as unsolvable constraints and is difficult to scale to large programs due to path explosion. This paper proposes a novel fuzzing approach that aims to produce test inputs to explore deep program semantics effectively and efficiently. The fuzzing process comprises two stages. At the first stage, a traditional blackbox fuzzing approach is applied for test data generation. This process is guided by a novel test case similarity metric. At the second stage, a subset of the test inputs generated at the first stage is selected based on the test case similarity metric. Then, combination testing is applied on these selected test inputs to further generate new inputs. As a result, less redundant test inputs, i.e.; inputs that just explore shallow program paths, are created at the first stage, and more distinct test inputs, i.e.; inputs that explore deep program paths, are produced at the second stage. A prototype tool SimFuzz is developed and evaluated on real programs, and the experimental results are promising. ?? 2011 Elsevier Inc.},
	number = {1},
	journal = {Journal of Systems and Software},
	author = {Zhang, Dazhi and Liu, Donggang and Lei, Yu and Kung, David and Csallner, Christoph and Nystrom, Nathaniel and Wang, Wenhua},
	year = {2012},
	note = {00005},
	keywords = {Fuzzing, Software testing, Software vulnerability},
	pages = {102--111}
}

@inproceedings{garn_evaluation_2015,
	title = {Evaluation of the {IPO}-{Family} {Algorithms} for {Test} {Case} {Generation} in {Web} {Security} {Testing}},
	isbn = {978-1-4799-1885-0},
	author = {Garn, Bernhard},
	year = {2015},
	note = {00002},
	keywords = {Combinatorial testing, FV, constraints, ipo-family al-}
}

@inproceedings{aydin_automated_2014,
	title = {Automated test generation from vulnerability signatures},
	isbn = {978-0-7695-5185-2},
	doi = {10.1109/ICST.2014.32},
	booktitle = {Proceedings - {IEEE} 7th {International} {Conference} on {Software} {Testing}, {Verification} and {Validation}, {ICST} 2014},
	author = {Aydin, Abdulbaki and Alkhalaf, Muath and Bultan, Tevfik},
	year = {2014},
	note = {00004},
	keywords = {FV, automata-based test generation, string analysis, validation and sanitization, vulnerability signatures},
	pages = {193--202}
}

@inproceedings{huang_craxweb:_2013,
	title = {{CRAXweb}: {Automatic} web application testing and attack generation},
	isbn = {978-1-4799-0406-8},
	doi = {10.1109/SERE.2013.26},
	abstract = {This paper proposes to test web applications and generate the feasible exploits automatically, including cross-site scripting and SQL injection attacks. We test the web applications with initial random inputs by detecting symbolic queries to SQL servers or symbolic responses to HTTP servers. After symbolic outputs detected, we are able to generate attack strings and reproduce the results, emulating the manual attack behavior. In contrast with other traditional detection and prevention methods, we can determine the presence of vulnerabilities and prove the feasibility of attacks. This automatic generation process is based on a dynamic software testing method-symbolic execution by S2E. We have applied this automatic process to several known vulnerabilities on large-scale open source web applications, and generated the attack strings successfully. Our method is web platform independent, covering PHP, JSP, Rails, and Django due to the supports of the whole system environment of S2E.},
	booktitle = {Proceedings - 7th {International} {Conference} on {Software} {Security} and {Reliability}, {SERE} 2013},
	author = {Huang, Shih Kun and Lu, Han Lin and Leong, Wai Meng and Liu, Huan},
	year = {2013},
	note = {00003},
	keywords = {FV, Web security, automatic exploit generation, symbolic execution},
	pages = {208--217}
}

@article{brook_efficient_2007,
	title = {An {Efficient} {Black}-box {Technique} for {Defeating} {Web} {Application} {Attacks}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.153.1791&amp;rep=rep1&amp;type=pdf},
	abstract = {Over the past few years, injection vulnerabilities have become the primary target for remote exploits. SQL in- jection, command injection, and cross-site scripting are some of the popular attacks that exploit these vulnerabili- ties. Taint-tracking has emerged as one of the most promis- ing approaches for defending against these exploits, as it supports accurate detection (and prevention) of popular in- jection attacks. However, practical deployment of taint- tracking defenses has been hampered by a number of fac- tors, including: (a) high performance overheads (often over 100\%), (b) the need for deep instrumentation, which has the potential to impact application robustness and stabil- ity, and (c) specificity to the language in which an appli- cation is written. In order to overcome these limitations, we present a new technique in this paper called taint infer- ence. This technique does not require any source-code or binary instrumentation of the application to be protected; instead, it operates by intercepting requests and responses from this application. For most web applications, this inter- ception may be achieved using network layer interposition or library interposition. We then develop a class of policies called syntax- and taint-aware policies that can accurately detect and/or block most injection attacks. An experimental evaluation shows that our techniques are effective in detect- ing a broad range of attacks on applications written in mul- tiple languages (including PHP, Java and C), and impose low performance overheads (below 5\%).},
	number = {January 2006},
	journal = {Defense},
	author = {Brook, Stony},
	year = {2007},
	note = {00000},
	keywords = {FV}
}

@article{eassa_integrated_2014,
	title = {An integrated multi-agent testing tool for security checking of agent-based web applications},
	volume = {13},
	issn = {22242872},
	doi = {10.13189/wjcat.2013.010201},
	journal = {WSEAS Transactions on Computers},
	author = {Eassa, Fathy E. and Zaki, M. and Eassa, Ahmed M. and Aljehani, Tahani},
	year = {2014},
	note = {00000},
	keywords = {Assertion languages, Dynamic testing, Static testing, Temporal logic, Web applications security testing},
	pages = {9--19}
}

@article{hunting_fuzzer_2014,
	title = {Fuzzer : {Advances} in {Black}-{Box} {Dank} {U} ! {PhD} in “ {Black}-{Box} {Vulnerability}},
	author = {Hunting, Vulnerability},
	year = {2014},
	note = {00000},
	keywords = {FV}
}

@article{kiezun_computer_2008,
	title = {Computer {Science} and {Artificial} {Intelligence} {Laboratory} {Technical} {Report} {Automatic} {Creation} of {SQL} {Injection} and {Cross}-{Site} {Scripting} {Attacks}},
	author = {Kiezun, Adam and Guo, Philip J and Jayaraman, Karthick and Ernst, Michael D and Kie, Adam and Guo, Philip J and Ernst, Michael D},
	year = {2008},
	note = {00000}
}

@article{kopietz_dynamic_1989,
	title = {Dynamic {Test} {Generations} for {Large} {Binary} {Programs}},
	url = {http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:No+Title#3},
	abstract = {This thesis develops new methods for addressing the problem of software security bugs, shows that these methods scale to large commodity software, and lays the foundation for a service that makes automatic, effective security testing available at a modest cost per bug found. We make the following contributions: • We introduce a new search algorithm for systematic test generation that is optimized for large applications with large input files and exhibiting long execution traces where the search is bound to be incomplete (Chapter 2); • We introduce optimizations for checking multiple properties of a program simultaneously using dynamic test generation, and we formalize the notion of active property checking (Chapter 3); • We describe the implementation of tools that implement dynamic test generation of large binary programs for Windows and for Linux: SAGE and SmartFuzz. We explain the engineering choices behind their symbolic execution algorithm and the key optimization techniques enabling both tools to scale to program traces with hundreds of millions of instructions (Chapters 2 and 4); • We develop methods for coordinating large scale experiments with fuzz testing techniques, including methods to address the defect triage problem (Chapter 4);},
	journal = {Europhys. Lett},
	author = {Kopietz, P and Scharf, P and Skaf, Ms and Chakravarty, S},
	year = {1989},
	note = {00000}
}

@book{owasp.org_owasp_nodate,
	title = {{OWASP} {XSS} {Filter} {Evasion} {Cheat} {Sheet}},
	url = {https://www.owasp.org/index.php/XSS_Filter_Evasion_Cheat_Sheet},
	urldate = {2015-05-07},
	author = {{Owasp.org}},
	note = {00000},
	keywords = {attack patterns, xss}
}

@article{younan_runtime_2012,
	title = {Runtime {Countermeasures} for {Code} {Injection} {Attacks} {Against} {C} and {C}++ {Programs}},
	volume = {44},
	issn = {0360-0300},
	url = {http://doi.acm.org/10.1145/2187671.2187679},
	doi = {10.1145/2187671.2187679},
	abstract = {The lack of memory safety in C/C++ often leads to vulnerabilities. Code injection attacks exploit these vulnerabilities to gain control over the execution flow of applications. These attacks have played a key role in many major security incidents. Consequently, a huge body of research on countermeasures exists. We provide a comprehensive and structured survey of vulnerabilities and countermeasures that operate at runtime. These countermeasures make different trade-offs in terms of performance, effectivity, compatibility, etc., making it hard to evaluate and compare countermeasures in a given context. We define a classification and evaluation framework on the basis of which countermeasures can be assessed.},
	number = {3},
	urldate = {2016-03-07TZ},
	journal = {ACM Comput. Surv.},
	author = {Younan, Yves and Joosen, Wouter and Piessens, Frank},
	month = jun,
	year = {2012},
	note = {00027},
	keywords = {C, C++, Code injection, countermeasures},
	pages = {17:1--17:28}
}

@article{woodcock_formal_2009,
	title = {Formal {Methods}: {Practice} and {Experience}},
	volume = {41},
	issn = {0360-0300},
	shorttitle = {Formal {Methods}},
	url = {http://doi.acm.org/10.1145/1592434.1592436},
	doi = {10.1145/1592434.1592436},
	abstract = {Formal methods use mathematical models for analysis and verification at any part of the program life-cycle. We describe the state of the art in the industrial use of formal methods, concentrating on their increasing use at the earlier stages of specification and design. We do this by reporting on a new survey of industrial use, comparing the situation in 2009 with the most significant surveys carried out over the last 20 years. We describe some of the highlights of our survey by presenting a series of industrial projects, and we draw some observations from these surveys and records of experience. Based on this, we discuss the issues surrounding the industrial adoption of formal methods. Finally, we look to the future and describe the development of a Verified Software Repository, part of the worldwide Verified Software Initiative. We introduce the initial projects being used to populate the repository, and describe the challenges they address.},
	number = {4},
	urldate = {2016-03-07TZ},
	journal = {ACM Comput. Surv.},
	author = {Woodcock, Jim and Larsen, Peter Gorm and Bicarregui, Juan and Fitzgerald, John},
	month = oct,
	year = {2009},
	note = {00424},
	keywords = {Experimental software engineering, formal methods surveys, grand challenges, verified software initiative, verified software repository},
	pages = {19:1--19:36}
}

@article{chang_analyzing_2013,
	title = {Analyzing and {Defending} {Against} {Web}-based {Malware}},
	volume = {45},
	issn = {0360-0300},
	url = {http://doi.acm.org/10.1145/2501654.2501663},
	doi = {10.1145/2501654.2501663},
	abstract = {Web-based malware is a growing threat to today's Internet security. Attacks of this type are prevalent and lead to serious security consequences. Millions of malicious URLs are used as distribution channels to propagate malware all over the Web. After being infected, victim systems fall in the control of attackers, who can utilize them for various cyber crimes such as stealing credentials, spamming, and distributed denial-of-service attacks. Moreover, it has been observed that traditional security technologies such as firewalls and intrusion detection systems have only limited capability to mitigate this new problem. In this article, we survey the state-of-the-art research regarding the analysis of—and defense against—Web-based malware attacks. First, we study the attack model, the root cause, and the vulnerabilities that enable these attacks. Second, we analyze the status quo of the Web-based malware problem. Third, three categories of defense mechanisms are discussed in detail: (1) building honeypots with virtual machines or signature-based detection system to discover existing threats; (2) using code analysis and testing techniques to identify the vulnerabilities of Web applications; and (3) constructing reputation-based blacklists or smart sandbox systems to protect end-users from attacks. We show that these three categories of approaches form an extensive solution space to the Web-based malware problem. Finally, we compare the surveyed approaches and discuss possible future research directions.},
	number = {4},
	urldate = {2016-03-07TZ},
	journal = {ACM Comput. Surv.},
	author = {Chang, Jian and Venkatasubramanian, Krishna K. and West, Andrew G. and Lee, Insup},
	month = aug,
	year = {2013},
	note = {00016},
	keywords = {Web-based, malware},
	pages = {49:1--49:35}
}

@article{li_survey_2014,
	title = {A {Survey} on {Server}-side {Approaches} to {Securing} {Web} {Applications}},
	volume = {46},
	issn = {0360-0300},
	url = {http://doi.acm.org/10.1145/2541315},
	doi = {10.1145/2541315},
	abstract = {Web applications are one of the most prevalent platforms for information and service delivery over the Internet today. As they are increasingly used for critical services, web applications have become a popular and valuable target for security attacks. Although a large body of techniques have been developed to fortify web applications and mitigate attacks launched against them, there has been little effort devoted to drawing connections among these techniques and building the big picture of web application security research. This article surveys the area of securing web applications from the server side, with the aim of systematizing the existing techniques into a big picture that promotes future research. We first present the unique aspects of the web application development that cause inherent challenges in building secure web applications. We then discuss three commonly seen security vulnerabilities within web applications: input validation vulnerabilities, session management vulnerabilities, and application logic vulnerabilities, along with attacks that exploit these vulnerabilities. We organize the existing techniques along two dimensions: (1) the security vulnerabilities and attacks that they address and (2) the design objective and the phases of a web application during which they can be carried out. These phases are secure construction of new web applications, security analysis/testing of legacy web applications, and runtime protection of legacy web applications. Finally, we summarize the lessons learned and discuss future research opportunities in this area.},
	number = {4},
	urldate = {2016-03-07TZ},
	journal = {ACM Comput. Surv.},
	author = {Li, Xiaowei and Xue, Yuan},
	month = mar,
	year = {2014},
	note = {00018},
	keywords = {FV, Web application security, application logic vulnerability, input validation vulnerability, session management vulnerability},
	pages = {54:1--54:29}
}

@article{cytron_efficiently_1991,
	title = {Efficiently {Computing} {Static} {Single} {Assignment} {Form} and the {Control} {Dependence} {Graph}},
	volume = {13},
	issn = {0164-0925},
	url = {http://doi.acm.org/10.1145/115372.115320},
	doi = {10.1145/115372.115320},
	number = {4},
	urldate = {2016-03-07TZ},
	journal = {ACM Trans. Program. Lang. Syst.},
	author = {Cytron, Ron and Ferrante, Jeanne and Rosen, Barry K. and Wegman, Mark N. and Zadeck, F. Kenneth},
	month = oct,
	year = {1991},
	note = {02399},
	keywords = {READ, ST, control dependence, control flow graph, def-use chain, dominator, optimizing compilers},
	pages = {451--490}
}

@inproceedings{mohammadi_automatic_2016,
	address = {Austin, TX, USA},
	title = {Automatic {Web} {Security} {Unit} {Testing}: {XSS} {Vulnerability} {Detection}},
	doi = {10.1145/2896921.2896929},
	abstract = {Integrating security testing into the workflow of software developers not only can save resources for separate security testing but also reduce the cost of fixing security vulnerabilities by detecting them early in the development cycle. We present an automatic testing approach to detect a common type of Cross Site Scripting (XSS) vulnerability caused by improper encoding of untrusted data. We automatically extract encoding functions used in a web application to sanitize untrusted inputs and then evaluate their effectiveness by automatically generating XSS attack strings. Our evaluations show that this technique can detect 0-day XSS vulnerabilities that cannot be found by static analysis tools. We will also show that our approach can efficiently cover a common type of XSS vulnerability. This approach can be generalized to test for input validation against other types injections such as command line injection.},
	booktitle = {Proceedings of the 11th {International} {Workshop} on {Automation} of {Software} {Test}},
	publisher = {ACM},
	author = {Mohammadi, Mahmoud and Chu, Bill and Lipford, Heather Richter and Murphy-Hill, Emerson},
	year = {2016},
	note = {00000 
bibtex: Mohammadi2016},
	keywords = {Attack generation, Cross-site scripting (XSS), Security test harness, Verification Keywords Sanitization evaluation, program analysis, unit testing}
}

@misc{noauthor_guide_nodate,
	title = {Guide to using {Auto} {Escape}},
	url = {https://google-ctemplate.googlecode.com/svn/trunk/doc/auto_escape.html},
	urldate = {2016-03-02TZ},
	note = {00000}
}

@article{duchene_xss_2012,
	title = {{XSS} {Vulnerability} {Detection} {Using} {Model} {Inference} {Assisted} {Evolutionary} {Fuzzing}},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6200193},
	doi = {10.1109/ICST.2012.181},
	abstract = {We present an approach to detect web injection vulnerabilities by generating test inputs using a combination of model inference and evolutionary fuzzing. Model inference is used to obtain a knowledge about the application behavior. Based on this understanding, inputs are generated using genetic algorithm (GA). GA uses the learned formal model to automatically generate inputs with better fitness values towards triggering an instance of the given vulnerability.},
	number = {Itea 2},
	journal = {2012 IEEE Fifth International Conference on Software Testing, Verification and Validation},
	author = {Duchene, Fabien and Groz, Roland and Rawat, Sanjay and Richier, Jean-Luc},
	year = {2012},
	note = {00028},
	keywords = {Black-Box Security Testing, DN, Genetic Algorithm, READ, Tes},
	pages = {815--817}
}

@book{graff_secure_2003,
	title = {Secure coding: principles and practices},
	publisher = {" O'Reilly Media, Inc."},
	author = {Graff, Mark and Van Wyk, Kenneth R},
	year = {2003},
	note = {00144},
	keywords = {READ, VP}
}

@article{halfond_14_2011,
	title = {14) {Improving} penetration testing through static and dynamic analysis},
	volume = {Volume 21},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/stvr.450/pdf},
	doi = {10.1002/stvr},
	abstract = {Penetration testing is widely used to help ensure the security of web applications. Using penetration testing, testers discover vulnerabilities by simulating attacks on a target web application. To do this efﬁciently, testers rely on automated techniques that gather input vector information about the target web application and analyze the application’s responses to determine whether an attack was successful. Techniques for performing these steps are often incomplete, which can leave parts of the web application untested and vulnerabilities undiscovered. This paper proposes a new approach to penetration testing that addresses the limitations of current techniques. The approach incorporates two recently developed analysis techniques to improve input vector identiﬁcation and detect when attacks have been successful against a web application. This paper compares the proposed approach against two popular penetration testing tools for a suite of web applications with known and unknown vulnerabilities. The evaluation results show that the proposed approach performs a more thorough penetration testing and leads to the discovery of more vulnerabilities than both the tools.},
	number = {Issue 3},
	journal = {ICST 2009, the Second IEEE International Conference on Software Testing, Verification and Validation},
	author = {Halfond, William G J and Choudhary, Shauvik Roy and Orso, Alessandro},
	year = {2011},
	note = {00000},
	keywords = {penetration testing, test input generation, web applications},
	pages = {195--214}
}

@misc{www-03.ibm.com_ibm_2016,
	title = {{IBM} - {Software} - {IBM} {Security} {AppScan}},
	url = {http://www-03.ibm.com/software/products/en/appscan},
	author = {{Www-03.ibm.com}},
	year = {2016},
	note = {00000}
}

@inproceedings{xie_aside:_2011,
	address = {New York, New York, USA},
	title = {{ASIDE}: {IDE} {Support} {forWeb} {Application} {Security} {Jing}},
	isbn = {978-1-4503-0672-0},
	url = {http://dl.acm.org/citation.cfm?doid=2076732.2076770},
	doi = {10.1145/2076732.2076770},
	abstract = {Many of today's application security vulnerabilities are in- troduced by software developers writing insecure code. This may be due to either a lack of understanding of secure pro- gramming practices, and/or developers' lapses of attention on security. Much work on software security has focused on detecting software vulnerabilities through automated anal- ysis techniques. While they are effective, we believe they are not sufficient. We propose to increase developer aware- ness and promote practice of secure programming by interac- tively reminding programmers of secure programming prac- tices inside Integrated Development Environments (IDEs). We have implemented a proof-of-concept plugin for Eclipse and Java. Initial evaluation results show that this approach can detect and address common web application vulnerabil- ities and can serve as an effective aid for programmers. Our approach can also effectively complement existing software security best practices and significantly increase developer productivity.},
	booktitle = {Proceedings of the 27th {Annual} {Computer} {Security} {Applications} {Conference} on - {ACSAC} '11},
	publisher = {ACM Press},
	author = {Xie, Jing and Chu, Bill and Lipford, Heather Richter and Melton, John T},
	year = {2011},
	note = {00000},
	keywords = {application security, interactive sup-, port, secure programming, secure software development},
	pages = {267}
}

@misc{w3.org_extensible_2016,
	title = {Extensible {Markup} {Language} ({XML}) 1.0 ({Fifth} {Edition})},
	url = {https://www.w3.org/TR/REC-xml/},
	urldate = {2016-01-17},
	author = {{W3.org}},
	year = {2016},
	note = {00000}
}

@misc{owasp.org_owasp_2016,
	title = {{OWASP} {AntiSamy} {Project} - {OWASP}},
	url = {https://www.owasp.org/index.php/Category:OWASP_AntiSamy_Project},
	urldate = {2016-01-18},
	author = {{Owasp.org}},
	year = {2016},
	note = {00000}
}

@misc{owasp.org_cross-site_2016,
	title = {Cross-site {Scripting} ({XSS}) - {OWASP}},
	url = {https://www.owasp.org/index.php/Cross-site_Scripting_(XSS)},
	urldate = {2016-02-26},
	author = {{Owasp.org}},
	year = {2016},
	note = {00000}
}

@misc{ecma-international.org_ecmascript_2016,
	title = {{ECMAScript} {Language} {Specification} - {ECMA}-262 {Edition} 5.1},
	url = {http://www.ecma-international.org/ecma-262/5.1/},
	urldate = {2016-01-17},
	author = {{Ecma-international.org}},
	year = {2016},
	note = {00000}
}
